{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Foundational AI Terminology"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🧠 Foundational AI Terminology\n",
        "\n",
        "This file covers the basic terminology that forms the foundation of understanding AI and machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb1675f0",
      "metadata": {},
      "source": [
        "## 1. **Token**\n",
        "- A **token** is a small chunk of text that the model processes.\n",
        "- Can be:\n",
        "  - A **word** (e.g., \"hello\")\n",
        "  - A **sub-word** (e.g., \"walk\" + \"ing\")\n",
        "  - A **character** (e.g., \"h\", \"e\", \"l\", \"l\", \"o\")\n",
        "- Example: \"I'm fine.\" → tokens: `[\"I\", \"'m\", \"fine\", \".\"]`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc15f527",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "Tokenization is the very first step in processing text for language models. It's essentially the process of breaking down text into smaller, manageable pieces called tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e0b16d",
      "metadata": {},
      "source": [
        "#### Why Tokenization Matters\n",
        "Language models don't understand raw text - they work with numerical representations. Tokenization bridges this gap by:\n",
        "1. Converting text into discrete units that can be mapped to vectors\n",
        "2. Creating a finite vocabulary that the model can work with\n",
        "3. Allowing the model to process unknown words by breaking them into familiar subwords"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18d61e7",
      "metadata": {},
      "source": [
        "#### Tokenization Methods\n",
        "Different models use different tokenization strategies:\n",
        "\n",
        "1. **Word-based Tokenization**\n",
        "   - Splits text on spaces and punctuation\n",
        "   - Advantage: Preserves word meanings\n",
        "   - Disadvantage: Large vocabulary size, problems with unknown words\n",
        "\n",
        "2. **Character-based Tokenization**\n",
        "   - Splits text into individual characters\n",
        "   - Advantage: Tiny vocabulary, no unknown tokens\n",
        "   - Disadvantage: Long sequences, loses word-level semantics\n",
        "\n",
        "3. **Subword Tokenization** (Most common in modern LLMs)\n",
        "   - Breaks common words into single tokens\n",
        "   - Splits rare words into multiple subword tokens\n",
        "   - Examples: Byte-Pair Encoding (BPE), WordPiece, SentencePiece\n",
        "   - Advantage: Balance between vocabulary size and sequence length"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26500ab7",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "```\n",
        "Original Text: \"I don't understand tokenization\"\n",
        "\n",
        "Word Tokenization:\n",
        "[\"I\", \"don't\", \"understand\", \"tokenization\"]\n",
        "\n",
        "Character Tokenization:\n",
        "[\"I\", \" \", \"d\", \"o\", \"n\", \"'\", \"t\", \" \", \"u\", \"n\", \"d\", \"e\", \"r\", \"s\", \"t\", \"a\", \"n\", \"d\", \" \", \"t\", \"o\", \"k\", \"e\", \"n\", \"i\", \"z\", \"a\", \"t\", \"i\", \"o\", \"n\"]\n",
        "\n",
        "Subword Tokenization (BPE):\n",
        "[\"I\", \"don't\", \"under\", \"stand\", \"token\", \"ization\"]\n",
        "```\n",
        "\n",
        "The example illustrates how the same text gets broken down differently. For a rare word like \"tokenization,\" subword tokenization splits it into recognizable parts (\"token\" and \"ization\")."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a757c15",
      "metadata": {},
      "source": [
        "#### Impact on Model Understanding\n",
        "How a text is tokenized greatly affects how the model processes it:\n",
        "- More tokens = more steps to process the text\n",
        "- Splitting words into subwords can help models understand morphology (prefixes, suffixes)\n",
        "- Different languages may require different tokenization approaches\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8527fdcb",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, BertTokenizer, T5Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sample text\n",
        "text = \"I'm fine. Extraordinary tokenization demonstrates subword decomposition.\"\n",
        "\n",
        "# Different tokenizers\n",
        "tokenizers = {\n",
        "    \"GPT-2 (BPE)\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "    \"BERT (WordPiece)\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "    \"T5 (SentencePiece)\": T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "}\n",
        "\n",
        "# Visualize different tokenization strategies\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"\\n{name} tokens:\")\n",
        "    print(tokens)\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "    \n",
        "    # Show token IDs\n",
        "    token_ids = tokenizer.encode(text)\n",
        "    print(f\"Token IDs: {token_ids}\")\n",
        "    \n",
        "    # Verify we can decode back to original text\n",
        "    decoded = tokenizer.decode(token_ids)\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    \n",
        "    # Visualization code would go here in a notebook environment\n",
        "    # We'd create a visual showing the token boundaries\n",
        "\n",
        "# Additional example showing how rare words get broken down\n",
        "rare_word = \"antidisestablishmentarianism\"\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    tokens = tokenizer.tokenize(rare_word)\n",
        "    print(f\"\\n{name} tokenization of '{rare_word}':\")\n",
        "    print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6edbb2e7",
      "metadata": {},
      "source": [
        "### Real-World Application\n",
        "Tokenization directly impacts:\n",
        "1. **Model performance** - better tokenization = better understanding\n",
        "2. **Context window usage** - efficient tokenization allows more content to fit\n",
        "3. **Processing speed** - fewer tokens = faster processing\n",
        "4. **Multilingual capability** - some tokenizers handle multiple languages better\n",
        "\n",
        "In practice, when working with LLMs, understanding tokenization helps you:\n",
        "- Estimate costs (many APIs charge per token)\n",
        "- Optimize prompts to use fewer tokens\n",
        "- Debug issues where models misunderstand text due to unusual tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a577e4d1",
      "metadata": {},
      "source": [
        "## 2. **Embedding**\n",
        "- Converts tokens into **vectors** (numbers) in high-dimensional space.\n",
        "- Embeddings help models understand **relationships** between words.\n",
        "- Example: \"king\" and \"queen\" will have embeddings close to each other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1190270",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "Embeddings are the mathematical backbone of how language models understand meaning. They transform discrete tokens into continuous vector spaces where semantic relationships can be represented geometrically."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bc1dded",
      "metadata": {},
      "source": [
        "#### What Are Embeddings?\n",
        "An embedding is a dense vector of floating-point values. The number of dimensions typically ranges from 100 to 1024 or more, depending on the model. Each dimension captures some aspect of the token's meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe79417",
      "metadata": {},
      "source": [
        "#### Why Embeddings Matter\n",
        "Embeddings solve a fundamental challenge in NLP: how to represent words so that their relationships are mathematically accessible to the model. They allow:\n",
        "\n",
        "1. **Semantic similarity** - Words with similar meanings have similar vectors\n",
        "2. **Analogy representation** - Relationships can be captured through vector arithmetic\n",
        "3. **Dimensionality reduction** - Converting sparse one-hot encodings to dense vectors\n",
        "4. **Transfer learning** - Pre-trained embeddings can be used in multiple downstream tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54ee01e9",
      "metadata": {},
      "source": [
        "#### How Embeddings Work\n",
        "1. Initially, each token is randomly assigned a vector\n",
        "2. During training, these vectors are adjusted\n",
        "3. The model learns to place semantically similar words closer together\n",
        "4. The result is a rich space where distance and direction have meaning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f32fc0e",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "```\n",
        "Embedding Example in 2D Space (simplified from high-dimensional space):\n",
        "\n",
        "king   •                    • queen\n",
        "       \\                    /\n",
        "        \\                  /\n",
        "         \\                /\n",
        "          \\              /\n",
        "           \\            /\n",
        "            \\          /\n",
        "             \\        /\n",
        "man    •------\\------/------• woman\n",
        "               \\    /\n",
        "                \\  /\n",
        "                 \\/\n",
        "              language\n",
        "                 •\n",
        "```\n",
        "\n",
        "In the visualization above, you can see how relationships are preserved in the embedding space. The vector difference between \"man\" and \"woman\" is approximately the same as the vector difference between \"king\" and \"queen\".\n",
        "\n",
        "Mathematically, this is often expressed as:\n",
        "```\n",
        "king - man + woman ≈ queen\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a80f406",
      "metadata": {},
      "source": [
        "#### Types of Embeddings\n",
        "1. **Static Embeddings** (Word2Vec, GloVe, FastText)\n",
        "   - Each word has exactly one embedding regardless of context\n",
        "   - Faster but less nuanced\n",
        "\n",
        "2. **Contextual Embeddings** (BERT, GPT, T5)\n",
        "   - A word's embedding changes based on surrounding context\n",
        "   - More accurate for capturing multiple word senses\n",
        "   - Example: \"bank\" has different embeddings in \"river bank\" vs. \"bank account\"\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5643b30",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Words for embedding visualization\n",
        "words = [\"king\", \"queen\", \"man\", \"woman\", \"doctor\", \"nurse\", \"programmer\", \"artist\"]\n",
        "\n",
        "# Get embeddings from BERT\n",
        "def get_word_embedding(word):\n",
        "    # Add special tokens and convert to tensor\n",
        "    input_ids = tokenizer.encode(word, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get model embedding (without fine-tuning or gradient tracking)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        # Get the embedding of the actual word (not special tokens)\n",
        "        word_embedding = outputs.last_hidden_state[0, 1:-1].mean(dim=0)\n",
        "    \n",
        "    return word_embedding.numpy()\n",
        "\n",
        "# Collect embeddings for all words\n",
        "embeddings = [get_word_embedding(word) for word in words]\n",
        "embeddings_array = np.array(embeddings)\n",
        "\n",
        "# Reduce to 2D for visualization using PCA\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(embeddings_array)\n",
        "\n",
        "# Create a simple 2D plot of the embeddings\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c='blue', alpha=0.3)\n",
        "\n",
        "# Add labels for each point\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=12)\n",
        "\n",
        "plt.title(\"2D PCA projection of word embeddings\", fontsize=15)\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "# Demonstrate analogy calculations in embedding space\n",
        "def analogy(word1, word2, word3):\n",
        "    \"\"\"Find word4 such that word1 : word2 :: word3 : word4\"\"\"\n",
        "    emb1 = get_word_embedding(word1)\n",
        "    emb2 = get_word_embedding(word2)\n",
        "    emb3 = get_word_embedding(word3)\n",
        "    \n",
        "    # The analogy vector\n",
        "    analogy_vec = emb2 - emb1 + emb3\n",
        "    \n",
        "    # This would normally search a full vocabulary for closest match\n",
        "    # Here we just print the vector for demonstration\n",
        "    return analogy_vec\n",
        "\n",
        "print(\"Vector for king - man + woman ≈ queen:\")\n",
        "queen_vec = analogy(\"man\", \"woman\", \"king\")\n",
        "print(f\"Shape: {queen_vec.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a458aeb",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "Embeddings are the foundation for many NLP tasks:\n",
        "\n",
        "1. **Semantic Search**: Finding documents with similar meanings, not just keyword matches\n",
        "2. **Recommendation Systems**: Suggesting similar items based on embedding proximity\n",
        "3. **Machine Translation**: Mapping words between language embedding spaces\n",
        "4. **Sentiment Analysis**: Classifying text tone using embedding features\n",
        "5. **Named Entity Recognition**: Identifying entity types from contextual embeddings\n",
        "\n",
        "The quality of embeddings directly affects model performance - better embeddings lead to more accurate language understanding. Fine-tuning embeddings on domain-specific data is a common technique to improve performance for specialized tasks like medical or legal text analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a32f9df1",
      "metadata": {},
      "source": [
        "## 3. **Weights**\n",
        "- These are the **parameters** learned during training.\n",
        "- Represent the **knowledge** of the model.\n",
        "- Stored as **tensors** (multidimensional arrays).\n",
        "- Updated during **training** using **backpropagation**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b85e9ff",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "Weights are the core components that enable neural networks to learn. They are adjustable parameters that determine how input signals are transformed as they pass through the network. Understanding weights is essential to grasping how neural networks actually \"learn\" from data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7cac13",
      "metadata": {},
      "source": [
        "#### What Are Weights?\n",
        "\n",
        "In a neural network, weights are:\n",
        "- Numeric values that represent the strength of connections between neurons\n",
        "- Typically stored as matrices or tensors (multi-dimensional arrays)\n",
        "- Initialized with random values before training\n",
        "- Continuously adjusted during training to minimize the loss function\n",
        "\n",
        "Think of weights as knobs that the network adjusts to improve its performance on a specific task. Initially, these knobs are set randomly, and through training, they're gradually adjusted to optimal values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd35658c",
      "metadata": {},
      "source": [
        "#### Weights in Different Neural Network Architectures\n",
        "\n",
        "1. **Fully Connected (Dense) Layers**\n",
        "   - Each weight connects one neuron to another in the next layer\n",
        "   - For a layer with n inputs and m outputs, there are n×m weights\n",
        "   - Represented as a matrix W of shape (n, m)\n",
        "\n",
        "2. **Convolutional Layers**\n",
        "   - Weights are shared across the input (parameter sharing)\n",
        "   - Stored as filters/kernels (small matrices)\n",
        "   - Typical CNN filter sizes: 3×3, 5×5, 7×7\n",
        "   - Much fewer parameters than equivalent dense layers\n",
        "\n",
        "3. **Attention Mechanisms**\n",
        "   - Multiple weight matrices for queries, keys, and values\n",
        "   - Self-attention weights are dynamically computed during inference\n",
        "   - Transformer models contain billions of weights across all layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b47bcd3",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "Weights in a Simple Neural Network:\n",
        "\n",
        "      [Input Layer]        [Hidden Layer]       [Output Layer]\n",
        "          x₁                    \n",
        "           \\                h₁\n",
        "            \\               /\\\n",
        "             w₁₁           /  \\\n",
        "              \\           /    \\\n",
        "               \\         /      \\\n",
        "                →       →        → \n",
        "  Input        \\       /        /      Output\n",
        "   x₂ ────w₂₁───→ h₂ /        /\n",
        "           \\       \\ \\       /\n",
        "            \\       \\ \\     /\n",
        "             w₂₂     \\ \\   /\n",
        "              \\       \\ \\ /\n",
        "               \\       \\ /\n",
        "                →       →\n",
        "                     h₃                    \n",
        "                     \n",
        "Each arrow represents a weight (e.g., w₁₁, w₂₁, w₂₂)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2453219e",
      "metadata": {},
      "source": [
        "#### Weight Initialization\n",
        "\n",
        "The initial values of weights can significantly impact training:\n",
        "\n",
        "1. **Zero Initialization**: All weights = 0\n",
        "   - Problem: All neurons in a layer compute the same output, making the network useless\n",
        "\n",
        "2. **Random Initialization**: Random small values\n",
        "   - Basic approach: Uniform or normal distribution\n",
        "   - Helps break symmetry between neurons\n",
        "\n",
        "3. **Xavier/Glorot Initialization**: Scale based on number of inputs and outputs\n",
        "   - Weights ~ N(0, √(2/(n_in + n_out))\n",
        "   - Helps maintain variance across layers\n",
        "\n",
        "4. **He Initialization**: Optimized for ReLU activations\n",
        "   - Weights ~ N(0, √(2/n_in))\n",
        "   - Prevents vanishing gradients with ReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b047f9ca",
      "metadata": {},
      "source": [
        "#### Weight Updates During Training\n",
        "\n",
        "Weights are updated using gradient descent:\n",
        "\n",
        "```\n",
        "w_new = w_old - learning_rate * gradient\n",
        "\n",
        "Where:\n",
        "- w_old is the current weight\n",
        "- learning_rate controls the step size\n",
        "- gradient is the derivative of the loss with respect to the weight\n",
        "```\n",
        "\n",
        "The process forms the core of how neural networks learn:\n",
        "1. Initialize weights (usually randomly)\n",
        "2. Forward pass: compute predictions using current weights\n",
        "3. Calculate loss: measure error between predictions and true values\n",
        "4. Backward pass: compute gradients of loss with respect to weights\n",
        "5. Update weights: adjust weights to reduce loss\n",
        "6. Repeat steps 2-5 until convergence\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e042dd",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a simple neural network with 2 hidden layers\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size, init_method='default'):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.layer1 = nn.Linear(input_size, hidden1_size)\n",
        "        self.layer2 = nn.Linear(hidden1_size, hidden2_size)\n",
        "        self.layer3 = nn.Linear(hidden2_size, output_size)\n",
        "        \n",
        "        # Apply initialization based on method\n",
        "        self.init_weights(init_method)\n",
        "        \n",
        "        # Activation functions\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def init_weights(self, method):\n",
        "        if method == 'zeros':\n",
        "            for layer in [self.layer1, self.layer2, self.layer3]:\n",
        "                nn.init.zeros_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "        elif method == 'normal':\n",
        "            for layer in [self.layer1, self.layer2, self.layer3]:\n",
        "                nn.init.normal_(layer.weight, mean=0, std=0.1)\n",
        "                nn.init.normal_(layer.bias, mean=0, std=0.1)\n",
        "        elif method == 'xavier':\n",
        "            for layer in [self.layer1, self.layer2, self.layer3]:\n",
        "                nn.init.xavier_normal_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "        elif method == 'he':\n",
        "            for layer in [self.layer1, self.layer2, self.layer3]:\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "                nn.init.zeros_(layer.bias)\n",
        "        # Default uses PyTorch's default initialization\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "# Function to compare weight distributions\n",
        "def visualize_weight_distributions():\n",
        "    input_size, hidden1_size, hidden2_size, output_size = 10, 20, 15, 1\n",
        "    \n",
        "    # Create models with different initializations\n",
        "    models = {\n",
        "        'Default': SimpleNN(input_size, hidden1_size, hidden2_size, output_size),\n",
        "        'Zeros': SimpleNN(input_size, hidden1_size, hidden2_size, output_size, 'zeros'),\n",
        "        'Normal': SimpleNN(input_size, hidden1_size, hidden2_size, output_size, 'normal'),\n",
        "        'Xavier': SimpleNN(input_size, hidden1_size, hidden2_size, output_size, 'xavier'),\n",
        "        'He': SimpleNN(input_size, hidden1_size, hidden2_size, output_size, 'he')\n",
        "    }\n",
        "    \n",
        "    # Plot weight distributions\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, (name, model) in enumerate(models.items()):\n",
        "        # Get weights from first layer\n",
        "        weights = model.layer1.weight.detach().flatten().numpy()\n",
        "        \n",
        "        plt.subplot(2, 3, i+1)\n",
        "        plt.hist(weights, bins=50)\n",
        "        plt.title(f'{name} Initialization')\n",
        "        plt.xlabel('Weight Value')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('Weight Distributions with Different Initialization Methods', fontsize=16)\n",
        "    plt.subplots_adjust(top=0.9)\n",
        "    plt.show()\n",
        "\n",
        "# Function to track weight changes during training\n",
        "def visualize_weight_evolution():\n",
        "    # Create simple dataset\n",
        "    np.random.seed(42)\n",
        "    X = torch.FloatTensor(np.random.randn(1000, 5))\n",
        "    y = torch.FloatTensor((X[:, 0] > 0).reshape(-1, 1).astype(float))\n",
        "    \n",
        "    # Create model\n",
        "    model = SimpleNN(5, 8, 4, 1, init_method='xavier')\n",
        "    \n",
        "    # Training settings\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "    epochs = 50\n",
        "    \n",
        "    # Storage for weight snapshots\n",
        "    weight_history = []\n",
        "    loss_history = []\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        y_pred = model(X)\n",
        "        loss = criterion(y_pred, y)\n",
        "        \n",
        "        # Store current weights\n",
        "        weight_snapshot = model.layer1.weight[0, :].detach().clone().numpy()\n",
        "        weight_history.append(weight_snapshot)\n",
        "        loss_history.append(loss.item())\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "    \n",
        "    # Convert to numpy array for easier plotting\n",
        "    weight_history = np.array(weight_history)\n",
        "    \n",
        "    # Plot weight evolution\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Plot loss curve\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(loss_history)\n",
        "    plt.title('Loss During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot weight changes for first 5 weights\n",
        "    plt.subplot(2, 1, 2)\n",
        "    for i in range(5):\n",
        "        plt.plot(weight_history[:, i], label=f'Weight {i+1}')\n",
        "    \n",
        "    plt.title('Weight Evolution During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Weight Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the visualizations\n",
        "# visualize_weight_distributions()\n",
        "# visualize_weight_evolution()\n",
        "\n",
        "# Examine model's parameters more closely\n",
        "def examine_model_weights():\n",
        "    # Create a small model for easy visualization\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(10, 5),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(5, 1)\n",
        "    )\n",
        "    \n",
        "    # Display weight information\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params}\")\n",
        "    \n",
        "    # Access weights of first layer\n",
        "    first_layer = model[0]\n",
        "    first_layer_weights = first_layer.weight\n",
        "    first_layer_bias = first_layer.bias\n",
        "    \n",
        "    print(f\"\\nFirst layer weights shape: {first_layer_weights.shape}\")\n",
        "    print(f\"First layer bias shape: {first_layer_bias.shape}\")\n",
        "    \n",
        "    # Print sample of weights and bias\n",
        "    print(\"\\nSample of first layer weights:\")\n",
        "    print(first_layer_weights[:2, :3])  # First 2 neurons, first 3 inputs\n",
        "    \n",
        "    print(\"\\nFirst layer bias:\")\n",
        "    print(first_layer_bias)\n",
        "    \n",
        "    # Access weights of second layer\n",
        "    second_layer = model[2]\n",
        "    second_layer_weights = second_layer.weight\n",
        "    \n",
        "    print(f\"\\nSecond layer weights shape: {second_layer_weights.shape}\")\n",
        "    print(\"Second layer weights (all):\")\n",
        "    print(second_layer_weights)\n",
        "    \n",
        "    # Show parameter naming convention in PyTorch\n",
        "    print(\"\\nNamed parameters in the model:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: shape {param.shape}, requires_grad={param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385113e0",
      "metadata": {},
      "source": [
        "# Run the weight examination\n",
        "examine_model_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bafbdf0",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Understanding weights is critical for several practical applications:\n",
        "\n",
        "1. **Model Compression**\n",
        "   - Weight pruning: Setting unimportant weights to zero\n",
        "   - Weight quantization: Reducing precision (e.g., float32 → int8)\n",
        "   - Knowledge distillation: Transferring knowledge to smaller models\n",
        "\n",
        "2. **Transfer Learning**\n",
        "   - Pre-trained weights capture general knowledge\n",
        "   - Fine-tuning adjusts weights for specific tasks\n",
        "   - Feature extraction keeps early layer weights frozen\n",
        "\n",
        "3. **Model Interpretability**\n",
        "   - Analyzing weight magnitudes to identify important features\n",
        "   - Visualizing convolutional filters to understand what patterns they detect\n",
        "   - Attention weights show which inputs influence outputs\n",
        "\n",
        "4. **Model Deployment**\n",
        "   - Weights determine model size and memory requirements\n",
        "   - Efficient weight storage enables mobile/edge deployment\n",
        "   - Weight updates enable continuous learning\n",
        "\n",
        "5. **Adversarial Robustness**\n",
        "   - Weight regularization improves generalization and robustness\n",
        "   - Adversarial training adjusts weights to handle attack examples\n",
        "\n",
        "Understanding how weights work, how they're initialized, and how they evolve during training provides insight into the learning dynamics of neural networks and is essential for developing effective AI systems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b1b666",
      "metadata": {},
      "source": [
        "## 4. **Epoch**\n",
        "- One **full pass** through the entire training dataset.\n",
        "- Example: If your dataset has 10,000 examples, one epoch means training over all 10,000 once."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68ce960",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "An epoch is a fundamental concept in machine learning training that refers to one complete iteration through the entire training dataset. Understanding epochs is crucial for proper model training, convergence, and avoiding problems like overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70eba603",
      "metadata": {},
      "source": [
        "#### Why Epochs Matter\n",
        "\n",
        "Training data is fed through neural networks in small batches for practical reasons (memory limitations, computational efficiency). An epoch represents a logical unit where the model has had the opportunity to learn from all available training examples once.\n",
        "\n",
        "Epochs matter because:\n",
        "1. **Learning Process**: Models typically need multiple exposures to data to learn effectively\n",
        "2. **Convergence Tracking**: Performance is usually measured at epoch boundaries\n",
        "3. **Early Stopping**: The number of epochs often needs to be limited to prevent overfitting\n",
        "4. **Learning Rate Schedules**: Many optimization strategies change learning rates based on epoch counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69684f91",
      "metadata": {},
      "source": [
        "#### The Training Loop Structure\n",
        "\n",
        "Training neural networks involves nested loops:\n",
        "\n",
        "```\n",
        "TRAINING LOOP STRUCTURE:\n",
        "\n",
        "for epoch in range(num_epochs):               <- Epoch Loop\n",
        "    for batch in dataset:                     <- Batch Loop\n",
        "        # Forward pass\n",
        "        predictions = model(batch.inputs)\n",
        "        loss = loss_function(predictions, batch.targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        gradients = compute_gradients(loss)\n",
        "        update_model_parameters(gradients)\n",
        "    \n",
        "    # End of epoch operations\n",
        "    validate_model()\n",
        "    save_checkpoint()\n",
        "    update_learning_rate()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8222fc60",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "Epoch Visualization:\n",
        "\n",
        "DATASET (10 samples):    [1] [2] [3] [4] [5] [6] [7] [8] [9] [10]\n",
        "\n",
        "EPOCH 1:\n",
        "Batch 1: [1] [2] [3] [4]  → Update Model\n",
        "Batch 2: [5] [6] [7] [8]  → Update Model\n",
        "Batch 3: [9] [10]         → Update Model\n",
        "                          → Validate, Checkpoint, Adjust LR\n",
        "\n",
        "EPOCH 2:\n",
        "Batch 1: [1] [2] [3] [4]  → Update Model\n",
        "Batch 2: [5] [6] [7] [8]  → Update Model\n",
        "Batch 3: [9] [10]         → Update Model\n",
        "                          → Validate, Checkpoint, Adjust LR\n",
        "\n",
        "... and so on for EPOCH 3, EPOCH 4, etc.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50f3fc1d",
      "metadata": {},
      "source": [
        "#### Epoch vs. Iteration vs. Batch\n",
        "\n",
        "These terms are often confused:\n",
        "\n",
        "1. **Epoch**: One complete pass through the entire training dataset\n",
        "2. **Batch**: A subset of the training data processed in one forward/backward pass\n",
        "3. **Iteration**: One update step (processing one batch)\n",
        "\n",
        "The relationship is:\n",
        "- Iterations per epoch = Number of samples / Batch size\n",
        "- Example: With 1,000 samples and batch size of 10, one epoch equals 100 iterations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f70aa10",
      "metadata": {},
      "source": [
        "#### Learning Dynamics Across Epochs\n",
        "\n",
        "Typical learning dynamics over epochs:\n",
        "\n",
        "1. **Early Epochs**: Rapid decrease in training loss, significant weight updates\n",
        "2. **Middle Epochs**: Slower, more stable improvements\n",
        "3. **Later Epochs**: \n",
        "   - Ideal case: Both training and validation loss continue to decrease\n",
        "   - Common case: Training loss decreases but validation loss increases (overfitting)\n",
        "   - Plateau case: Training and validation loss flatten, indicating convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8390fc1",
      "metadata": {},
      "source": [
        "#### Determining the Optimal Number of Epochs\n",
        "\n",
        "There's no one-size-fits-all answer, but techniques include:\n",
        "\n",
        "1. **Early Stopping**: Stop training when validation performance stops improving or starts degrading\n",
        "2. **Learning Curves Analysis**: Plot training vs. validation loss across epochs\n",
        "3. **Rule of Thumb**: Small datasets often need more epochs; larger datasets may need fewer\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8ce72ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create a synthetic dataset\n",
        "def create_dataset(size=1000):\n",
        "    X = torch.randn(size, 10)  # 10 features\n",
        "    # Create a non-linear relationship\n",
        "    y = 0.2 * (X[:, 0]**2) + 0.5 * X[:, 1] - 0.7 * X[:, 2] + 0.1 * torch.randn(size)\n",
        "    y = y.unsqueeze(1)  # Add dimension for target\n",
        "    return X, y\n",
        "\n",
        "# Create model, dataset, and training components\n",
        "def setup_training(batch_size=32, learning_rate=0.01):\n",
        "    # Create data\n",
        "    X, y = create_dataset()\n",
        "    \n",
        "    # Split into train and validation sets (80/20)\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "    \n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "    \n",
        "    # Simple neural network for regression\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(10, 16),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(16, 8),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(8, 1)\n",
        "    )\n",
        "    \n",
        "    # Loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return model, train_loader, val_loader, criterion, optimizer, len(train_loader)\n",
        "\n",
        "# Training for multiple epochs with detailed logging\n",
        "def train_with_epoch_logging(num_epochs=20):\n",
        "    # Setup\n",
        "    model, train_loader, val_loader, criterion, optimizer, iterations_per_epoch = setup_training()\n",
        "    \n",
        "    # For tracking metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    epoch_metrics = []\n",
        "    weight_evolution = []\n",
        "    \n",
        "    # Store weight snapshots (first layer, first neuron)\n",
        "    first_layer = model[0]\n",
        "    \n",
        "    # Detailed tracking for a single epoch\n",
        "    batch_losses = []\n",
        "    batch_iterations = []\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        model.train()\n",
        "        \n",
        "        # For detailed logging of first epoch\n",
        "        detailed_epoch = epoch == 0\n",
        "        \n",
        "        # Batch loop\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Track loss\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            # Detailed logging for visualization\n",
        "            if detailed_epoch:\n",
        "                batch_losses.append(loss.item())\n",
        "                batch_iterations.append(i)\n",
        "        \n",
        "        # Record weights at end of epoch\n",
        "        weight_sample = first_layer.weight[0, :5].detach().clone().numpy()\n",
        "        weight_evolution.append(weight_sample)\n",
        "        \n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        # Store epoch metrics\n",
        "        epoch_metrics.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': avg_train_loss,\n",
        "            'val_loss': avg_val_loss\n",
        "        })\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "    \n",
        "    # Convert to numpy arrays for easier plotting\n",
        "    train_losses = np.array(train_losses)\n",
        "    val_losses = np.array(val_losses)\n",
        "    weight_evolution = np.array(weight_evolution)\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'epoch_metrics': epoch_metrics,\n",
        "        'weight_evolution': weight_evolution,\n",
        "        'batch_losses': batch_losses,\n",
        "        'batch_iterations': batch_iterations,\n",
        "        'iterations_per_epoch': iterations_per_epoch\n",
        "    }\n",
        "\n",
        "# Visualize the training process\n",
        "def visualize_epochs():\n",
        "    # Run training\n",
        "    results = train_with_epoch_logging(num_epochs=20)\n",
        "    \n",
        "    # Create subplots\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    \n",
        "    # 1. Learning curves\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(range(1, len(results['train_losses'])+1), results['train_losses'], label='Training Loss')\n",
        "    plt.plot(range(1, len(results['val_losses'])+1), results['val_losses'], label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Learning Curves Across Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 2. Detailed view of first epoch\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(results['batch_iterations'], results['batch_losses'])\n",
        "    plt.xlabel('Batch Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Loss During First Epoch (Iterations per Epoch: {results[\"iterations_per_epoch\"]})')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 3. Weight evolution\n",
        "    plt.subplot(3, 1, 3)\n",
        "    for i in range(results['weight_evolution'].shape[1]):\n",
        "        plt.plot(range(1, len(results['weight_evolution'])+1), \n",
        "                 results['weight_evolution'][:, i], \n",
        "                 label=f'Weight {i+1}')\n",
        "    \n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Weight Value')\n",
        "    plt.title('Evolution of Selected Weights Across Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create table of epoch metrics\n",
        "    print(\"\\nEpoch Metrics:\")\n",
        "    print(f\"{'Epoch':<10} {'Training Loss':<15} {'Validation Loss':<15}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for metric in results['epoch_metrics']:\n",
        "        print(f\"{metric['epoch']:<10} {metric['train_loss']:<15.6f} {metric['val_loss']:<15.6f}\")\n",
        "\n",
        "# Demonstrate overfitting with epochs\n",
        "def demonstrate_overfitting():\n",
        "    # Create a very small dataset to encourage overfitting\n",
        "    X = torch.randn(50, 5)  # Only 50 examples with 5 features\n",
        "    noise = torch.randn(50) * 0.1\n",
        "    y = X[:, 0] - 2 * X[:, 1] + noise\n",
        "    y = y.unsqueeze(1)\n",
        "    \n",
        "    # Split into train/validation sets\n",
        "    split = int(0.7 * len(X))\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "    \n",
        "    # Create datasets and loaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=4)\n",
        "    \n",
        "    # Create an oversized model to encourage overfitting\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(5, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 1)\n",
        "    )\n",
        "    \n",
        "    # Loss and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    \n",
        "    # Track losses\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    # Train for many epochs\n",
        "    num_epochs = 200\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        \n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
        "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
        "                  f\"Val Loss: {avg_val_loss:.4f}\")\n",
        "    \n",
        "    # Plot training and validation losses\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')\n",
        "    \n",
        "    # Find the epoch where validation loss is minimum\n",
        "    best_epoch = np.argmin(val_losses) + 1\n",
        "    plt.axvline(x=best_epoch, color='r', linestyle='--', label=f'Best Epoch: {best_epoch}')\n",
        "    \n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Overfitting Demonstration: Training vs Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nOptimal stopping point: Epoch {best_epoch}\")\n",
        "    print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
        "    print(f\"Final validation loss: {val_losses[-1]:.6f}\")\n",
        "    print(f\"Best validation loss: {val_losses[best_epoch-1]:.6f}\")\n",
        "\n",
        "# Run the visualizations\n",
        "# visualize_epochs()  # Basic epoch visualization\n",
        "# demonstrate_overfitting()  # Demonstrate the importance of early stopping\n",
        "\n",
        "# Basic training loop example\n",
        "def basic_epoch_example():\n",
        "    # Create dummy data\n",
        "    X = torch.randn(1000, 10)\n",
        "    y = torch.randn(1000, 1)\n",
        "    dataset = TensorDataset(X, y)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    # Model\n",
        "    model = nn.Linear(10, 1)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        batch_count = 0\n",
        "        \n",
        "        for batch_X, batch_y in dataloader:\n",
        "            # Forward pass\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            batch_count += 1\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/batch_count:.4f}\")\n",
        "        \n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Number of batches per epoch: {len(dataloader)}\")\n",
        "    print(f\"Total training iterations: {num_epochs * len(dataloader)}\")\n",
        "\n",
        "# Run the simplified example\n",
        "basic_epoch_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c8002f",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Understanding epochs has several practical applications:\n",
        "\n",
        "1. **Early Stopping Implementation**:\n",
        "   - Monitor validation metrics across epochs\n",
        "   - Stop training when no improvement for N consecutive epochs\n",
        "   - Save the model checkpoint with best validation performance\n",
        "\n",
        "2. **Learning Rate Scheduling**:\n",
        "   - Reduce learning rate after certain epochs\n",
        "   - Implement warmup periods in early epochs\n",
        "   - Use cyclical learning rates tied to epoch boundaries\n",
        "\n",
        "3. **Progressive Training Techniques**:\n",
        "   - Curriculum learning: increase task difficulty with epochs\n",
        "   - Transfer learning: unfreeze more layers in later epochs\n",
        "   - Progressive resizing: increase input resolution by epoch\n",
        "\n",
        "4. **Practical Training Management**:\n",
        "   - Checkpoint models at epoch boundaries for resuming\n",
        "   - Report progress and performance by epoch\n",
        "   - Ensemble models from different epochs\n",
        "\n",
        "In practice, the concept of epochs provides a structured approach to manage the training process, allowing for systematic evaluation and optimization of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a9dc73",
      "metadata": {},
      "source": [
        "## 5. **Loss**\n",
        "- A **number** that measures how **wrong** the model is.\n",
        "- Lower loss = better performance.\n",
        "- Common LLM loss: **Cross Entropy Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3858482",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "The loss function (also called cost function or objective function) is the compass that guides the learning process. It quantifies the difference between the model's predictions and the actual target values, providing a signal for how to adjust the model's parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbc53898",
      "metadata": {},
      "source": [
        "#### Why Loss Functions Matter\n",
        "\n",
        "Loss functions are crucial because:\n",
        "1. They define what it means for a model to perform \"well\"\n",
        "2. They provide the gradient signal for optimization\n",
        "3. Different tasks require different loss functions\n",
        "4. The choice of loss function impacts learning dynamics and final performance\n",
        "\n",
        "Think of the loss function as the \"goal\" you're asking the neural network to achieve. Just as different sports have different scoring systems, different machine learning tasks have different ways of measuring success."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2e5da07",
      "metadata": {},
      "source": [
        "#### Common Loss Functions and Their Applications\n",
        "\n",
        "1. **Mean Squared Error (MSE)**\n",
        "   - Used for: Regression problems\n",
        "   - Formula: MSE = (1/n) * Σ(y_true - y_pred)²\n",
        "   - Properties: Heavily penalizes large errors, less sensitive to small errors\n",
        "   - Example use case: Predicting house prices, temperature forecasting\n",
        "\n",
        "2. **Cross-Entropy Loss**\n",
        "   - Used for: Classification problems\n",
        "   - Formula (binary): -(y_true * log(y_pred) + (1-y_true) * log(1-y_pred))\n",
        "   - Properties: Penalizes confident incorrect predictions very heavily\n",
        "   - Example use case: Image classification, sentiment analysis\n",
        "\n",
        "3. **Sparse Categorical Cross-Entropy**\n",
        "   - Used for: Multi-class classification when labels are integers\n",
        "   - Special case of cross-entropy optimized for efficiency\n",
        "   - Example use case: Next token prediction in language models\n",
        "\n",
        "4. **Kullback-Leibler Divergence (KL Divergence)**\n",
        "   - Used for: Measuring difference between probability distributions\n",
        "   - Formula: Σ(p(x) * log(p(x)/q(x)))\n",
        "   - Properties: Asymmetric measure (KL(p||q) ≠ KL(q||p))\n",
        "   - Example use case: Variational autoencoders, policy distillation\n",
        "\n",
        "5. **Contrastive Loss**\n",
        "   - Used for: Learning embeddings/representations\n",
        "   - Brings similar examples closer, pushes dissimilar examples apart\n",
        "   - Example use case: Face recognition, semantic similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1236299",
      "metadata": {},
      "source": [
        "#### Visual Representation of Loss Functions\n",
        "\n",
        "```\n",
        "Different Loss Functions Visualized:\n",
        "\n",
        "MSE (Regression):\n",
        "\n",
        "  Loss\n",
        "   ↑\n",
        "   |     *\n",
        "   |    / \\\n",
        "   |   /   \\\n",
        "   |  /     \\\n",
        "   | /       \\\n",
        "   |/         \\\n",
        "   +------------→ Prediction\n",
        "       Target\n",
        "\n",
        "Cross-Entropy (Binary Classification):\n",
        "\n",
        "  Loss\n",
        "   ↑\n",
        "   |\n",
        "   |        *\n",
        "   |       /\n",
        "   |      /\n",
        "   |     /\n",
        "   |____/\n",
        "   +------------→ Prediction\n",
        "   0            1\n",
        "       Target=1\n",
        "\n",
        "KL Divergence:\n",
        "\n",
        "  Loss\n",
        "   ↑\n",
        "   |      *\n",
        "   |     / \\\n",
        "   |    /   \\\n",
        "   |   /     \\\n",
        "   |  /       *\n",
        "   | /       / \\\n",
        "   |/_______/___\\\n",
        "   +------------→ Prediction Distribution\n",
        "       Target Distribution\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "508aeb19",
      "metadata": {},
      "source": [
        "#### Loss Landscapes\n",
        "\n",
        "The loss function creates a high-dimensional surface (landscape) that the optimization algorithm navigates:\n",
        "\n",
        "```\n",
        "2D Loss Landscape (simplified):\n",
        "\n",
        "         Global Minimum\n",
        "              *\n",
        "             / \\\n",
        "  Local     /   \\\n",
        "  Minimum  *     \\\n",
        "          / \\     \\\n",
        "         /   \\     \\\n",
        "        /     \\_____\\\n",
        "       /            /\n",
        "______/____________/\n",
        "      Starting Point\n",
        "```\n",
        "\n",
        "Real loss landscapes have millions or billions of dimensions (one per model parameter), making visualization challenging. However, we can visualize 2D slices to gain insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6247df37",
      "metadata": {},
      "source": [
        "#### Loss in Language Models\n",
        "\n",
        "For language models specifically, the loss function is typically:\n",
        "\n",
        "1. **Cross-entropy loss** on token predictions\n",
        "2. Applied over entire sequences of tokens\n",
        "3. Computed as the average negative log-likelihood of the correct next token\n",
        "\n",
        "For a language model predicting the next token in a sequence:\n",
        "- Each output is a probability distribution over the entire vocabulary\n",
        "- The target is the actual next token (one-hot encoded)\n",
        "- The loss measures how far the predicted distribution is from putting all probability on the correct token\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "931eeb22",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Different Loss Functions Visualization\n",
        "def visualize_loss_functions():\n",
        "    # Generate predictions from 0 to 1\n",
        "    y_pred = np.linspace(0.001, 0.999, 1000)\n",
        "    \n",
        "    # Calculate losses for different target values\n",
        "    # Binary Cross-Entropy Loss\n",
        "    bce_loss_true = -np.log(y_pred)  # For target=1\n",
        "    bce_loss_false = -np.log(1 - y_pred)  # For target=0\n",
        "    \n",
        "    # Mean Squared Error\n",
        "    mse_loss_true = (1 - y_pred) ** 2  # For target=1\n",
        "    mse_loss_false = y_pred ** 2  # For target=0\n",
        "    \n",
        "    # Plot the losses\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Binary Cross-Entropy\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(y_pred, bce_loss_true, 'b-', label='Target = 1')\n",
        "    plt.plot(y_pred, bce_loss_false, 'r-', label='Target = 0')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Binary Cross-Entropy Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Mean Squared Error\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(y_pred, mse_loss_true, 'b-', label='Target = 1')\n",
        "    plt.plot(y_pred, mse_loss_false, 'r-', label='Target = 0')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Mean Squared Error Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Log scale for BCE to show the asymptotic behavior\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(y_pred, bce_loss_true, 'b-', label='Target = 1')\n",
        "    plt.plot(y_pred, bce_loss_false, 'r-', label='Target = 0')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Loss (log scale)')\n",
        "    plt.title('Binary Cross-Entropy Loss (Log Scale)')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Comparison of both losses for Target = 1\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(y_pred, bce_loss_true, 'b-', label='BCE')\n",
        "    plt.plot(y_pred, mse_loss_true, 'g-', label='MSE')\n",
        "    plt.xlabel('Predicted Probability')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('BCE vs MSE (Target = 1)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 2. Loss Landscape Visualization\n",
        "def visualize_loss_landscape():\n",
        "    # Create a simple 2D loss landscape\n",
        "    x = np.linspace(-5, 5, 100)\n",
        "    y = np.linspace(-5, 5, 100)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    \n",
        "    # A function with multiple local minima\n",
        "    Z = 0.1 * (X**2 + Y**2) + np.sin(X) * np.cos(Y) + 0.1 * np.sin(5*X) * np.cos(5*Y)\n",
        "    \n",
        "    # 3D surface plot\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Surface plot\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "    ax1.set_xlabel('Parameter 1')\n",
        "    ax1.set_ylabel('Parameter 2')\n",
        "    ax1.set_zlabel('Loss')\n",
        "    ax1.set_title('3D Loss Landscape')\n",
        "    \n",
        "    # Contour plot\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    contour = ax2.contourf(X, Y, Z, 50, cmap='viridis')\n",
        "    plt.colorbar(contour, ax=ax2)\n",
        "    \n",
        "    # Plot an optimization trajectory (simulated gradient descent)\n",
        "    start_x, start_y = 4.0, 4.0\n",
        "    lr = 0.1\n",
        "    num_steps = 50\n",
        "    \n",
        "    trajectory_x = [start_x]\n",
        "    trajectory_y = [start_y]\n",
        "    \n",
        "    # Simulate gradient descent\n",
        "    for _ in range(num_steps):\n",
        "        # Compute gradients (partial derivatives)\n",
        "        dx = 0.2 * trajectory_x[-1] + np.cos(trajectory_x[-1]) * np.cos(trajectory_y[-1]) + 0.5 * np.cos(5*trajectory_x[-1]) * np.cos(5*trajectory_y[-1])\n",
        "        dy = 0.2 * trajectory_y[-1] - np.sin(trajectory_x[-1]) * np.sin(trajectory_y[-1]) - 0.5 * np.sin(5*trajectory_x[-1]) * np.sin(5*trajectory_y[-1])\n",
        "        \n",
        "        # Update position\n",
        "        new_x = trajectory_x[-1] - lr * dx\n",
        "        new_y = trajectory_y[-1] - lr * dy\n",
        "        \n",
        "        trajectory_x.append(new_x)\n",
        "        trajectory_y.append(new_y)\n",
        "    \n",
        "    # Plot the trajectory\n",
        "    ax2.plot(trajectory_x, trajectory_y, 'ro-', linewidth=2, markersize=3)\n",
        "    ax2.plot(trajectory_x[0], trajectory_y[0], 'go', markersize=8, label='Start')\n",
        "    ax2.plot(trajectory_x[-1], trajectory_y[-1], 'bo', markersize=8, label='End')\n",
        "    \n",
        "    ax2.set_xlabel('Parameter 1')\n",
        "    ax2.set_ylabel('Parameter 2')\n",
        "    ax2.set_title('Loss Landscape Contours with Optimization Path')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 3. Language Model Loss Example\n",
        "def language_model_loss_example():\n",
        "    # Vocabulary size\n",
        "    vocab_size = 10000\n",
        "    \n",
        "    # Create a simple language model example\n",
        "    class SimpleLM(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256):\n",
        "            super(SimpleLM, self).__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            output, _ = self.lstm(embedded)\n",
        "            logits = self.fc(output)\n",
        "            return logits\n",
        "    \n",
        "    # Create model\n",
        "    model = SimpleLM(vocab_size)\n",
        "    \n",
        "    # Example input sequence and target\n",
        "    batch_size = 3\n",
        "    seq_length = 10\n",
        "    \n",
        "    # Random input and target tokens\n",
        "    input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "    target_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "    \n",
        "    # Forward pass\n",
        "    logits = model(input_ids)\n",
        "    \n",
        "    # Compute loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = criterion(logits.view(-1, vocab_size), target_ids.view(-1))\n",
        "    \n",
        "    print(f\"Language Model Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    # Cross-entropy formula explanation\n",
        "    print(\"\\nCross-Entropy Loss Explained:\")\n",
        "    print(\"1. Model outputs logits (raw scores) for each token in vocabulary\")\n",
        "    print(\"2. Softmax converts logits to probabilities\")\n",
        "    print(\"3. Loss = -log(probability of correct token)\")\n",
        "    print(\"4. Averaged over all predictions in the sequence\")\n",
        "    \n",
        "    # Example for a single prediction\n",
        "    # Create a simplified example for visualization\n",
        "    vocab_size_simple = 5\n",
        "    logits_simple = torch.tensor([2.0, 1.0, 0.5, 0.0, -1.0])\n",
        "    target_simple = torch.tensor([1])  # Second token (index 1) is correct\n",
        "    \n",
        "    # Convert logits to probabilities with softmax\n",
        "    probs_simple = F.softmax(logits_simple, dim=0)\n",
        "    \n",
        "    # Calculate cross-entropy loss\n",
        "    loss_simple = F.cross_entropy(logits_simple.unsqueeze(0), target_simple)\n",
        "    \n",
        "    # Print and visualize\n",
        "    print(\"\\nSimplified Example:\")\n",
        "    print(f\"Logits: {logits_simple.tolist()}\")\n",
        "    print(f\"Probabilities: {probs_simple.tolist()}\")\n",
        "    print(f\"Target token index: {target_simple.item()}\")\n",
        "    print(f\"Probability of correct token: {probs_simple[target_simple.item()]:.4f}\")\n",
        "    print(f\"Loss: -log(prob) = {loss_simple.item():.4f}\")\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Probability distribution\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(range(vocab_size_simple), probs_simple.detach().numpy())\n",
        "    plt.axvline(x=target_simple.item(), color='r', linestyle='--', label='Target Token')\n",
        "    plt.xlabel('Token ID')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title('Model\\'s Predicted Probability Distribution')\n",
        "    plt.legend()\n",
        "    \n",
        "    # Loss for different probability values of the target\n",
        "    plt.subplot(1, 2, 2)\n",
        "    p_values = np.linspace(0.01, 1.0, 100)\n",
        "    ce_losses = -np.log(p_values)\n",
        "    \n",
        "    plt.plot(p_values, ce_losses)\n",
        "    plt.scatter([probs_simple[target_simple.item()].item()], [loss_simple.item()], \n",
        "                color='red', s=100, zorder=5)\n",
        "    plt.annotate(f'Current loss: {loss_simple.item():.4f}',\n",
        "                 xy=(probs_simple[target_simple.item()].item(), loss_simple.item()),\n",
        "                 xytext=(0.5, 5),\n",
        "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "    plt.xlabel('Probability of Correct Token')\n",
        "    plt.ylabel('Loss Value')\n",
        "    plt.title('Cross-Entropy Loss vs. Correct Token Probability')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the visualizations\n",
        "# visualize_loss_functions()\n",
        "# visualize_loss_landscape()\n",
        "# language_model_loss_example()\n",
        "\n",
        "# Simple Cross Entropy Loss example\n",
        "def simple_cross_entropy_example():\n",
        "    # Cross Entropy Loss (common for language models)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # Example: predicting next word with vocabulary size 10,000\n",
        "    logits = torch.randn(1, 10000)  # Raw model outputs\n",
        "    target = torch.tensor([42])     # Correct word index\n",
        "    \n",
        "    loss = criterion(logits, target)\n",
        "    print(f\"Loss: {loss.item():.4f}\")\n",
        "    \n",
        "    # What's happening under the hood\n",
        "    print(\"\\nUnder the hood, CrossEntropyLoss does:\")\n",
        "    print(\"1. Apply softmax to convert logits to probabilities\")\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    print(f\"   - Probability of target token (42): {probs[0, 42]:.6f}\")\n",
        "    \n",
        "    print(\"2. Take the negative log of the probability for the target token\")\n",
        "    manual_loss = -torch.log(probs[0, 42])\n",
        "    print(f\"   - -log(prob) = {manual_loss.item():.4f}\")\n",
        "    \n",
        "    print(f\"\\nComparing PyTorch's loss ({loss.item():.4f}) to manual calculation ({manual_loss.item():.4f})\")\n",
        "\n",
        "# Run the simple example\n",
        "simple_cross_entropy_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc48d86b",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Loss functions are central to model development and have many practical applications:\n",
        "\n",
        "1. **Model Selection**\n",
        "   - Different losses create models with different behaviors\n",
        "   - Example: Mean squared error (MSE) vs. mean absolute error (MAE) for outlier handling\n",
        "\n",
        "2. **Custom Losses for Specific Requirements**\n",
        "   - Adding regularization terms to encourage sparsity, smoothness, etc.\n",
        "   - Weighting certain examples or classes more heavily\n",
        "   - Focal loss for handling extreme class imbalance\n",
        "\n",
        "3. **Multi-Task Learning**\n",
        "   - Combining multiple loss functions for different tasks\n",
        "   - Example: Image generation with both pixel-wise and perceptual losses\n",
        "\n",
        "4. **Learning from Human Feedback**\n",
        "   - RLHF uses a reward model to create a loss function from human preferences\n",
        "   - DPO (Direct Preference Optimization) directly optimizes for human preferences\n",
        "\n",
        "5. **Evaluation and Monitoring**\n",
        "   - Loss metrics help track model convergence during training\n",
        "   - Validation loss is an early indicator of overfitting/underfitting\n",
        "   - Different losses provide different insights into model performance\n",
        "\n",
        "Understanding loss functions helps you select the right objective for your specific task and interpret model behavior. The right loss function can mean the difference between a model that learns effectively and one that struggles to capture the patterns in your data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b4948a",
      "metadata": {},
      "source": [
        "## 6. **Gradient**\n",
        "- The **direction and magnitude** for changing weights.\n",
        "- Used in **Gradient Descent** to optimize the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae18e4ad",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "The gradient is at the heart of how neural networks learn. It represents the direction and magnitude of the steepest increase in a function, and its negative points to the direction of steepest decrease. In machine learning, gradients are used to navigate the loss landscape to find the optimal parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17007d60",
      "metadata": {},
      "source": [
        "#### What Are Gradients?\n",
        "\n",
        "A gradient is a vector of partial derivatives that indicates how the output of a function changes when you change its inputs. For a function f(x₁, x₂, ..., xₙ), the gradient ∇f is:\n",
        "\n",
        "∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]\n",
        "\n",
        "For neural networks:\n",
        "- The function is the loss function L\n",
        "- The variables are the model parameters (weights and biases) θ\n",
        "- The gradient ∇L(θ) tells us how the loss would change if we adjust each parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9339631",
      "metadata": {},
      "source": [
        "#### Why Gradients Matter\n",
        "\n",
        "Gradients are essential because:\n",
        "1. They provide the direction to update parameters to minimize loss\n",
        "2. Their magnitude indicates how significant the update should be\n",
        "3. They enable backpropagation, the core algorithm for training neural networks\n",
        "4. They can reveal issues like vanishing or exploding gradients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802daa20",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "Gradient in 2D:\n",
        "\n",
        "         ↑ Loss\n",
        "         │\n",
        "         │     * (Current position)\n",
        "         │    /│\n",
        "         │   / │\n",
        "         │  /  │\n",
        "         │ /   │\n",
        "         │/    │\n",
        "         │     │\n",
        "         │     V Gradient\n",
        "         │      \\\n",
        "         │       \\\n",
        "         │        \\\n",
        "         │         \\\n",
        "         │          \\\n",
        "         │           * (Next position after gradient step)\n",
        "         │\n",
        "         └─────────────────→ Weight\n",
        "```\n",
        "\n",
        "In the above visualization, the gradient points in the direction of steepest increase in the loss. By moving in the negative direction of the gradient (gradient descent), we reduce the loss."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ec5088",
      "metadata": {},
      "source": [
        "#### Computing Gradients in Neural Networks\n",
        "\n",
        "Modern deep learning frameworks use automatic differentiation to compute gradients:\n",
        "\n",
        "1. **Forward Pass**: Compute the output and loss\n",
        "2. **Backward Pass (Backpropagation)**: \n",
        "   - Start from the loss and move backward through the network\n",
        "   - Apply the chain rule to compute gradients for each parameter\n",
        "   - Accumulate gradients layer by layer\n",
        "\n",
        "The chain rule is fundamental to backpropagation. For a composite function f(g(x)), the derivative is:\n",
        "f'(g(x)) × g'(x)\n",
        "\n",
        "For deep networks with many layers, this extends to multiple nested functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee521247",
      "metadata": {},
      "source": [
        "#### Gradient Descent Algorithms\n",
        "\n",
        "Several optimization algorithms use gradients to update parameters:\n",
        "\n",
        "1. **Vanilla Gradient Descent**: \n",
        "   - θ_new = θ_old - learning_rate × ∇L(θ_old)\n",
        "   - Updates using the full dataset gradient\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD)**:\n",
        "   - Updates based on a single example's gradient\n",
        "   - Faster but noisier updates\n",
        "\n",
        "3. **Mini-batch SGD**:\n",
        "   - Uses a small batch of examples\n",
        "   - Balance between speed and stability\n",
        "\n",
        "4. **Advanced Optimizers**:\n",
        "   - Momentum: Adds a velocity term to continue moving in consistent directions\n",
        "   - RMSprop: Adapts learning rates based on gradient history\n",
        "   - Adam: Combines momentum and adaptive learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3995bc35",
      "metadata": {},
      "source": [
        "#### Challenges with Gradients\n",
        "\n",
        "Several problems can affect gradients during training:\n",
        "\n",
        "1. **Vanishing Gradients**:\n",
        "   - Gradients become extremely small as they propagate backward\n",
        "   - Early layers learn very slowly or not at all\n",
        "   - Solutions: ReLU activations, skip connections, batch normalization\n",
        "\n",
        "2. **Exploding Gradients**:\n",
        "   - Gradients become extremely large\n",
        "   - Causes unstable training with huge parameter updates\n",
        "   - Solutions: Gradient clipping, weight regularization\n",
        "\n",
        "3. **Saddle Points**:\n",
        "   - Flat regions where gradient is zero in some directions but not all\n",
        "   - Can slow down training significantly\n",
        "   - Solutions: Momentum, adaptive learning rate methods\n",
        "\n",
        "4. **Local Minima**:\n",
        "   - Points where gradient is zero in all directions but not the global minimum\n",
        "   - More common issue in theory than in high-dimensional practical cases\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9bda00",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 1. Simple Gradient Computation\n",
        "def simple_gradient_example():\n",
        "    # Create a tensor with requires_grad=True to track gradients\n",
        "    x = torch.tensor([2.0], requires_grad=True)\n",
        "    y = torch.tensor([3.0], requires_grad=True)\n",
        "    \n",
        "    # Define a simple function: f(x, y) = x^2 + 2*y^2\n",
        "    z = x**2 + 2*y**2\n",
        "    \n",
        "    # Compute gradients\n",
        "    z.backward()\n",
        "    \n",
        "    # Access the gradients\n",
        "    dx = x.grad.item()  # df/dx = 2x = 2*2 = 4\n",
        "    dy = y.grad.item()  # df/dy = 4y = 4*3 = 12\n",
        "    \n",
        "    print(f\"Function: f(x, y) = x^2 + 2*y^2\")\n",
        "    print(f\"At point x={x.item()}, y={y.item()}\")\n",
        "    print(f\"Gradient df/dx = {dx}\")\n",
        "    print(f\"Gradient df/dy = {dy}\")\n",
        "    print(f\"The gradient vector is [{dx}, {dy}]\")\n",
        "    \n",
        "    # Visualize the function and gradient\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # 3D surface\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    x_range = np.linspace(-3, 3, 50)\n",
        "    y_range = np.linspace(-3, 3, 50)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = X**2 + 2*Y**2\n",
        "    \n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\n",
        "    ax1.set_xlabel('X')\n",
        "    ax1.set_ylabel('Y')\n",
        "    ax1.set_zlabel('f(X, Y)')\n",
        "    ax1.set_title('Function: f(x, y) = x^2 + 2*y^2')\n",
        "    \n",
        "    # Plot the current point\n",
        "    ax1.scatter([x.item()], [y.item()], [z.item()], color='red', s=50)\n",
        "    \n",
        "    # Contour plot with gradient\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    contour = ax2.contour(X, Y, Z, 20, cmap='viridis')\n",
        "    ax2.set_xlabel('X')\n",
        "    ax2.set_ylabel('Y')\n",
        "    ax2.set_title('Contour Plot with Gradient')\n",
        "    \n",
        "    # Plot the current point\n",
        "    ax2.scatter(x.item(), y.item(), color='red', s=50, label='Current Point')\n",
        "    \n",
        "    # Plot the gradient vector\n",
        "    arrow_length = 0.5\n",
        "    ax2.arrow(x.item(), y.item(), \n",
        "              arrow_length * dx / np.sqrt(dx**2 + dy**2), \n",
        "              arrow_length * dy / np.sqrt(dx**2 + dy**2),\n",
        "              head_width=0.2, head_length=0.2, fc='blue', ec='blue', label='Gradient Direction')\n",
        "    \n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 2. Gradient Descent Visualization\n",
        "def gradient_descent_visualization():\n",
        "    # Function to optimize: f(x, y) = x^2 + 2*y^2\n",
        "    def f(x, y):\n",
        "        return x**2 + 2*y**2\n",
        "    \n",
        "    # Gradient of f: [df/dx, df/dy] = [2x, 4y]\n",
        "    def grad_f(x, y):\n",
        "        return np.array([2*x, 4*y])\n",
        "    \n",
        "    # Initial point\n",
        "    x, y = 2.0, 2.0\n",
        "    learning_rate = 0.1\n",
        "    num_iterations = 10\n",
        "    \n",
        "    # Track trajectory\n",
        "    trajectory = [(x, y, f(x, y))]\n",
        "    \n",
        "    # Perform gradient descent\n",
        "    for i in range(num_iterations):\n",
        "        gradient = grad_f(x, y)\n",
        "        x = x - learning_rate * gradient[0]\n",
        "        y = y - learning_rate * gradient[1]\n",
        "        trajectory.append((x, y, f(x, y)))\n",
        "        print(f\"Iteration {i+1}: x={x:.4f}, y={y:.4f}, f(x,y)={f(x, y):.4f}\")\n",
        "    \n",
        "    # Convert trajectory to arrays for plotting\n",
        "    trajectory = np.array(trajectory)\n",
        "    \n",
        "    # Visualize\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # 3D surface\n",
        "    ax1 = fig.add_subplot(121, projection='3d')\n",
        "    x_range = np.linspace(-3, 3, 50)\n",
        "    y_range = np.linspace(-3, 3, 50)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "    Z = X**2 + 2*Y**2\n",
        "    \n",
        "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.6)\n",
        "    ax1.set_xlabel('X')\n",
        "    ax1.set_ylabel('Y')\n",
        "    ax1.set_zlabel('f(X, Y)')\n",
        "    ax1.set_title('Gradient Descent Trajectory in 3D')\n",
        "    \n",
        "    # Plot trajectory\n",
        "    ax1.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], 'ro-', linewidth=2, markersize=5)\n",
        "    \n",
        "    # Contour plot\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    contour = ax2.contour(X, Y, Z, 20, cmap='viridis')\n",
        "    ax2.set_xlabel('X')\n",
        "    ax2.set_ylabel('Y')\n",
        "    ax2.set_title('Gradient Descent Trajectory in 2D')\n",
        "    \n",
        "    # Plot trajectory\n",
        "    ax2.plot(trajectory[:, 0], trajectory[:, 1], 'ro-', linewidth=2, markersize=5)\n",
        "    \n",
        "    # Mark start and end points\n",
        "    ax2.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=8, label='Start')\n",
        "    ax2.plot(trajectory[-1, 0], trajectory[-1, 1], 'bo', markersize=8, label='End')\n",
        "    \n",
        "    ax2.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 3. Neural Network Gradient Example\n",
        "def neural_network_gradient_example():\n",
        "    # Create a simple neural network\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(2, 3),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(3, 1)\n",
        "    )\n",
        "    \n",
        "    # Input data\n",
        "    x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
        "    target = torch.tensor([[3.0]])\n",
        "    \n",
        "    # Forward pass\n",
        "    output = model(x)\n",
        "    loss = nn.MSELoss()(output, target)\n",
        "    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Print gradients for each parameter\n",
        "    print(\"Neural Network Gradients:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name} - shape: {param.shape}, gradient magnitude: {param.grad.abs().mean().item():.6f}\")\n",
        "    \n",
        "    # Visualize parameter gradients\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    # Extract gradients\n",
        "    grad_data = []\n",
        "    param_names = []\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        grad_flat = param.grad.flatten().detach().numpy()\n",
        "        for i, g in enumerate(grad_flat):\n",
        "            grad_data.append(abs(g))\n",
        "            param_names.append(f\"{name}_{i}\")\n",
        "    \n",
        "    # Create gradient magnitude plot\n",
        "    plt.bar(range(len(grad_data)), grad_data)\n",
        "    plt.xlabel('Parameter Index')\n",
        "    plt.ylabel('Gradient Magnitude (absolute value)')\n",
        "    plt.title('Gradient Magnitudes Across Neural Network Parameters')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add vertical lines separating layers\n",
        "    param_counts = [6, 3, 3, 1]  # 2x3 weights, 3 biases, 3x1 weights, 1 bias\n",
        "    separators = [sum(param_counts[:i]) - 0.5 for i in range(1, len(param_counts))]\n",
        "    \n",
        "    for sep in separators:\n",
        "        plt.axvline(x=sep, color='r', linestyle='--', alpha=0.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4. Gradient Problems Demonstration\n",
        "def gradient_problems_demonstration():\n",
        "    # Create models with different activation functions\n",
        "    def create_deep_model(activation):\n",
        "        layers = []\n",
        "        for i in range(20):  # A very deep model to demonstrate vanishing gradients\n",
        "            layers.append(nn.Linear(10, 10))\n",
        "            if activation == 'sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation == 'relu':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation == 'tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "        layers.append(nn.Linear(10, 1))\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    # Create models with different activations\n",
        "    models = {\n",
        "        'Sigmoid': create_deep_model('sigmoid'),\n",
        "        'ReLU': create_deep_model('relu'),\n",
        "        'Tanh': create_deep_model('tanh')\n",
        "    }\n",
        "    \n",
        "    # Input data\n",
        "    x = torch.randn(1, 10)\n",
        "    target = torch.tensor([[1.0]])\n",
        "    \n",
        "    # Store gradients for each model\n",
        "    gradient_stats = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        # Forward pass\n",
        "        output = model(x)\n",
        "        loss = nn.MSELoss()(output, target)\n",
        "        \n",
        "        # Backward pass\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Collect gradient statistics by layer\n",
        "        gradient_magnitudes = []\n",
        "        for i, (param_name, param) in enumerate(model.named_parameters()):\n",
        "            if 'weight' in param_name:  # Only look at weights, not biases\n",
        "                layer_idx = i // 2  # Each layer has weights and biases\n",
        "                grad_mag = param.grad.abs().mean().item()\n",
        "                gradient_magnitudes.append((layer_idx, grad_mag))\n",
        "        \n",
        "        gradient_stats[name] = gradient_magnitudes\n",
        "    \n",
        "    # Visualize gradient magnitudes by layer depth\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for name, gradients in gradient_stats.items():\n",
        "        layer_indices = [g[0] for g in gradients]\n",
        "        magnitudes = [g[1] for g in gradients]\n",
        "        plt.semilogy(layer_indices, magnitudes, 'o-', label=name)\n",
        "    \n",
        "    plt.xlabel('Layer Index')\n",
        "    plt.ylabel('Average Gradient Magnitude (log scale)')\n",
        "    plt.title('Gradient Magnitudes Across Network Depth')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print analysis\n",
        "    print(\"\\nGradient Analysis:\")\n",
        "    for name, gradients in gradient_stats.items():\n",
        "        first_layer = gradients[0][1]\n",
        "        last_layer = gradients[-1][1]\n",
        "        ratio = last_layer / first_layer if first_layer != 0 else float('inf')\n",
        "        \n",
        "        print(f\"{name} activation:\")\n",
        "        print(f\"  First layer gradient magnitude: {first_layer:.8f}\")\n",
        "        print(f\"  Last layer gradient magnitude: {last_layer:.8f}\")\n",
        "        print(f\"  Ratio (last/first): {ratio:.8f}\")\n",
        "        if ratio < 0.01:\n",
        "            print(\"  VERDICT: Severe vanishing gradient problem!\")\n",
        "        elif ratio < 0.1:\n",
        "            print(\"  VERDICT: Moderate vanishing gradient problem\")\n",
        "        elif ratio > 100:\n",
        "            print(\"  VERDICT: Potential exploding gradient problem!\")\n",
        "        else:\n",
        "            print(\"  VERDICT: Healthy gradient flow\")\n",
        "\n",
        "# Run the examples\n",
        "simple_gradient_example()\n",
        "# gradient_descent_visualization()\n",
        "# neural_network_gradient_example()\n",
        "# gradient_problems_demonstration()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d975278",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Understanding gradients has several critical applications in machine learning:\n",
        "\n",
        "1. **Optimization Strategy Selection**:\n",
        "   - Different problems require different gradient-based optimizers\n",
        "   - Sparse data may benefit from adaptive methods like Adam\n",
        "   - Some problems require second-order methods that use Hessian matrices\n",
        "\n",
        "2. **Training Diagnostics**:\n",
        "   - Monitoring gradient magnitudes helps detect vanishing/exploding gradients\n",
        "   - Histogram of gradients reveals training health\n",
        "   - Plateaus in loss may correlate with small gradients\n",
        "\n",
        "3. **Model Architecture Design**:\n",
        "   - Skip connections in ResNets help gradient flow\n",
        "   - Batch normalization stabilizes gradients\n",
        "   - Activation function choice (ReLU vs sigmoid) affects gradient propagation\n",
        "\n",
        "4. **Transfer Learning**:\n",
        "   - Gradients guide which layers to fine-tune\n",
        "   - Lower gradients in early layers suggest they can remain frozen\n",
        "\n",
        "5. **Neural Architecture Search**:\n",
        "   - Gradient-based NAS uses gradients to optimize architecture choices\n",
        "   - Meta-learning leverages gradients to learn how to learn\n",
        "\n",
        "Mastering gradients is essential for effective deep learning. By understanding how gradients guide parameter updates, you can better diagnose issues, select appropriate optimization techniques, and design models that learn efficiently across their entire architecture.\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16317566",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Simple model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# Forward pass with dummy data\n",
        "x = torch.randn(1, 10)\n",
        "y_true = torch.tensor([[1.0]])\n",
        "y_pred = model(x)\n",
        "\n",
        "# Compute loss\n",
        "loss = nn.MSELoss()(y_pred, y_true)\n",
        "\n",
        "# Compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# Access and print gradients\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name} - gradient shape: {param.grad.shape}\")\n",
        "    print(f\"Sample gradient values: {param.grad[:2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf7525d",
      "metadata": {},
      "source": [
        "## 7. **Training vs Inference**\n",
        "- **Training**: Teaching the model by adjusting weights.\n",
        "- **Inference**: Using the model to generate output (no learning happens)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0f645fb",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "The distinction between training and inference represents two fundamentally different phases in a model's lifecycle. Understanding these phases is crucial for optimizing both model development and deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5497bc80",
      "metadata": {},
      "source": [
        "#### The Training Phase\n",
        "\n",
        "Training is the process where a model learns from data by adjusting its parameters. This involves:\n",
        "\n",
        "1. **Forward Pass**: Computing predictions based on current parameters\n",
        "2. **Loss Calculation**: Measuring error between predictions and ground truth\n",
        "3. **Backward Pass**: Computing gradients of the loss with respect to parameters\n",
        "4. **Parameter Updates**: Adjusting parameters in the direction that reduces loss\n",
        "\n",
        "Key characteristics of the training phase:\n",
        "- Requires both input data and target labels/values\n",
        "- Computationally intensive (both forward and backward passes)\n",
        "- Memory-intensive (storing intermediate activations for backpropagation)\n",
        "- Usually performed on specialized hardware (GPUs, TPUs)\n",
        "- Uses specific model settings (dropout active, batch normalization in training mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc0509f9",
      "metadata": {},
      "source": [
        "#### The Inference Phase\n",
        "\n",
        "Inference is the process where a trained model makes predictions on new data. This involves:\n",
        "\n",
        "1. **Forward Pass Only**: Computing outputs based on fixed learned parameters\n",
        "2. **No Parameter Updates**: The model remains unchanged\n",
        "\n",
        "Key characteristics of the inference phase:\n",
        "- Requires only input data (no labels/targets needed)\n",
        "- Computationally less intensive (only forward pass)\n",
        "- Lower memory requirements (no need to store information for backpropagation)\n",
        "- Can be performed on various hardware (CPUs, mobile devices, edge devices)\n",
        "- Uses different model settings (dropout inactive, batch normalization in evaluation mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7adb0134",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "TRAINING:\n",
        "                                  │\n",
        "   ┌────────────┐   ┌──────────┐  │   ┌──────────┐   ┌─────────────┐\n",
        "   │            │   │          │  │   │          │   │             │\n",
        "   │ Input Data ├──►│ Model    ├──┼──►│ Loss     ├──►│ Optimizer   │\n",
        "   │            │   │ Forward  │  │   │ Function │   │             │\n",
        "   └────────────┘   └──────────┘  │   └──────────┘   └──────┬──────┘\n",
        "         ▲                        │                         │\n",
        "         │           ┌────────────┤                         │\n",
        "         │           │            │                         │\n",
        "         │           │  Target    │                         │\n",
        "         │           │  Labels    │                         │\n",
        "         │           │            │                         │\n",
        "         │           └────────────┘                         │\n",
        "         │                                                  │\n",
        "         └──────────────────────────────────────────────────┘\n",
        "                     Parameter Updates\n",
        "\n",
        "INFERENCE:\n",
        "   ┌────────────┐   ┌───────────┐   ┌────────────┐\n",
        "   │            │   │           │   │            │\n",
        "   │ Input Data ├──►│ Model     ├──►│ Prediction │\n",
        "   │            │   │ Forward   │   │            │\n",
        "   └────────────┘   └───────────┘   └────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d49b2838",
      "metadata": {},
      "source": [
        "#### Key Differences in Implementation\n",
        "\n",
        "Several model components behave differently during training versus inference:\n",
        "\n",
        "1. **Dropout Layers**:\n",
        "   - Training: Randomly \"drop\" (set to zero) a fraction of activations to prevent overfitting\n",
        "   - Inference: No neurons are dropped; instead, activations are scaled appropriately\n",
        "\n",
        "2. **Batch Normalization**:\n",
        "   - Training: Normalizes using batch statistics and updates running statistics\n",
        "   - Inference: Uses pre-computed running statistics for normalization\n",
        "\n",
        "3. **Data Augmentation**:\n",
        "   - Training: Applies random transformations to increase data diversity\n",
        "   - Inference: Typically not applied (or uses fixed augmentation strategies)\n",
        "\n",
        "4. **Gradient Computation**:\n",
        "   - Training: Requires gradient computation and tracking\n",
        "   - Inference: Can disable gradient tracking for efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53795e8c",
      "metadata": {},
      "source": [
        "#### Performance Optimization\n",
        "\n",
        "The distinct characteristics of training and inference lead to different optimization strategies:\n",
        "\n",
        "**Training Optimization**:\n",
        "- Model parallelism and distributed training\n",
        "- Mixed precision training (e.g., fp16)\n",
        "- Gradient accumulation for large batches\n",
        "- Optimized data pipelines to keep GPUs fed\n",
        "\n",
        "**Inference Optimization**:\n",
        "- Model quantization (int8, int4)\n",
        "- Model pruning (removing unnecessary connections)\n",
        "- Knowledge distillation (transferring to smaller models)\n",
        "- Model compilation and fusion of operations\n",
        "- Batching for throughput optimization\n",
        "- Caching for repeatedly used inputs\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ffeea8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define a simple model with components that behave differently in training vs inference\n",
        "class ExampleModel(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_size=256, output_size=10, dropout_rate=0.5):\n",
        "        super(ExampleModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # First layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to demonstrate the difference in behavior and performance\n",
        "def compare_training_inference():\n",
        "    # Create model and sample data\n",
        "    model = ExampleModel()\n",
        "    \n",
        "    # Create random batch of data\n",
        "    batch_size = 64\n",
        "    input_data = torch.randn(batch_size, 784)\n",
        "    target_data = torch.randint(0, 10, (batch_size,))\n",
        "    \n",
        "    # Setup for training\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "    \n",
        "    # 1. Demonstrate the difference in computation\n",
        "    print(\"TRAINING MODE vs INFERENCE MODE\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Training behavior\n",
        "    model.train()\n",
        "    with torch.enable_grad():\n",
        "        # Time the forward and backward passes\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Forward pass (with dropout active)\n",
        "        output_train = model(input_data)\n",
        "        loss = criterion(output_train, target_data)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_time = time.time() - start_time\n",
        "    \n",
        "    # Inference behavior\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Time just the forward pass\n",
        "        start_time = time.time()\n",
        "        output_eval = model(input_data)\n",
        "        inference_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"Training time (forward + backward): {train_time:.6f} seconds\")\n",
        "    print(f\"Inference time (forward only): {inference_time:.6f} seconds\")\n",
        "    print(f\"Ratio (Training/Inference): {train_time/inference_time:.2f}x\")\n",
        "    \n",
        "    # 2. Demonstrate dropout behavior\n",
        "    print(\"\\nDROPOUT BEHAVIOR\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create a simple model with visible dropout for demonstration\n",
        "    class DropoutDemoModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(DropoutDemoModel, self).__init__()\n",
        "            self.dropout = nn.Dropout(p=0.5)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.dropout(x)\n",
        "    \n",
        "    dropout_model = DropoutDemoModel()\n",
        "    demo_input = torch.ones(1, 10)  # All ones for clear visualization\n",
        "    \n",
        "    # Behavior in training mode - neurons are dropped\n",
        "    dropout_model.train()\n",
        "    train_outputs = [dropout_model(demo_input).detach().numpy() for _ in range(5)]\n",
        "    \n",
        "    # Behavior in eval mode - no neurons are dropped, output is scaled\n",
        "    dropout_model.eval()\n",
        "    eval_outputs = [dropout_model(demo_input).detach().numpy() for _ in range(5)]\n",
        "    \n",
        "    print(\"Training Mode (with dropout):\")\n",
        "    for i, out in enumerate(train_outputs):\n",
        "        print(f\"Run {i+1}: {out[0][:5]}...\")  # Show first 5 values\n",
        "    \n",
        "    print(\"\\nEvaluation Mode (no dropout):\")\n",
        "    for i, out in enumerate(eval_outputs):\n",
        "        print(f\"Run {i+1}: {out[0][:5]}...\")  # Show first 5 values\n",
        "    \n",
        "    # 3. Demonstrate BatchNorm behavior\n",
        "    print(\"\\nBATCH NORMALIZATION BEHAVIOR\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Create model with batch norm\n",
        "    class BNDemoModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(BNDemoModel, self).__init__()\n",
        "            self.bn = nn.BatchNorm1d(10)\n",
        "        \n",
        "        def forward(self, x):\n",
        "            return self.bn(x)\n",
        "    \n",
        "    bn_model = BNDemoModel()\n",
        "    \n",
        "    # Generate data with different distributions\n",
        "    data1 = torch.randn(100, 10) * 5 + 10  # Mean=10, Std=5\n",
        "    data2 = torch.randn(100, 10) * 2 - 5   # Mean=-5, Std=2\n",
        "    \n",
        "    # Training mode - updates running statistics\n",
        "    bn_model.train()\n",
        "    output_train1 = bn_model(data1)\n",
        "    print(f\"Training - Batch 1 - Input Mean: {data1.mean().item():.2f}, Output Mean: {output_train1.mean().item():.2f}\")\n",
        "    print(f\"Training - Batch 1 - Input Std: {data1.std().item():.2f}, Output Std: {output_train1.std().item():.2f}\")\n",
        "    \n",
        "    output_train2 = bn_model(data2)\n",
        "    print(f\"Training - Batch 2 - Input Mean: {data2.mean().item():.2f}, Output Mean: {output_train2.mean().item():.2f}\")\n",
        "    print(f\"Training - Batch 2 - Input Std: {data2.std().item():.2f}, Output Std: {output_train2.std().item():.2f}\")\n",
        "    \n",
        "    # Get the running statistics after training\n",
        "    running_mean = bn_model.bn.running_mean.detach().numpy()\n",
        "    running_var = bn_model.bn.running_var.detach().numpy()\n",
        "    \n",
        "    print(f\"\\nRunning Mean after training: {running_mean[:3]}...\")\n",
        "    print(f\"Running Variance after training: {running_var[:3]}...\")\n",
        "    \n",
        "    # Evaluation mode - uses running statistics\n",
        "    bn_model.eval()\n",
        "    output_eval1 = bn_model(data1)\n",
        "    output_eval2 = bn_model(data2)\n",
        "    \n",
        "    print(f\"\\nEvaluation - Batch 1 - Output Mean: {output_eval1.mean().item():.2f}\")\n",
        "    print(f\"Evaluation - Batch 2 - Output Mean: {output_eval2.mean().item():.2f}\")\n",
        "    \n",
        "    # 4. Memory Usage Comparison\n",
        "    print(\"\\nMEMORY USAGE\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Reset model\n",
        "    model = ExampleModel()\n",
        "    \n",
        "    # Create large batch for demonstration\n",
        "    large_batch = torch.randn(512, 784)\n",
        "    \n",
        "    # Memory usage in training mode\n",
        "    model.train()\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory if available\n",
        "    \n",
        "    # Run training mode with gradient tracking\n",
        "    optimizer.zero_grad()\n",
        "    torch.autograd.set_grad_enabled(True)\n",
        "    output = model(large_batch)\n",
        "    dummy_loss = output.sum()\n",
        "    dummy_loss.backward()\n",
        "    \n",
        "    # Memory usage in inference mode\n",
        "    model.eval()\n",
        "    torch.cuda.empty_cache()  # Clear GPU memory if available\n",
        "    \n",
        "    # Run inference mode without gradient tracking\n",
        "    with torch.no_grad():\n",
        "        output = model(large_batch)\n",
        "    \n",
        "    print(\"Training mode requires more memory due to:\")\n",
        "    print(\"1. Storage of intermediate activations for backpropagation\")\n",
        "    print(\"2. Gradient buffers for each parameter\")\n",
        "    print(\"3. Optimizer state (momentum, etc.)\")\n",
        "    print(\"\\nInference mode uses less memory due to:\")\n",
        "    print(\"1. No need to store activation history\")\n",
        "    print(\"2. No gradient storage\")\n",
        "    print(\"3. No optimizer state\")\n",
        "\n",
        "# Run the comparisons\n",
        "compare_training_inference()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68495f3",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "The training-inference distinction has crucial implications for real-world ML applications:\n",
        "\n",
        "1. **Model Deployment Strategies**:\n",
        "   - Training happens in data centers with powerful GPUs\n",
        "   - Inference may occur on consumer devices, in browsers, or at the edge\n",
        "   - Different hardware targets require different optimization approaches\n",
        "\n",
        "2. **Model Serving Infrastructure**:\n",
        "   - Dedicated hardware configurations for training (multi-GPU) vs. inference (CPU, specialized accelerators)\n",
        "   - Auto-scaling for inference to handle variable load\n",
        "   - Batching strategies to maximize throughput\n",
        "\n",
        "3. **Performance Monitoring**:\n",
        "   - Training: Monitor loss curves, convergence, gradient statistics\n",
        "   - Inference: Monitor latency, throughput, accuracy drift\n",
        "\n",
        "4. **Resource Management**:\n",
        "   - Training: Long-running processes with high resource utilization\n",
        "   - Inference: Often needs to be low-latency with consistent performance\n",
        "\n",
        "5. **Error Handling**:\n",
        "   - Training: Can often continue despite some errors in data\n",
        "   - Inference: Requires robust error handling for production reliability\n",
        "\n",
        "6. **Versioning and Reproducibility**:\n",
        "   - Training: Seed setting, deterministic operations for reproducibility\n",
        "   - Inference: Model versioning, model registry, and deployment tracking\n",
        "\n",
        "Understanding the distinctions between training and inference is essential for the entire ML lifecycle, from research to production deployment. This knowledge helps in designing models that both train efficiently and serve effectively in production environments.\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "614640d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define model\n",
        "model = nn.Linear(10, 1)\n",
        "\n",
        "# TRAINING MODE\n",
        "model.train()  # Set to training mode\n",
        "x = torch.randn(5, 10)\n",
        "y_true = torch.randn(5, 1)\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(3):\n",
        "    # Forward pass\n",
        "    y_pred = model(x)\n",
        "    loss = nn.MSELoss()(y_pred, y_true)\n",
        "    \n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# INFERENCE MODE\n",
        "model.eval()  # Set to evaluation mode\n",
        "with torch.no_grad():  # No gradients needed for inference\n",
        "    x_test = torch.randn(2, 10)\n",
        "    predictions = model(x_test)\n",
        "    print(f\"Predictions: {predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f26a415",
      "metadata": {},
      "source": [
        "## 8. **Prompt**\n",
        "- The input you give to a language model.\n",
        "- Example: `\"Translate to French: Hello\"` is a prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "246727e1",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "A prompt is the input text provided to a language model to elicit a desired response. While conceptually simple, effective prompting has become a sophisticated field with significant impact on model performance, often referred to as \"prompt engineering.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79e95ef2",
      "metadata": {},
      "source": [
        "#### The Anatomy of a Prompt\n",
        "\n",
        "At its core, a prompt consists of:\n",
        "\n",
        "1. **Instructions**: Directives telling the model what task to perform\n",
        "2. **Context**: Background information to inform the model's response\n",
        "3. **Input Data**: Specific information the model needs to process\n",
        "4. **Output Format**: Optional guidance on how the response should be structured\n",
        "\n",
        "Different prompt structures serve various purposes:\n",
        "\n",
        "```\n",
        "Basic Prompt:\n",
        "\"Translate 'hello' to French.\"\n",
        "\n",
        "Instruction with Examples (Few-shot):\n",
        "\"Translate English to French:\n",
        "English: hello\n",
        "French: bonjour\n",
        "English: thank you\n",
        "French: merci\n",
        "English: goodbye\n",
        "French: ?\"\n",
        "\n",
        "System & User Messages:\n",
        "System: You are a helpful French translator.\n",
        "User: How do I say \"Where is the train station?\" in French?\n",
        "\n",
        "Chain-of-thought Prompt:\n",
        "\"Solve this math problem step by step:\n",
        "If 3x + 7 = 22, what is the value of x?\"\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db5196c4",
      "metadata": {},
      "source": [
        "#### Prompt Engineering Techniques\n",
        "\n",
        "Prompt engineering has evolved various approaches to improve model performance:\n",
        "\n",
        "1. **Zero-shot Prompting**: Asking the model to perform a task without examples\n",
        "   - Example: \"Classify this review as positive or negative: 'I loved this movie!'\"\n",
        "\n",
        "2. **Few-shot Prompting**: Providing a few examples before asking the model to perform a task\n",
        "   - Example: \"Classify these reviews:\n",
        "     'Amazing product!' -> Positive\n",
        "     'Terrible experience.' -> Negative\n",
        "     'It was okay I guess.' -> Neutral\n",
        "     'I can't recommend this enough!' -> ?\"\n",
        "\n",
        "3. **Chain-of-Thought (CoT)**: Encouraging the model to reason step-by-step\n",
        "   - Example: \"Think through this step by step: If I have 5 apples and give 2 to my friend, then buy 3 more, how many apples do I have?\"\n",
        "\n",
        "4. **Self-Consistency**: Generating multiple reasoning paths and taking the majority answer\n",
        "   - Example: Generate multiple solution attempts for a math problem and choose the most common answer\n",
        "\n",
        "5. **ReAct**: Combining reasoning and action to interact with external tools\n",
        "   - Example: \"To answer this question, first search for relevant information, then analyze the results.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8673ffa3",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "PROMPT ARCHITECTURE:\n",
        "\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ System Message (Optional)                                       │\n",
        "│ \"You are a helpful, accurate, and concise assistant.\"           │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                              ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ Context (Optional)                                              │\n",
        "│ \"The following is an excerpt from a financial report...\"        │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                              ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ Few-shot Examples (Optional)                                    │\n",
        "│ \"Example 1: Input... Output...\"                                 │\n",
        "│ \"Example 2: Input... Output...\"                                 │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                              ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ Task Instruction                                                │\n",
        "│ \"Summarize the key financial metrics in this report.\"           │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                              ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ Input Data                                                      │\n",
        "│ \"Revenue increased by 12% year-over-year...\"                    │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "                              ↓\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│ Output Format (Optional)                                        │\n",
        "│ \"Format your answer as a bullet-point list.\"                    │\n",
        "└─────────────────────────────────────────────────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb41a8c",
      "metadata": {},
      "source": [
        "#### Prompt Design Principles\n",
        "\n",
        "Effective prompts typically follow these principles:\n",
        "\n",
        "1. **Clarity**: Provide clear, unambiguous instructions\n",
        "2. **Specificity**: Be specific about what you're asking for\n",
        "3. **Relevance**: Include only relevant context for the task\n",
        "4. **Structure**: Organize the prompt logically\n",
        "5. **Constraints**: Set appropriate constraints or parameters\n",
        "6. **Examples**: Provide examples for complex tasks\n",
        "7. **Iterative Refinement**: Improve prompts based on model responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b6b2a2",
      "metadata": {},
      "source": [
        "#### Advanced Prompt Techniques\n",
        "\n",
        "Beyond basic prompting, advanced techniques can significantly improve model outputs:\n",
        "\n",
        "1. **Role Prompting**: Assigning the model a specific role\n",
        "   - Example: \"As an expert physicist, explain the concept of quantum entanglement.\"\n",
        "\n",
        "2. **Format Specification**: Explicitly defining the desired output format\n",
        "   - Example: \"Respond with a JSON object with fields 'name', 'price', and 'availability'.\"\n",
        "\n",
        "3. **Temperature Control**: Adjusting the randomness of responses\n",
        "   - Low temperature: More deterministic, focused responses\n",
        "   - High temperature: More creative, diverse responses\n",
        "\n",
        "4. **Prompt Chaining**: Breaking complex tasks into sequential prompts\n",
        "   - Example: First generate an outline, then expand each section\n",
        "\n",
        "5. **Retrieval-Augmented Generation (RAG)**: Enhancing prompts with information retrieved from external sources\n",
        "   - Example: Including relevant passages from a knowledge base in the prompt\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db5a33d8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Demonstrate different prompt types\n",
        "def demonstrate_prompt_types():\n",
        "    prompts = {\n",
        "        \"Zero-shot\": \"Explain the concept of machine learning in simple terms.\",\n",
        "        \"Few-shot\": \"Translate English to French:\\nEnglish: hello\\nFrench: bonjour\\nEnglish: thank you\\nFrench: merci\\nEnglish: goodbye\\nFrench:\",\n",
        "        \"Chain-of-thought\": \"Think step by step: If it takes 5 machines 5 minutes to make 5 widgets, how long would it take 100 machines to make 100 widgets?\",\n",
        "        \"Role-based\": \"As an experienced chef, provide a recipe for chocolate chip cookies.\",\n",
        "        \"Format-specific\": \"Generate a JSON object for a product with the following properties: name, price, and description.\"\n",
        "    }\n",
        "    \n",
        "    # Generate responses for each prompt type\n",
        "    responses = {}\n",
        "    for name, prompt in prompts.items():\n",
        "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        \n",
        "        # Generate response with different parameters based on the prompt type\n",
        "        if name == \"Chain-of-thought\":\n",
        "            # For CoT, we want more detailed reasoning, so use higher max_length\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=150,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        elif name == \"Format-specific\":\n",
        "            # For format-specific, we want more deterministic output\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.3,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        else:\n",
        "            # Default parameters\n",
        "            output = model.generate(\n",
        "                input_ids,\n",
        "                max_length=100,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.7,\n",
        "                no_repeat_ngram_size=2\n",
        "            )\n",
        "        \n",
        "        # Decode and store response\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        responses[name] = response\n",
        "    \n",
        "    # Print each prompt and its response\n",
        "    for name, prompt in prompts.items():\n",
        "        print(f\"\\n{'-'*20} {name} PROMPT {'-'*20}\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"\\nResponse: {responses[name]}\")\n",
        "        print('-' * 60)\n",
        "\n",
        "# Demonstrate temperature effect on generation\n",
        "def demonstrate_temperature_effect():\n",
        "    base_prompt = \"Write a short story about a robot who discovers emotions.\"\n",
        "    temperatures = [0.2, 0.5, 0.8, 1.2]\n",
        "    \n",
        "    responses = []\n",
        "    for temp in temperatures:\n",
        "        input_ids = tokenizer.encode(base_prompt, return_tensors=\"pt\")\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_length=100,\n",
        "            num_return_sequences=1,\n",
        "            temperature=temp,\n",
        "            no_repeat_ngram_size=2\n",
        "        )\n",
        "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        responses.append(response)\n",
        "    \n",
        "    # Print responses at different temperatures\n",
        "    print(\"\\nEFFECT OF TEMPERATURE ON GENERATION\")\n",
        "    print('-' * 60)\n",
        "    for i, temp in enumerate(temperatures):\n",
        "        print(f\"Temperature = {temp}\")\n",
        "        print(f\"Response: {responses[i][:150]}...\")  # Print first 150 chars\n",
        "        print('-' * 40)\n",
        "    \n",
        "    # Measure lexical diversity at different temperatures\n",
        "    def lexical_diversity(text):\n",
        "        # Simple measure: unique words / total words\n",
        "        words = text.lower().split()\n",
        "        return len(set(words)) / len(words) if words else 0\n",
        "    \n",
        "    diversity_scores = [lexical_diversity(resp) for resp in responses]\n",
        "    \n",
        "    # Plot diversity vs temperature\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(temperatures, diversity_scores, 'o-', linewidth=2, markersize=10)\n",
        "    plt.xlabel('Temperature')\n",
        "    plt.ylabel('Lexical Diversity (unique words / total words)')\n",
        "    plt.title('Effect of Temperature on Output Diversity')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Analyze token probabilities in prompt completion\n",
        "def analyze_token_probabilities():\n",
        "    prompt = \"The capital of France is\"\n",
        "    \n",
        "    # Tokenize input\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        logits = outputs.logits\n",
        "    \n",
        "    # Get probabilities for next token\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    next_token_probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
        "    \n",
        "    # Get top 10 most likely next tokens\n",
        "    topk_probs, topk_indices = torch.topk(next_token_probs, 10)\n",
        "    \n",
        "    # Convert to words and probabilities\n",
        "    topk_tokens = [tokenizer.decode([idx.item()]) for idx in topk_indices]\n",
        "    topk_probs = topk_probs.numpy()\n",
        "    \n",
        "    # Print results\n",
        "    print(\"\\nNEXT TOKEN PREDICTION ANALYSIS\")\n",
        "    print('-' * 60)\n",
        "    print(f\"Prompt: \\\"{prompt}\\\"\")\n",
        "    print(\"\\nTop 10 most likely next tokens:\")\n",
        "    for token, prob in zip(topk_tokens, topk_probs):\n",
        "        print(f\"{token}: {prob:.4f}\")\n",
        "    \n",
        "    # Plot token probabilities\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(topk_tokens)), topk_probs)\n",
        "    plt.xticks(range(len(topk_tokens)), topk_tokens, rotation=45)\n",
        "    plt.xlabel('Token')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.title('Next Token Probability Distribution')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run the demonstrations\n",
        "# demonstrate_prompt_types()\n",
        "# demonstrate_temperature_effect()\n",
        "# analyze_token_probabilities()\n",
        "\n",
        "# Simple prompt example\n",
        "def simple_prompt_example():\n",
        "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Create a prompt\n",
        "    prompt = \"Translate to French: Hello\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "    )\n",
        "\n",
        "    # Decode and print response\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Response: {response}\")\n",
        "\n",
        "# Run the simple example\n",
        "simple_prompt_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420b486c",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Effective prompting has numerous practical applications across industries:\n",
        "\n",
        "1. **Content Creation**:\n",
        "   - Writing assistance, summarization, translation\n",
        "   - Creative content generation (stories, poems, marketing copy)\n",
        "   - Format conversion (e.g., bullet points to paragraphs)\n",
        "\n",
        "2. **Data Analysis**:\n",
        "   - Extracting structured data from unstructured text\n",
        "   - Analyzing sentiment in customer feedback\n",
        "   - Generating insights from reports\n",
        "\n",
        "3. **Education**:\n",
        "   - Personalized tutoring and explanations\n",
        "   - Question answering in various domains\n",
        "   - Generating practice problems and quizzes\n",
        "\n",
        "4. **Software Development**:\n",
        "   - Code generation and explanation\n",
        "   - Debugging assistance\n",
        "   - Documentation writing\n",
        "\n",
        "5. **Business Operations**:\n",
        "   - Automated email drafting and responses\n",
        "   - Meeting summarization\n",
        "   - Report generation from data\n",
        "\n",
        "6. **Research**:\n",
        "   - Literature review assistance\n",
        "   - Hypothesis generation\n",
        "   - Experimental design suggestions\n",
        "\n",
        "Prompt engineering is a critical skill that bridges the gap between language model capabilities and practical applications. As models become more powerful, the art of crafting effective prompts becomes increasingly valuable for extracting their full potential.\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8a985f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create a prompt\n",
        "prompt = \"Translate to French: Hello\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate response\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.7,\n",
        ")\n",
        "\n",
        "# Decode and print response\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46742726",
      "metadata": {},
      "source": [
        "## 9. **Context Window (Context Length)**\n",
        "- The number of tokens the model can \"see\" at once.\n",
        "- E.g., GPT-3 has a 2048-token context window.\n",
        "- Long prompts or conversations may be **truncated**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ff36951",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "The context window is a fundamental constraint in language models that defines the maximum span of text a model can process in a single operation. Understanding context windows is critical for effectively working with language models and designing applications around their limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92145838",
      "metadata": {},
      "source": [
        "#### What is a Context Window?\n",
        "\n",
        "A context window represents the \"memory\" of a language model—the maximum number of tokens it can consider when generating a response. This includes:\n",
        "\n",
        "1. **The prompt**: Your instructions and input to the model\n",
        "2. **Previous exchanges**: In a conversation, earlier messages\n",
        "3. **Generated text**: Tokens the model has already produced in its response\n",
        "\n",
        "The context window acts like a sliding window that determines what information is available to the model at any given time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b55a69ac",
      "metadata": {},
      "source": [
        "#### Why Context Windows Matter\n",
        "\n",
        "Context windows are important because they:\n",
        "1. **Limit the information available** to the model at once\n",
        "2. **Define memory constraints** for sequential processing\n",
        "3. **Set boundaries for attention mechanisms** in transformers\n",
        "4. **Impact computational requirements** (larger windows need more compute)\n",
        "5. **Determine effective application design** around model limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4bf2c2b",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "Context Window Visualization:\n",
        "\n",
        "Token Position:  0   1   2   3   4   5   6   7   8   9  10  11  ...  N-1\n",
        "                ┌───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬───┬─────┬───┐\n",
        "Token Content:  │ I │ am│ a │ mo│del│ tr│ain│ed │ to│ he│lp │ yo│ ... │u. │\n",
        "                └───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴───┴─────┴───┘\n",
        "                ◄────────────────── Context Window (N tokens) ──────────────►\n",
        "\n",
        "\n",
        "For a model with a 4-token context window generating the next token:\n",
        "\n",
        "Position:        0   1   2   3   |   4\n",
        "                ┌───┬───┬───┬───┐ | ┌───┐\n",
        "Content:        │The│cat│sat│on │ | │ ? │\n",
        "                └───┴───┴───┴───┘ | └───┘\n",
        "                ◄─Context Window─► | Next\n",
        "                                   | Token\n",
        "\n",
        "The model can only use the 4 tokens in its context window to predict the 5th token.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f70220a",
      "metadata": {},
      "source": [
        "#### Context Window Implementations in Different Models\n",
        "\n",
        "Context windows vary significantly across models:\n",
        "\n",
        "1. **Smaller Models**:\n",
        "   - GPT-2: 1,024 tokens\n",
        "   - BERT: 512 tokens\n",
        "   - Original RoBERTa: 512 tokens\n",
        "\n",
        "2. **Large Language Models**:\n",
        "   - GPT-3: 2,048 tokens\n",
        "   - GPT-3.5 (ChatGPT): 4,096 to 16,384 tokens\n",
        "   - GPT-4: Up to 32,768 tokens\n",
        "   - Claude: Up to 100,000 tokens\n",
        "\n",
        "3. **Long-Context Models**:\n",
        "   - Special architectures like Transformer-XL, Longformer, etc.\n",
        "   - Some experimental models support millions of tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981e5cf1",
      "metadata": {},
      "source": [
        "#### Context Window Challenges\n",
        "\n",
        "Working with context windows presents several challenges:\n",
        "\n",
        "1. **Truncation**:\n",
        "   - When inputs exceed the context window, information is truncated\n",
        "   - Most systems truncate from the beginning, preserving recent context\n",
        "   - Critical information may be lost\n",
        "\n",
        "2. **Information Retrieval**:\n",
        "   - Models may struggle to access information at the far end of a large context window\n",
        "   - Attention tends to decay with distance, creating a recency bias\n",
        "\n",
        "3. **Computational Complexity**:\n",
        "   - Self-attention mechanisms scale quadratically with sequence length (O(n²))\n",
        "   - Larger context windows dramatically increase computation and memory needs\n",
        "\n",
        "4. **Token Budgeting**:\n",
        "   - Prompts, instructions, and examples consume the same token budget as content\n",
        "   - Efficient prompt design becomes essential for maximizing useful context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddf23f6",
      "metadata": {},
      "source": [
        "#### Techniques for Working with Context Windows\n",
        "\n",
        "Several techniques can help overcome context window limitations:\n",
        "\n",
        "1. **Document Chunking**:\n",
        "   - Split large documents into overlapping chunks\n",
        "   - Process each chunk individually\n",
        "   - Combine or chain the results\n",
        "\n",
        "2. **Summarization**:\n",
        "   - Compress long texts to fit within context limits\n",
        "   - Use recursive summarization for very large documents\n",
        "\n",
        "3. **Retrieval-Augmented Generation (RAG)**:\n",
        "   - Store content externally in a vector database\n",
        "   - Retrieve only relevant pieces as needed\n",
        "   - Include only pertinent information in the prompt\n",
        "\n",
        "4. **Context Distillation**:\n",
        "   - Extract and maintain only the most relevant information\n",
        "   - Periodically summarize conversation history\n",
        "\n",
        "5. **Long-Context Fine-tuning**:\n",
        "   - Train models specifically to handle long-range dependencies\n",
        "   - Use architectures designed for extended context\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71469ad2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# GPT-2 context window visualization\n",
        "def visualize_context_window():\n",
        "    # Define context window size for GPT-2\n",
        "    context_size = 1024\n",
        "    \n",
        "    # Create a long text that exceeds the context window\n",
        "    long_text = \"Once upon a time, \" * 300  # Will be around 1200+ tokens\n",
        "    \n",
        "    # Tokenize and get token count\n",
        "    tokens = tokenizer.encode(long_text)\n",
        "    token_count = len(tokens)\n",
        "    \n",
        "    # Check if it exceeds context window\n",
        "    exceeds = token_count > context_size\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Plot context window\n",
        "    plt.axvspan(0, context_size, alpha=0.2, color='green', label='Context Window')\n",
        "    \n",
        "    # Plot token count\n",
        "    plt.axvline(x=token_count, color='red', linestyle='--', label=f'Text Length: {token_count} tokens')\n",
        "    \n",
        "    # Show truncation if needed\n",
        "    if exceeds:\n",
        "        plt.axvspan(context_size, token_count, alpha=0.2, color='red', label='Truncated Content')\n",
        "    \n",
        "    plt.xlim(0, max(token_count + 100, context_size + 100))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xlabel('Token Position')\n",
        "    plt.title('GPT-2 Context Window vs. Text Length')\n",
        "    plt.legend(loc='upper center')\n",
        "    \n",
        "    # Remove y-axis ticks and labels\n",
        "    plt.yticks([])\n",
        "    plt.ylabel('')\n",
        "    \n",
        "    # Add text annotations\n",
        "    plt.annotate('Available to model', xy=(context_size/2, 0.5), xytext=(context_size/2, 0.5),\n",
        "                 ha='center', va='center', color='darkgreen', fontsize=12)\n",
        "    \n",
        "    if exceeds:\n",
        "        plt.annotate('Not seen by model', xy=(context_size + (token_count-context_size)/2, 0.5), \n",
        "                     xytext=(context_size + (token_count-context_size)/2, 0.5),\n",
        "                     ha='center', va='center', color='darkred', fontsize=12)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print text statistics\n",
        "    print(f\"Original text length: {len(long_text)} characters\")\n",
        "    print(f\"Tokenized length: {token_count} tokens\")\n",
        "    print(f\"Context window size: {context_size} tokens\")\n",
        "    if exceeds:\n",
        "        print(f\"Truncation: {token_count - context_size} tokens will be truncated\")\n",
        "        \n",
        "        # Show first and last few tokens of what fits in context\n",
        "        kept_text = tokenizer.decode(tokens[:context_size])\n",
        "        print(f\"\\nFirst 50 chars that fit in context: {kept_text[:50]}...\")\n",
        "        print(f\"Last 50 chars that fit in context: ...{kept_text[-50:]}\")\n",
        "        \n",
        "        # Show first few tokens that get truncated\n",
        "        truncated_text = tokenizer.decode(tokens[context_size:])\n",
        "        print(f\"\\nFirst 50 chars that get truncated: {truncated_text[:50]}...\")\n",
        "\n",
        "# Demonstrate attention decay over long contexts\n",
        "def visualize_attention_over_distance():\n",
        "    # Create a text with a clear reference at the beginning\n",
        "    beginning_reference = \"The secret code is 92478365.\"\n",
        "    middle_text = \"This is some filler text. \" * 50\n",
        "    query = \"What was the secret code mentioned earlier?\"\n",
        "    \n",
        "    full_text = beginning_reference + \" \" + middle_text + \" \" + query\n",
        "    \n",
        "    # Tokenize\n",
        "    tokens = tokenizer.encode(full_text)\n",
        "    token_count = len(tokens)\n",
        "    \n",
        "    # Simplified attention score simulation\n",
        "    # In real attention mechanisms, scores would be computed via query-key dot products\n",
        "    def simulate_attention(position, query_position, context_size=1024):\n",
        "        # Simulate how attention decays with distance\n",
        "        # This is a simplified model: attention tends to be highest near the query\n",
        "        # and at the beginning of the text, with exponential decay in between\n",
        "        if position >= context_size:\n",
        "            return 0  # Out of context window\n",
        "        \n",
        "        # Distance from query position (negative means before query)\n",
        "        distance = position - query_position\n",
        "        \n",
        "        # Attention tends to focus on:\n",
        "        # 1. Positions very close to the query (local context)\n",
        "        # 2. Positions at the very beginning (global importance)\n",
        "        # 3. With some decay in between\n",
        "        \n",
        "        # Local attention near query\n",
        "        local_attention = np.exp(-abs(distance) / 10) if distance <= 0 else 0\n",
        "        \n",
        "        # Special attention to beginning of text (e.g., instructions)\n",
        "        beginning_attention = np.exp(-position / 30) if position < 20 else 0\n",
        "        \n",
        "        # Combine attention sources\n",
        "        return local_attention + beginning_attention\n",
        "    \n",
        "    # Find query position\n",
        "    query_position = full_text.find(query)\n",
        "    query_token_position = len(tokenizer.encode(full_text[:query_position]))\n",
        "    \n",
        "    # Calculate attention for each token\n",
        "    attention_scores = [simulate_attention(i, query_token_position) for i in range(token_count)]\n",
        "    \n",
        "    # Normalize scores\n",
        "    max_score = max(attention_scores)\n",
        "    if max_score > 0:\n",
        "        attention_scores = [score / max_score for score in attention_scores]\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    # Plot attention scores\n",
        "    plt.plot(attention_scores, color='blue', alpha=0.7)\n",
        "    plt.fill_between(range(token_count), attention_scores, alpha=0.2, color='blue')\n",
        "    \n",
        "    # Mark important positions\n",
        "    secret_pos = len(tokenizer.encode(beginning_reference)) - 1\n",
        "    plt.scatter([secret_pos], [attention_scores[secret_pos]], color='green', s=100, \n",
        "                label=f'Secret Code Position (token {secret_pos})')\n",
        "    \n",
        "    plt.scatter([query_token_position], [attention_scores[query_token_position]], color='red', s=100,\n",
        "                label=f'Query Position (token {query_token_position})')\n",
        "    \n",
        "    # Add context window boundary\n",
        "    context_size = 1024\n",
        "    if token_count > context_size:\n",
        "        plt.axvline(x=context_size, color='red', linestyle='--', \n",
        "                   label=f'Context Window Limit ({context_size} tokens)')\n",
        "    \n",
        "    plt.xlim(0, token_count)\n",
        "    plt.ylim(0, 1.1)\n",
        "    plt.xlabel('Token Position')\n",
        "    plt.ylabel('Simulated Attention Score')\n",
        "    plt.title('Attention Decay Over Distance in Context Window')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Report statistics\n",
        "    print(f\"Total text length: {token_count} tokens\")\n",
        "    print(f\"Secret code position: token {secret_pos}\")\n",
        "    print(f\"Query position: token {query_token_position}\")\n",
        "    print(f\"Distance between secret and query: {query_token_position - secret_pos} tokens\")\n",
        "    \n",
        "    if token_count > context_size and secret_pos < context_size:\n",
        "        print(\"\\nThe secret code is within the context window, but far from the query.\")\n",
        "        print(\"This creates a 'needle in a haystack' problem - the model may struggle to retrieve it.\")\n",
        "    elif token_count > context_size and secret_pos >= context_size:\n",
        "        print(\"\\nThe secret code is outside the context window!\")\n",
        "        print(\"The model has no way to access this information when responding to the query.\")\n",
        "\n",
        "# Demonstrate chunking technique\n",
        "def demonstrate_chunking():\n",
        "    # Create a long document\n",
        "    paragraphs = []\n",
        "    for i in range(20):\n",
        "        if i == 5:\n",
        "            # Insert critical information in paragraph 6\n",
        "            paragraphs.append(f\"Paragraph {i+1}: The meeting will be held on November 15th at 2:30 PM in Conference Room B.\")\n",
        "        else:\n",
        "            paragraphs.append(f\"Paragraph {i+1}: This is standard content for paragraph {i+1} of the document.\")\n",
        "    \n",
        "    full_document = \"\\n\\n\".join(paragraphs)\n",
        "    \n",
        "    # Tokenize to check length\n",
        "    tokens = tokenizer.encode(full_document)\n",
        "    token_count = len(tokens)\n",
        "    \n",
        "    # Chunk size (smaller than real context windows for demonstration)\n",
        "    chunk_size = 100\n",
        "    \n",
        "    # Create chunks with overlap\n",
        "    overlap = 20\n",
        "    chunks = []\n",
        "    chunk_ranges = []\n",
        "    \n",
        "    for i in range(0, token_count, chunk_size - overlap):\n",
        "        end = min(i + chunk_size, token_count)\n",
        "        chunk = tokens[i:end]\n",
        "        chunks.append(chunk)\n",
        "        chunk_ranges.append((i, end))\n",
        "        \n",
        "        if end == token_count:\n",
        "            break\n",
        "    \n",
        "    # Decode chunks for display\n",
        "    decoded_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
        "    \n",
        "    # Visualize chunking\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    \n",
        "    # Plot the full document as a bar\n",
        "    plt.barh(0, token_count, height=0.5, color='lightgray', alpha=0.5, label='Full Document')\n",
        "    \n",
        "    # Plot each chunk\n",
        "    chunk_colors = plt.cm.viridis(np.linspace(0, 1, len(chunks)))\n",
        "    for i, ((start, end), color) in enumerate(zip(chunk_ranges, chunk_colors)):\n",
        "        plt.barh(i+1, end-start, left=start, height=0.5, color=color, alpha=0.7,\n",
        "                label=f'Chunk {i+1}')\n",
        "        \n",
        "        # Show overlap\n",
        "        if i > 0:\n",
        "            prev_end = chunk_ranges[i-1][1]\n",
        "            if start < prev_end:\n",
        "                plt.barh(i+1, prev_end-start, left=start, height=0.5, color='red', alpha=0.3)\n",
        "    \n",
        "    # Highlight the paragraph with critical info\n",
        "    critical_para = 5  # 0-indexed\n",
        "    critical_start = full_document.find(f\"Paragraph {critical_para+1}\")\n",
        "    critical_end = full_document.find(f\"Paragraph {critical_para+2}\")\n",
        "    if critical_end == -1:  # If it's the last paragraph\n",
        "        critical_end = len(full_document)\n",
        "    \n",
        "    critical_tokens_start = len(tokenizer.encode(full_document[:critical_start]))\n",
        "    critical_tokens_end = len(tokenizer.encode(full_document[:critical_end]))\n",
        "    \n",
        "    plt.barh(0, critical_tokens_end - critical_tokens_start, left=critical_tokens_start, \n",
        "            height=0.5, color='green', alpha=0.5, label='Critical Information')\n",
        "    \n",
        "    # Find which chunks contain the critical information\n",
        "    critical_chunks = []\n",
        "    for i, (start, end) in enumerate(chunk_ranges):\n",
        "        if (start <= critical_tokens_start < end) or (start < critical_tokens_end <= end) or \\\n",
        "           (critical_tokens_start <= start and end <= critical_tokens_end):\n",
        "            critical_chunks.append(i)\n",
        "    \n",
        "    # Emphasize the chunks containing critical info\n",
        "    for i in critical_chunks:\n",
        "        start, end = chunk_ranges[i]\n",
        "        plt.barh(i+1, end-start, left=start, height=0.5, \n",
        "                color='green', alpha=0.3)\n",
        "    \n",
        "    plt.yticks([0] + [i+1 for i in range(len(chunks))], \n",
        "               ['Full Document'] + [f'Chunk {i+1}' for i in range(len(chunks))])\n",
        "    plt.xlabel('Token Position')\n",
        "    plt.title('Document Chunking for Context Window Management')\n",
        "    \n",
        "    # Custom legend without duplicate entries\n",
        "    handles, labels = plt.gca().get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    plt.legend(by_label.values(), by_label.keys(), loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print stats and info about which chunks contain the critical information\n",
        "    print(f\"Full document: {token_count} tokens\")\n",
        "    print(f\"Chunk size: {chunk_size} tokens with {overlap} tokens overlap\")\n",
        "    print(f\"Total chunks: {len(chunks)}\")\n",
        "    print(f\"\\nCritical information located at tokens {critical_tokens_start}-{critical_tokens_end}\")\n",
        "    print(f\"Critical information appears in chunks: {[i+1 for i in critical_chunks]}\")\n",
        "    \n",
        "    # Print the chunk containing the critical information\n",
        "    for i in critical_chunks:\n",
        "        print(f\"\\n--- CHUNK {i+1} CONTENT ---\")\n",
        "        print(decoded_chunks[i])\n",
        "        print(\"------------------------\")\n",
        "\n",
        "# Run the basic example\n",
        "def basic_context_window_example():\n",
        "    # A long text example\n",
        "    long_text = \"\"\"\n",
        "    This is a long article that exceeds the context window limits of GPT-2.\n",
        "    It contains multiple paragraphs of information that the model would need to process.\n",
        "    [Imagine many more paragraphs here...]\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize and check length\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    tokens = tokenizer.encode(long_text)\n",
        "    print(f\"Token count: {len(tokens)}\")\n",
        "\n",
        "    # Check if it exceeds GPT-2's context window (1024 tokens)\n",
        "    if len(tokens) > 1024:\n",
        "        print(\"Text exceeds context window, will be truncated\")\n",
        "        \n",
        "        # Truncate to fit\n",
        "        truncated_tokens = tokens[:1024]\n",
        "        truncated_text = tokenizer.decode(truncated_tokens)\n",
        "        print(f\"Truncated to {len(truncated_tokens)} tokens\")\n",
        "\n",
        "# Run the examples\n",
        "# visualize_context_window()\n",
        "# visualize_attention_over_distance()\n",
        "# demonstrate_chunking()\n",
        "basic_context_window_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80c26418",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Understanding context windows is essential for practical applications:\n",
        "\n",
        "1. **Document Processing Systems**:\n",
        "   - Legal document analysis using chunking and summarization\n",
        "   - Academic research assistants that process long papers\n",
        "   - Contract analysis tools that handle multi-page agreements\n",
        "\n",
        "2. **Conversation Management**:\n",
        "   - Chatbots that maintain conversation history within context limits\n",
        "   - Customer service systems that summarize lengthy support threads\n",
        "   - Meeting assistants that provide real-time summaries of discussions\n",
        "\n",
        "3. **Content Generation**:\n",
        "   - Book or story writing assistants that maintain narrative coherence\n",
        "   - Report generation from extensive data sources\n",
        "   - Long-form article writing with consistent themes and references\n",
        "\n",
        "4. **Knowledge Management**:\n",
        "   - RAG systems that retrieve and integrate knowledge from large datasets\n",
        "   - Question answering over extensive documentation\n",
        "   - Research tools that synthesize information across multiple sources\n",
        "\n",
        "5. **Educational Applications**:\n",
        "   - Tutoring systems that track student context over multiple sessions\n",
        "   - Personalized learning that adapts to student history and progress\n",
        "   - Educational content summarization for enhanced comprehension\n",
        "\n",
        "6. **Decision Support**:\n",
        "   - Medical diagnosis systems that process patient history\n",
        "   - Financial analysis tools that review market trends and reports\n",
        "   - Legal research assistants that analyze case law and precedents\n",
        "\n",
        "Context window management is often the difference between a model that appears to have deep understanding versus one that seems forgetful or disconnected. By applying appropriate techniques for working within or extending effective context, developers can create more powerful and coherent AI applications.\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71d6f6c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# A long text example\n",
        "long_text = \"\"\"\n",
        "[Long article or conversation that exceeds the context window]\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and check length\n",
        "tokens = tokenizer.encode(long_text)\n",
        "print(f\"Token count: {len(tokens)}\")\n",
        "\n",
        "# Check if it exceeds GPT-2's context window (1024 tokens)\n",
        "if len(tokens) > 1024:\n",
        "    print(\"Text exceeds context window, will be truncated\")\n",
        "    \n",
        "    # Truncate to fit\n",
        "    truncated_tokens = tokens[:1024]\n",
        "    truncated_text = tokenizer.decode(truncated_tokens)\n",
        "    print(f\"Truncated to {len(truncated_tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7a7155",
      "metadata": {},
      "source": [
        "## 10. **Vocabulary**\n",
        "- All the unique tokens the model can understand.\n",
        "- The size of the vocabulary affects performance and memory."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c07822",
      "metadata": {},
      "source": [
        "### Detailed Theory\n",
        "\n",
        "A vocabulary in the context of language models is the complete set of unique tokens that the model recognizes and can process. The vocabulary is a critical component that bridges the gap between human-readable text and the numerical representations that neural networks can process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "453d8c02",
      "metadata": {},
      "source": [
        "#### What is a Vocabulary?\n",
        "\n",
        "A vocabulary is essentially a fixed dictionary that maps text segments (tokens) to unique identifiers (typically integers). These identifiers are then used to look up corresponding vector representations (embeddings) in the model.\n",
        "\n",
        "Key components of a vocabulary include:\n",
        "1. **Tokens**: The basic units (words, subwords, or characters)\n",
        "2. **Token IDs**: Unique integer identifiers for each token\n",
        "3. **Special Tokens**: Special-purpose tokens like [PAD], [CLS], [SEP], [MASK], etc.\n",
        "4. **Out-of-Vocabulary (OOV) Handling**: Strategies for dealing with unknown tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96fbc493",
      "metadata": {},
      "source": [
        "#### Vocabulary Construction\n",
        "\n",
        "Vocabulary construction is a crucial pre-training step that impacts model performance. Several approaches exist:\n",
        "\n",
        "1. **Word-level Vocabularies**:\n",
        "   - Each token represents a complete word\n",
        "   - Advantages: Preserves word meaning, intuitive\n",
        "   - Disadvantages: Large vocabulary size, struggles with rare words and morphology\n",
        "\n",
        "2. **Character-level Vocabularies**:\n",
        "   - Each token represents a single character\n",
        "   - Advantages: Tiny vocabulary, no OOV issues\n",
        "   - Disadvantages: Very long sequences, poor semantic capture\n",
        "\n",
        "3. **Subword Vocabularies** (Most common in modern LLMs):\n",
        "   - Tokens represent common words or subword pieces\n",
        "   - Common algorithms: BPE (Byte-Pair Encoding), WordPiece, SentencePiece, Unigram\n",
        "   - Advantages: Balance between vocabulary size and sequence length"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "377ac180",
      "metadata": {},
      "source": [
        "#### Visual Representation\n",
        "\n",
        "```\n",
        "Vocabulary Construction and Usage:\n",
        "\n",
        "   Raw Text: \"The transformer model works well with subword tokenization\"\n",
        "          ↓\n",
        "┌─────────────────────────────────────┐\n",
        "│           Tokenization              │\n",
        "└─────────────────────────────────────┘\n",
        "          ↓\n",
        "  Tokens: [\"The\", \"transform\", \"##er\", \"model\", \"works\", \"well\", \"with\", \"sub\", \"##word\", \"token\", \"##ization\"]\n",
        "          ↓\n",
        "┌─────────────────────────────────────┐\n",
        "│        Vocabulary Lookup            │\n",
        "│                                     │\n",
        "│    Token      →    Token ID         │\n",
        "│  \"The\"        →       8             │\n",
        "│  \"transform\"  →     416             │\n",
        "│  \"##er\"       →      92             │\n",
        "│  \"model\"      →     318             │\n",
        "│   ...         →      ...            │\n",
        "└─────────────────────────────────────┘\n",
        "          ↓\n",
        "  Token IDs: [8, 416, 92, 318, 712, 188, 19, 1453, 75, 6054, 129]\n",
        "          ↓\n",
        "┌─────────────────────────────────────┐\n",
        "│        Embedding Lookup             │\n",
        "└─────────────────────────────────────┘\n",
        "          ↓\n",
        "  Embeddings: [ [0.1, 0.3, ...], [0.5, -0.2, ...], ... ]  (vectors)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd062198",
      "metadata": {},
      "source": [
        "#### Byte-Pair Encoding (BPE) Example\n",
        "\n",
        "BPE is one of the most common subword tokenization algorithms. Here's how it works:\n",
        "\n",
        "1. Start with a vocabulary of individual characters\n",
        "2. Count the frequency of adjacent pairs of tokens\n",
        "3. Merge the most frequent pair into a new token\n",
        "4. Repeat the process for a specified number of merges\n",
        "\n",
        "```\n",
        "Example BPE Process:\n",
        "\n",
        "Initial vocab: ['a', 'b', 'c', 'd', 'e', 'l', 'o', 'r', 't', 'w']\n",
        "\n",
        "Training corpus: \"lower lower lower tower tower\"\n",
        "\n",
        "Word splits: ['l','o','w','e','r'] ['l','o','w','e','r'] ['l','o','w','e','r'] ['t','o','w','e','r'] ['t','o','w','e','r']\n",
        "\n",
        "Frequency count:\n",
        "'l' + 'o' = 3\n",
        "'o' + 'w' = 5\n",
        "'w' + 'e' = 5\n",
        "'e' + 'r' = 5\n",
        "'t' + 'o' = 2\n",
        "\n",
        "Merge most frequent: 'o' + 'w' → 'ow'\n",
        "Updated corpus: ['l','ow','e','r'] ['l','ow','e','r'] ['l','ow','e','r'] ['t','ow','e','r'] ['t','ow','e','r']\n",
        "\n",
        "Next frequency count:\n",
        "'l' + 'ow' = 3\n",
        "'ow' + 'e' = 5\n",
        "'e' + 'r' = 5\n",
        "'t' + 'ow' = 2\n",
        "\n",
        "...and so on.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32fd97a7",
      "metadata": {},
      "source": [
        "#### Vocabulary Size Considerations\n",
        "\n",
        "The vocabulary size is a key hyperparameter that affects model performance:\n",
        "\n",
        "1. **Larger Vocabularies**:\n",
        "   - Can represent more whole words directly\n",
        "   - Reduce the need for subword splitting\n",
        "   - Increase model size (embedding matrix grows)\n",
        "   - Lead to sparser training for rare tokens\n",
        "\n",
        "2. **Smaller Vocabularies**:\n",
        "   - Require more aggressive subword splitting\n",
        "   - May lose some semantic coherence\n",
        "   - Reduce model size and memory usage\n",
        "   - Allow better learning for all tokens\n",
        "\n",
        "Typical vocabulary sizes range from:\n",
        "- Small models: 10,000-30,000 tokens\n",
        "- Medium models: 30,000-50,000 tokens\n",
        "- Large models: 50,000-100,000+ tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d7d3c31",
      "metadata": {},
      "source": [
        "#### Special Tokens and Their Purpose\n",
        "\n",
        "Special tokens serve specific functions in language models:\n",
        "\n",
        "1. **[PAD]** or `<pad>`: Padding token to ensure uniform sequence length\n",
        "2. **[CLS]** or `<s>`: Classification token, often used to represent entire sequence\n",
        "3. **[SEP]** or `</s>`: Separator token between different segments of text\n",
        "4. **[MASK]**: Masked token for masked language modeling\n",
        "5. **[UNK]** or `<unk>`: Unknown token for words not in vocabulary\n",
        "6. **[BOS]** or `<bos>`: Beginning of sequence marker\n",
        "7. **[EOS]** or `<eos>`: End of sequence marker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8fe070",
      "metadata": {},
      "source": [
        "#### Multi-language Considerations\n",
        "\n",
        "For multilingual models, vocabulary construction presents additional challenges:\n",
        "\n",
        "1. **Script Coverage**: Ensuring all target language scripts are represented\n",
        "2. **Language Balance**: Preventing dominant languages from taking most vocabulary slots\n",
        "3. **Transliteration**: Handling cross-script representations\n",
        "4. **Character Set**: Supporting various Unicode ranges\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6875338",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, BertTokenizer, T5Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compare different tokenizers and their vocabularies\n",
        "def compare_tokenizers():\n",
        "    # Load different tokenizers\n",
        "    tokenizers = {\n",
        "        \"GPT-2 (BPE)\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "        \"BERT (WordPiece)\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "        \"T5 (SentencePiece)\": T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    }\n",
        "    \n",
        "    # Print vocabulary sizes\n",
        "    print(\"Vocabulary Size Comparison:\")\n",
        "    for name, tokenizer in tokenizers.items():\n",
        "        print(f\"{name}: {len(tokenizer)} tokens\")\n",
        "    \n",
        "    # Sample texts for tokenization comparison\n",
        "    texts = [\n",
        "        \"The transformer architecture revolutionized NLP.\",\n",
        "        \"Unsupervised pretraining works remarkably well.\",\n",
        "        \"Tokenization breaks text into smaller units.\",\n",
        "        \"COVID-19 has accelerated digital transformation.\",\n",
        "        \"The model can't understand hyperconscientiousness easily.\"\n",
        "    ]\n",
        "    \n",
        "    # Compare tokenization results\n",
        "    print(\"\\nTokenization Comparison:\")\n",
        "    for i, text in enumerate(texts):\n",
        "        print(f\"\\nText {i+1}: \\\"{text}\\\"\")\n",
        "        for name, tokenizer in tokenizers.items():\n",
        "            tokens = tokenizer.tokenize(text)\n",
        "            print(f\"{name}: {tokens}\")\n",
        "            print(f\"Token count: {len(tokens)}\")\n",
        "    \n",
        "    # Compare subword splitting behavior\n",
        "    complex_words = [\n",
        "        \"unconstitutional\",\n",
        "        \"internationalization\",\n",
        "        \"misunderstanding\",\n",
        "        \"hyperparameters\",\n",
        "        \"pretraining\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nSubword Splitting Comparison:\")\n",
        "    for word in complex_words:\n",
        "        print(f\"\\nWord: \\\"{word}\\\"\")\n",
        "        for name, tokenizer in tokenizers.items():\n",
        "            tokens = tokenizer.tokenize(word)\n",
        "            print(f\"{name}: {tokens}\")\n",
        "    \n",
        "    # Visualize token length distribution in a larger text\n",
        "    sample_book = \"\"\"\n",
        "    Machine learning is a subfield of artificial intelligence that focuses on developing systems that can learn from data.\n",
        "    These systems improve their performance over time without being explicitly programmed for specific tasks.\n",
        "    The field encompasses various techniques including supervised learning, unsupervised learning, reinforcement learning, and deep learning.\n",
        "    Neural networks, particularly deep neural networks, have revolutionized the field in recent years.\n",
        "    Transformers represent a breakthrough architecture that uses self-attention mechanisms to process sequential data efficiently.\n",
        "    Language models built on transformer architectures have demonstrated remarkable capabilities in understanding and generating human language.\n",
        "    The tokenization process is fundamental to these language models, as it determines how text is broken down into manageable units.\n",
        "    Different tokenization strategies balance vocabulary size against sequence length, with subword tokenization emerging as a popular approach.\n",
        "    \"\"\"\n",
        "    \n",
        "    token_lengths = {}\n",
        "    for name, tokenizer in tokenizers.items():\n",
        "        tokens = tokenizer.tokenize(sample_book)\n",
        "        # Get length of each token in characters\n",
        "        lengths = [len(token) for token in tokens]\n",
        "        token_lengths[name] = lengths\n",
        "    \n",
        "    # Plot token length distributions\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for name, lengths in token_lengths.items():\n",
        "        plt.hist(lengths, alpha=0.7, bins=range(1, max(max(lengths) + 1, 15)), label=name)\n",
        "    \n",
        "    plt.xlabel('Token Length (characters)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Token Length Distribution by Tokenizer')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize vocabulary coverage and frequency\n",
        "def visualize_vocabulary_coverage():\n",
        "    # Load tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    # Sample texts from different domains\n",
        "    domains = {\n",
        "        \"General\": \"The quick brown fox jumps over the lazy dog. This is a common pangram that contains all letters.\",\n",
        "        \"Technical\": \"The convolutional neural network achieved state-of-the-art results on the benchmark dataset.\",\n",
        "        \"Medical\": \"The patient exhibited symptoms of hypertension and hypercholesterolemia requiring immediate intervention.\",\n",
        "        \"Legal\": \"The aforementioned party shall henceforth be referred to as the lessee in accordance with the statutory requirements.\",\n",
        "        \"Social Media\": \"OMG this is sooo cool! can't wait 2 try it out 😍 #awesome #techlife @friendsaccount\"\n",
        "    }\n",
        "    \n",
        "    # Analyze token frequency by domain\n",
        "    domain_stats = {}\n",
        "    for domain, text in domains.items():\n",
        "        # Tokenize\n",
        "        tokens = tokenizer.tokenize(text)\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        \n",
        "        # Count tokens in vocab vs. unknown\n",
        "        unknowns = token_ids.count(tokenizer.unk_token_id)\n",
        "        vocab_coverage = (len(tokens) - unknowns) / len(tokens) * 100\n",
        "        \n",
        "        # Store stats\n",
        "        domain_stats[domain] = {\n",
        "            'tokens': tokens,\n",
        "            'unique_tokens': len(set(tokens)),\n",
        "            'token_count': len(tokens),\n",
        "            'unknown_count': unknowns,\n",
        "            'vocab_coverage': vocab_coverage\n",
        "        }\n",
        "    \n",
        "    # Visualize results\n",
        "    domains_list = list(domains.keys())\n",
        "    coverage = [domain_stats[d]['vocab_coverage'] for d in domains_list]\n",
        "    unique_counts = [domain_stats[d]['unique_tokens'] for d in domains_list]\n",
        "    \n",
        "    # Plot coverage\n",
        "    plt.figure(figsize=(14, 6))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.bar(domains_list, coverage, color='skyblue')\n",
        "    plt.axhline(y=100, color='r', linestyle='--', alpha=0.7, label='Full Coverage')\n",
        "    plt.ylabel('Vocabulary Coverage (%)')\n",
        "    plt.title('Vocabulary Coverage by Domain')\n",
        "    plt.ylim(0, 105)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(domains_list, unique_counts, color='lightgreen')\n",
        "    plt.ylabel('Unique Token Count')\n",
        "    plt.title('Lexical Diversity by Domain')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print detailed stats\n",
        "    print(\"Vocabulary Coverage Analysis:\")\n",
        "    for domain, stats in domain_stats.items():\n",
        "        print(f\"\\n{domain}:\")\n",
        "        print(f\"  Total tokens: {stats['token_count']}\")\n",
        "        print(f\"  Unique tokens: {stats['unique_tokens']}\")\n",
        "        print(f\"  Unknown tokens: {stats['unknown_count']}\")\n",
        "        print(f\"  Vocabulary coverage: {stats['vocab_coverage']:.2f}%\")\n",
        "        print(f\"  Sample tokens: {stats['tokens'][:10]}...\")\n",
        "\n",
        "# Demonstrate OOV handling and subword tokenization\n",
        "def demonstrate_oov_handling():\n",
        "    # Load tokenizers\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    \n",
        "    # Words not likely to be in vocabulary\n",
        "    rare_words = [\n",
        "        \"supercalifragilisticexpialidocious\",\n",
        "        \"pneumonoultramicroscopicsilicovolcanoconiosis\",\n",
        "        \"COVID19\",\n",
        "        \"blockchain\",\n",
        "        \"cryptocurrency\",\n",
        "        \"neuromorphic\",\n",
        "        \"transformerarchitecture\"\n",
        "    ]\n",
        "    \n",
        "    # Show how they get tokenized\n",
        "    print(\"Handling Out-of-Vocabulary Words:\")\n",
        "    for word in rare_words:\n",
        "        tokens = tokenizer.tokenize(word)\n",
        "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        \n",
        "        # Check for unknown tokens\n",
        "        has_unk = tokenizer.unk_token in tokens\n",
        "        \n",
        "        print(f\"\\nWord: {word}\")\n",
        "        print(f\"Tokens: {tokens}\")\n",
        "        if has_unk:\n",
        "            print(\"Contains unknown tokens!\")\n",
        "        else:\n",
        "            print(f\"Successfully split into {len(tokens)} subword tokens\")\n",
        "    \n",
        "    # Demonstrate how context affects tokenization\n",
        "    context_examples = [\n",
        "        (\"playing\", \"The children are playing in the park.\"),\n",
        "        (\"playing\", \"I am playing a new song on the piano.\"),\n",
        "        (\"playing\", \"We're playing to win the championship.\"),\n",
        "        (\"bank\", \"I need to go to the bank to withdraw money.\"),\n",
        "        (\"bank\", \"The river bank was eroding after the flood.\"),\n",
        "        (\"bank\", \"Let's bank on their support for the project.\")\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nContextual Tokenization (Same Word, Different Contexts):\")\n",
        "    for word, context in context_examples:\n",
        "        # Highlight the target word\n",
        "        highlighted_context = context.replace(word, f\"**{word}**\")\n",
        "        print(f\"\\nContext: {highlighted_context}\")\n",
        "        \n",
        "        # Tokenize\n",
        "        tokens = tokenizer.tokenize(context)\n",
        "        print(f\"Full tokenization: {tokens}\")\n",
        "        \n",
        "        # Find position of the word in context\n",
        "        word_pos = context.find(word)\n",
        "        before_tokens = tokenizer.tokenize(context[:word_pos])\n",
        "        target_tokens = tokenizer.tokenize(word)\n",
        "        \n",
        "        print(f\"Target word '{word}' tokenized as: {target_tokens}\")\n",
        "\n",
        "# Run the examples\n",
        "# compare_tokenizers()\n",
        "# visualize_vocabulary_coverage()\n",
        "# demonstrate_oov_handling()\n",
        "\n",
        "# Basic vocabulary example\n",
        "def basic_vocabulary_example():\n",
        "    from transformers import GPT2Tokenizer, BertTokenizer\n",
        "\n",
        "    # GPT-2 vocabulary\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    print(f\"GPT-2 vocabulary size: {len(gpt2_tokenizer)}\")\n",
        "\n",
        "    # BERT vocabulary\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    print(f\"BERT vocabulary size: {len(bert_tokenizer)}\")\n",
        "\n",
        "    # Example tokens from vocabulary\n",
        "    word = \"learning\"\n",
        "    gpt2_token = gpt2_tokenizer.encode(word)[0]\n",
        "    bert_token = bert_tokenizer.encode(word)[1]  # Skip CLS token\n",
        "\n",
        "    print(f\"'{word}' token ID in GPT-2: {gpt2_token}\")\n",
        "    print(f\"'{word}' token ID in BERT: {bert_token}\")\n",
        "\n",
        "# Run the example\n",
        "basic_vocabulary_example()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb93ea8c",
      "metadata": {},
      "source": [
        "### Real-World Applications\n",
        "\n",
        "Understanding vocabulary design impacts several aspects of language model development and usage:\n",
        "\n",
        "1. **Language Model Efficiency**:\n",
        "   - Vocabulary optimization reduces model size and memory requirements\n",
        "   - Efficient tokenization minimizes input sequence lengths\n",
        "   - Well-balanced vocabularies improve training stability\n",
        "\n",
        "2. **Multilingual Support**:\n",
        "   - Cross-lingual vocabularies enable zero-shot translation\n",
        "   - Script-balanced vocabulary allocation improves performance across languages\n",
        "   - Shared subwords between languages enhance representation learning\n",
        "\n",
        "3. **Domain Adaptation**:\n",
        "   - Custom vocabularies for specialized domains (legal, medical, scientific)\n",
        "   - Domain-specific token optimization reduces out-of-vocabulary occurrences\n",
        "   - Extended vocabularies for jargon-heavy applications\n",
        "\n",
        "4. **Privacy and Security**:\n",
        "   - Vocabulary control to prevent memorization of sensitive data\n",
        "   - Tokenization-based data anonymization techniques\n",
        "   - Special token handling for personally identifiable information\n",
        "\n",
        "5. **Compression and Efficiency**:\n",
        "   - Vocabulary pruning to reduce model size for edge deployments\n",
        "   - Quantized token embeddings for memory-constrained environments\n",
        "   - Hybrid tokenization schemes to balance efficiency and accuracy\n",
        "\n",
        "6. **Language Evolution**:\n",
        "   - Vocabulary updating to incorporate new terms and expressions\n",
        "   - Handling of emerging entities, products, and concepts\n",
        "   - Neologism and slang adaptation in social media applications\n",
        "\n",
        "The vocabulary design balances multiple competing concerns: model size, sequence length, semantic coherence, and out-of-vocabulary handling. Well-designed vocabularies are fundamental to creating effective and efficient language models.\n",
        "\n",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b18055c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, BertTokenizer\n",
        "\n",
        "# GPT-2 vocabulary\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "print(f\"GPT-2 vocabulary size: {len(gpt2_tokenizer)}\")\n",
        "\n",
        "# BERT vocabulary\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "print(f\"BERT vocabulary size: {len(bert_tokenizer)}\")\n",
        "\n",
        "# Example tokens from vocabulary\n",
        "word = \"learning\"\n",
        "gpt2_token = gpt2_tokenizer.encode(word)[0]\n",
        "bert_token = bert_tokenizer.encode(word)[1]  # Skip CLS token\n",
        "\n",
        "print(f\"'{word}' token ID in GPT-2: {gpt2_token}\")\n",
        "print(f\"'{word}' token ID in BERT: {bert_token}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
