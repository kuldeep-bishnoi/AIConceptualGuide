{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Advanced AI Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Advanced AI Concepts",
        "",
        "This file covers advanced AI techniques and optimizations used in modern language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. **LoRA (Low-Rank Adaptation)**",
        "- Fine-tunes models efficiently with fewer parameters.",
        "- Saves cost and memory by updating low-rank matrices instead of full weights.",
        "- Originally proposed by Microsoft Research in 2021 in the paper \"LoRA: Low-Rank Adaptation of Large Language Models\".",
        "- Used extensively in fine-tuning LLMs for specific tasks without the cost of full fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning approach that dramatically reduces the number of trainable parameters while preserving model quality. It enables efficient adaptation of large models for specific tasks and domains without the computational burden of full fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Problem LoRA Solves",
        "",
        "Training state-of-the-art language models faces significant challenges:",
        "",
        "1. **Parameter Explosion**: Modern LLMs have billions of parameters (GPT-3: 175B, PaLM: 540B, etc.)",
        "2. **Memory Requirements**: Full fine-tuning requires storing optimizer states for all parameters",
        "3. **Compute Costs**: Gradient updates for all parameters demand substantial computational resources",
        "4. **Storage Overhead**: Maintaining separate copies of fine-tuned models is impractical",
        "",
        "LoRA addresses these problems through a key insight: **the updates to the weight matrices during adaptation have low intrinsic rank**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundations",
        "",
        "In a neural network, weight matrices often undergo relatively low-rank updates during fine-tuning, meaning most information can be captured in a smaller dimensional space.",
        "",
        "For a pre-trained weight matrix $W_0 \\in \\mathbb{R}^{d \\times k}$, the conventional fine-tuning approach updates the entire matrix to $W_0 + \\Delta W$.",
        "",
        "LoRA instead represents the update $\\Delta W$ as a product of two low-rank matrices:",
        "",
        "$$\\Delta W = BA$$",
        "",
        "Where:",
        "- $B \\in \\mathbb{R}^{d \\times r}$",
        "- $A \\in \\mathbb{R}^{r \\times k}$",
        "- $r \\ll \\min(d, k)$ is the rank (typically 4, 8, 16, 64)",
        "",
        "The full forward pass becomes:",
        "",
        "$$h = W_0 x + \\Delta W x = W_0 x + BA x$$",
        "",
        "The key benefits:",
        "- Only $r \\times (d + k)$ parameters need to be trained instead of $d \\times k$",
        "- The original weights $W_0$ remain frozen (no gradients)",
        "- Multiple task adaptations can share the same base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "LoRA Architecture:",
        "                                                    ",
        "           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                             ",
        "Frozen     \u2502   Pre-trained   \u2502                             ",
        "Parameters \u2502   Weight Matrix \u2502                             ",
        "           \u2502      (W\u2080)       \u2502                             ",
        "           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                             ",
        "                    \u2502                                      ",
        "                    \u2502                                      ",
        "                    \u25bc                                      ",
        "Input  \u2500\u2500\u2500\u2500\u2500\u25ba Multiplication \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     ",
        "   x                                 \u2502                     ",
        "                                     \u251c\u2500\u2500\u2500\u25ba Add \u2500\u2500\u2500\u25ba Output ",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502         h           ",
        "                    \u2502     A     \u2502    \u2502                     ",
        "Input  \u2500\u2500\u2500\u2500\u2500\u25ba       \u2502  (r\u00d7k)    \u2502    \u2502                     ",
        "   x        \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                     ",
        "            \u2502             \u2502          \u2502                     ",
        "            \u2502             \u25bc          \u2502                     ",
        "            \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502                     ",
        "            \u2502       \u2502     B     \u2502    \u2502                     ",
        "            \u2502       \u2502  (d\u00d7r)    \u2502    \u2502                     ",
        "            \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518    \u2502                     ",
        "            \u2502             \u2502          \u2502                     ",
        "            \u2514\u2500\u2500\u2500\u2500\u2500\u25ba Multiplication \u2500\u2500\u2500\u2518                     ",
        "                          \u2502                                ",
        "                          \u2502                                ",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                          ",
        "Trainable    \u250c\u2500\u2500\u25ba  \u2502     BA     \u2502 \u25c4\u2500\u2500\u2510                     ",
        "Parameters   \u2502     \u2502   (d\u00d7k)    \u2502    \u2502                     ",
        "             \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502                     ",
        "             \u2502                       \u2502                     ",
        "             \u2514\u2500\u2500\u2500 Low-Rank Update \u2500\u2500\u2500\u2518                     ",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling and Adaptation",
        "",
        "LoRA introduces a scaling parameter \u03b1 to adjust the magnitude of the update:",
        "",
        "$$h = W_0 x + \\frac{\\alpha}{r} BA x$$",
        "",
        "Where:",
        "- \u03b1 is a scaling factor (typically similar to r)",
        "- Dividing by r helps keep the scale of the update similar across different rank values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Where to Apply LoRA",
        "",
        "In transformer models, LoRA can be applied to different weight matrices:",
        "",
        "1. **Query/Key/Value projections** in attention mechanisms",
        "2. **Output projections** after attention",
        "3. **Feed-forward network** weights",
        "4. **Word embeddings** (less common)",
        "",
        "Research shows applying LoRA selectively to attention layers often provides the best performance/parameter tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LoRA Compared to Other Methods",
        "",
        "| Method | Parameters | Performance | Inference Speed |",
        "|--------|------------|-------------|-----------------|",
        "| Full Fine-tuning | 100% | Baseline | 1x |",
        "| Adapter Layers | ~1-5% | Slightly lower | Slower (extra layers) |",
        "| Prompt Tuning | <1% | Lower | 1x |",
        "| **LoRA** | ~0.1-1% | Near baseline | 1x (can be merged) |",
        "",
        "LoRA's key advantage is maintaining inference speed: for deployment, LoRA weights can be merged with the original weights by computing $W_0 + BA$, resulting in zero inference overhead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameters in LoRA",
        "",
        "Key hyperparameters to consider:",
        "",
        "1. **Rank (r)**: Controls the expressivity of the update (higher rank = more capacity)",
        "2. **Alpha (\u03b1)**: Scales the contribution of the low-rank update",
        "3. **Target Modules**: Which weight matrices to adapt with LoRA",
        "4. **Dropout**: Applied to the LoRA activations to prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Advanced LoRA Variants",
        "",
        "Recent research has extended the basic LoRA approach:",
        "",
        "1. **Adaptive LoRA**: Dynamically adjusts rank based on importance",
        "2. **QLoRA**: Combines LoRA with quantization for even more efficiency",
        "3. **LoRA+**: Applies different ranks to different layers based on sensitivity",
        "4. **GLoRA**: Adds gating mechanisms to control update influence",
        "",
        "**Installation:**",
        "```bash",
        "pip install peft transformers",
        "```",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "from peft import get_peft_model, LoraConfig, TaskType",
        "",
        "# Load base model",
        "model_name = \"gpt2\"  # A smaller model for demonstration",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)",
        "",
        "# Count base parameters",
        "base_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)",
        "print(f\"Base model trainable parameters: {base_params:,}\")",
        "",
        "# Configure LoRA",
        "lora_config = LoraConfig(",
        "    r=8,                     # Rank of update matrices",
        "    lora_alpha=32,           # Parameter scaling factor",
        "    target_modules=[\"c_attn\"], # Which modules to apply LoRA to",
        "    lora_dropout=0.05,       # Dropout probability for LoRA layers",
        "    bias=\"none\",            # Add bias to LoRA layers",
        "    task_type=TaskType.CAUSAL_LM  # Task type",
        ")",
        "",
        "# Create LoRA model",
        "lora_model = get_peft_model(base_model, lora_config)",
        "",
        "# Count parameters after LoRA",
        "lora_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)",
        "print(f\"LoRA model trainable parameters: {lora_params:,}\")",
        "print(f\"Parameter reduction: {lora_params / base_params * 100:.2f}% of original\")",
        "",
        "# Example usage",
        "inputs = tokenizer(\"AI is transforming how we\", return_tensors=\"pt\")",
        "with torch.no_grad():",
        "    outputs = lora_model.generate(**inputs, max_length=50)",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Applications",
        "",
        "LoRA has become essential in the AI ecosystem:",
        "",
        "1. **Fine-tuning Foundation Models**: Efficiently adapt large models like LLaMA, PaLM, and GPT for specific domains",
        "2. **Personalization**: Create user-specific versions of models with minimal storage overhead",
        "3. **Domain Adaptation**: Customize models for specialized fields like medicine, law, or finance",
        "4. **Multi-task Learning**: Train separate LoRA modules for different tasks sharing the same base model",
        "5. **Resource-constrained Environments**: Enable fine-tuning on consumer hardware like single GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation Considerations",
        "",
        "When implementing LoRA:",
        "",
        "1. **Target the Right Modules**: Focus on attention layers first, as they benefit most from LoRA",
        "2. **Use Appropriate Ranks**: Start with r=8 or r=16 and adjust based on performance",
        "3. **Tune Learning Rate**: LoRA often benefits from higher learning rates than full fine-tuning",
        "4. **Consider Merging**: For deployment, merge LoRA weights with the base model for optimal inference",
        "",
        "The efficiency of LoRA has made it one of the most important advances in practical LLM deployment, enabling widespread fine-tuning and adaptation of models that would otherwise be prohibitively expensive to customize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. **Quantization**",
        "- Reducing model precision (e.g., from 32-bit to 8-bit) for speed and deployment.",
        "- Trades minimal accuracy for huge efficiency gains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Quantization is a technique that reduces the precision of numerical representations in neural networks to improve computational efficiency and memory usage. It converts high-precision floating-point values (typically 32-bit or 16-bit) to lower-precision formats (8-bit, 4-bit, or even binary), dramatically reducing model size and accelerating inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Quantization Matters",
        "",
        "As language models grow in size (reaching hundreds of billions of parameters), they face practical deployment challenges:",
        "",
        "1. **Memory Constraints**: A 175B parameter model in full precision (FP32) requires ~700GB of memory",
        "2. **Inference Latency**: High-precision arithmetic operations are computationally expensive",
        "3. **Energy Consumption**: Higher precision calculations consume more power",
        "4. **Deployment Limitations**: Full-precision models might not run on edge devices or consumer hardware",
        "",
        "Quantization addresses these issues, enabling:",
        "- Up to 4x reduction in memory footprint when moving from FP32 to INT8",
        "- 2-3x speedup in inference time",
        "- Lower energy consumption",
        "- Deployment on resource-constrained devices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundations",
        "",
        "Quantization maps values from a high-precision space to a discrete, lower-precision space. The process involves determining a scale and zero-point to map floating-point values to integers.",
        "",
        "For a tensor with floating-point values $x_f$, the quantization process to integers $x_q$ can be represented as:",
        "",
        "$$x_q = \\text{round}\\left(\\frac{x_f}{s} + z\\right)$$",
        "",
        "Where:",
        "- $s$ is the scale factor (maps the range of float values to integer range)",
        "- $z$ is the zero-point (the integer value representing 0 in the floating-point space)",
        "- $\\text{round}$ is the rounding operation to the nearest integer",
        "",
        "The inverse operation (dequantization) converts back to floating-point:",
        "",
        "$$x_f = s \\cdot (x_q - z)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Quantization Process:",
        "",
        "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "   \u2502   Float Tensor    \u2502            \u2502 Quantized Tensor  \u2502",
        "   \u2502 (32-bit/16-bit)   \u2502    -->     \u2502  (8-bit/4-bit)    \u2502",
        "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "          \u25b2                                  \u25b2",
        "          \u2502                                  \u2502",
        "          \u2502                                  \u2502",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502   FP32 Value       \u2502            \u2502 INT8 Value         \u2502",
        "\u2502 (e.g., 0.234375)   \u2502            \u2502 (e.g., 30)         \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "          \u25b2                                  \u25b2",
        "          \u2502                                  \u2502",
        "          \u2502                                  \u2502",
        "          \u2502                                  \u2502",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502  32 bits           \u2502            \u2502  8 bits            \u2502",
        "\u2502  0100 0111 0111    \u2502            \u2502  0001 1110         \u2502",
        "\u2502  1000 0000 0000... \u2502            \u2502                    \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "",
        "    Storage: 4 bytes                Storage: 1 byte",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Quantization",
        "",
        "1. **Post-Training Quantization (PTQ)**",
        "   - Applied after model training without retraining",
        "   - Calibration on a small dataset to determine quantization parameters",
        "   - Faster to implement, but may have higher accuracy impact",
        "",
        "2. **Quantization-Aware Training (QAT)**",
        "   - Incorporates quantization effects during training",
        "   - Simulates quantization in the forward pass, but uses full precision in backward pass",
        "   - Better accuracy preservation, but requires retraining",
        "",
        "3. **Dynamic Quantization**",
        "   - Weights are quantized once, activations are quantized on-the-fly",
        "   - Good balance between accuracy and performance for NLP models",
        "   - Less memory reduction than static quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quantization Methods by Bit-Width",
        "",
        "| Precision | Bits | Value Range | Memory Reduction | Common Applications |",
        "|-----------|------|-------------|------------------|---------------------|",
        "| FP32      | 32   | Vast        | Baseline         | Training, Research  |",
        "| FP16/BF16 | 16   | Large       | 2x               | Training, High-end Inference |",
        "| INT8      | 8    | 256 values  | 4x               | Production Inference |",
        "| INT4      | 4    | 16 values   | 8x               | Edge Devices, Mobile |",
        "| Binary    | 1    | 2 values    | 32x              | Extremely Constrained Devices |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Advanced Quantization Techniques",
        "",
        "1. **Mixed-Precision Quantization**",
        "   - Different bit-widths for different parts of the model",
        "   - Sensitive layers (e.g., first and last) kept at higher precision",
        "   - Balances accuracy and efficiency",
        "",
        "2. **Vector Quantization**",
        "   - Compresses weights by grouping them into clusters",
        "   - Each weight replaced by an index to its cluster centroid",
        "   - Often used in conjunction with scalar quantization",
        "",
        "3. **Zero-Shot Quantization**",
        "   - Quantizes models without requiring calibration data",
        "   - Uses statistical properties of weights and activations",
        "   - Important for privacy-sensitive applications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Challenges and Mitigations",
        "",
        "1. **Accuracy Degradation**",
        "   - Outlier handling with per-channel quantization",
        "   - Smooth Quant: Redistribute outliers between weights and activations",
        "   - GPTQ: Layerwise quantization that minimizes error",
        "",
        "2. **Attention Layers**",
        "   - Particularly sensitive to quantization",
        "   - Often kept at higher precision (e.g., INT8 with FP16 attention)",
        "   - Special handling for softmax operations",
        "",
        "3. **Hardware Support**",
        "   - Hardware acceleration varies by platform",
        "   - INT8 widely supported, INT4 growing",
        "   - Tensor cores and specialized instructions boost performance",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "import time",
        "",
        "# Load model",
        "model_name = \"gpt2\"",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "",
        "# Load in different precision formats",
        "fp32_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float32)",
        "fp16_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)",
        "int8_model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True)",
        "",
        "# Compare model sizes",
        "def get_model_size(model):",
        "    param_size = 0",
        "    for param in model.parameters():",
        "        param_size += param.nelement() * param.element_size()",
        "    buffer_size = 0",
        "    for buffer in model.buffers():",
        "        buffer_size += buffer.nelement() * buffer.element_size()",
        "    return (param_size + buffer_size) / 1024**2  # Size in MB",
        "",
        "print(f\"FP32 model size: {get_model_size(fp32_model):.2f} MB\")",
        "print(f\"FP16 model size: {get_model_size(fp16_model):.2f} MB\")",
        "print(f\"INT8 model size: {get_model_size(int8_model):.2f} MB\")",
        "",
        "# Compare inference speed",
        "input_text = \"Artificial intelligence is revolutionizing \" * 5  # Repeated to make a longer sequence",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")",
        "",
        "# Measure inference time for each model",
        "def measure_inference_time(model, inputs, num_runs=10):",
        "    model.eval()",
        "    with torch.no_grad():",
        "        # Warmup",
        "        _ = model.generate(**inputs, max_length=100)",
        "        ",
        "        # Measure time",
        "        start_time = time.time()",
        "        for _ in range(num_runs):",
        "            _ = model.generate(**inputs, max_length=100)",
        "        end_time = time.time()",
        "    ",
        "    return (end_time - start_time) / num_runs",
        "",
        "print(f\"FP32 inference time: {measure_inference_time(fp32_model, inputs):.4f} seconds per run\")",
        "print(f\"FP16 inference time: {measure_inference_time(fp16_model, inputs):.4f} seconds per run\")",
        "print(f\"INT8 inference time: {measure_inference_time(int8_model, inputs):.4f} seconds per run\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Applications",
        "",
        "Quantization has transformed large model deployment:",
        "",
        "1. **Mobile AI**: Enabling on-device inference for applications like keyboard prediction, translation",
        "2. **Cloud Cost Reduction**: Reducing inference costs in production environments",
        "3. **Edge Computing**: Running models on IoT devices with limited resources",
        "4. **Large Model Deployment**: Making models like LLaMA-2 70B usable on consumer hardware",
        "5. **Real-time Applications**: Achieving latency requirements for interactive systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation Considerations",
        "",
        "When implementing quantization:",
        "",
        "1. **Choose the Right Method**: PTQ for simplicity, QAT for maximum accuracy",
        "2. **Model Architecture Impacts**: Transformers generally quantize well, but attention requires care",
        "3. **Layer Selection**: Consider keeping first and last layers at higher precision",
        "4. **Calibration Data**: Representative data improves quantization parameter selection",
        "5. **Hardware Alignment**: Select quantization scheme matching your deployment target",
        "",
        "Quantization represents one of the most practical advancements in AI deployment, democratizing access to powerful models by enabling them to run efficiently on widely available hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. **Distillation**",
        "- Training a smaller model to mimic a larger one.",
        "- Faster inference, smaller size, while retaining much of the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Knowledge Distillation is a model compression technique where a smaller model (student) is trained to mimic the behavior of a larger, more powerful model (teacher). Originally proposed by Hinton, Vinyals, and Dean in 2015, distillation transfers knowledge from complex models to simpler ones, enabling efficient deployment while preserving much of the original performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Knowledge Distillation Works",
        "",
        "Knowledge distillation works because of two key insights:",
        "",
        "1. **Dark Knowledge**: Large models encode rich information in their output distributions, not just in their top predictions. Even when a teacher model assigns low probabilities to incorrect classes, the relative rankings of these probabilities contain valuable information.",
        "",
        "2. **Smoother Probability Distributions**: The teacher's softened probability distribution provides better training signals than hard labels, making optimization easier for the student model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundations",
        "",
        "In standard classification, models are trained using one-hot encoded labels and a cross-entropy loss. Knowledge distillation modifies this approach by incorporating the teacher model's predictions.",
        "",
        "The distillation process typically uses a softened softmax, controlled by a temperature parameter T:",
        "",
        "$$p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$",
        "",
        "Where:",
        "- $z_i$ are the logits (pre-softmax outputs) for class i",
        "- T is the temperature (typically > 1)",
        "- Higher T produces a softer probability distribution",
        "",
        "The distillation loss is then computed as the KL divergence between the teacher and student softened distributions:",
        "",
        "$$L_{distill} = D_{KL}(p^{teacher}||p^{student})$$",
        "",
        "The total loss combines distillation loss with the standard task loss:",
        "",
        "$$L_{total} = \\alpha \\cdot L_{task} + (1-\\alpha) \\cdot L_{distill}$$",
        "",
        "Where $\\alpha$ balances task performance and mimicking the teacher."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Knowledge Distillation Process:",
        "",
        "      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "      \u2502  Training Data  \u2502",
        "      \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502     \u2502",
        "            \u2502     \u2502",
        "            \u25bc     \u2502",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502",
        "\u2502    Teacher Model    \u2502 \u2502",
        "\u2502  (Large, Complex)   \u2502 \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502",
        "          \u2502             \u2502",
        "          \u2502             \u2502",
        "          \u25bc             \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 Soft Targets     \u2502 \u2502 Hard Targets     \u2502",
        "\u2502 (Probabilities)  \u2502 \u2502 (Ground Truth)   \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "         \u2502                    \u2502",
        "         \u2502                    \u2502",
        "         \u25bc                    \u25bc",
        "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "     \u2502     Combined Loss          \u2502",
        "     \u2502 \u03b1\u00b7L_task + (1-\u03b1)\u00b7L_distill \u2502",
        "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                  \u2502",
        "                  \u2502",
        "                  \u25bc",
        "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "        \u2502   Student Model    \u2502",
        "        \u2502 (Small, Efficient) \u2502",
        "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Knowledge Distillation",
        "",
        "1. **Output Distillation (Original KD)**",
        "   - Matching final layer output distributions",
        "   - Simplest approach, works well for classification",
        "",
        "2. **Feature Distillation**",
        "   - Matching intermediate representations",
        "   - Helps with transfer learning and complex tasks",
        "   - Student needs to have comparable feature maps",
        "",
        "3. **Relation Distillation**",
        "   - Matching relationships between examples or features",
        "   - Preserves structural knowledge",
        "   - Examples include attention transfer and correlation congruence",
        "",
        "4. **Self-Distillation**",
        "   - Model serves as both teacher and student",
        "   - Earlier checkpoints or ensemble of models teach final model",
        "   - Surprisingly effective without requiring larger models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Advanced Distillation Techniques",
        "",
        "1. **Born-Again Networks**",
        "   - Iterative distillation where student becomes teacher",
        "   - Can surpass original teacher performance",
        "",
        "2. **Multi-Teacher Distillation**",
        "   - Combining knowledge from multiple specialized teachers",
        "   - Student can integrate complementary strengths",
        "",
        "3. **Online Distillation**",
        "   - Teacher and student trained simultaneously",
        "   - Deep mutual learning, where peers teach each other",
        "",
        "4. **Task-Specific Distillation**",
        "   - For language models:",
        "     - Response-Based: Output token probabilities",
        "     - Feature-Based: Hidden states, attention matrices",
        "     - Relation-Based: Pairwise relationships between tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distillation Performance Tradeoffs",
        "",
        "| Approach | Size Reduction | Speed Improvement | Performance Retention |",
        "|----------|----------------|-------------------|------------------------|",
        "| Simple KD | 2-4x | 2-5x | 90-95% |",
        "| Feature KD | 2-4x | 2-5x | 95-98% |",
        "| Ensemble KD | 5-10x | 5-10x | 85-95% |",
        "| Progressive KD | 10-100x | 10-100x | 70-90% |",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "from transformers import BertModel, BertTokenizer, BertConfig",
        "",
        "# Teacher model (pretrained BERT)",
        "teacher_model = BertModel.from_pretrained('bert-base-uncased')",
        "teacher_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')",
        "",
        "# Student model (smaller version of BERT)",
        "student_config = BertConfig.from_pretrained('bert-base-uncased')",
        "student_config.num_hidden_layers = 3  # Reduced layers",
        "student_config.hidden_size = 256      # Reduced hidden size",
        "student_model = BertModel(student_config)",
        "",
        "# Distillation loss function",
        "def distillation_loss(student_outputs, teacher_outputs, alpha=0.5, temperature=2.0):",
        "    \"\"\"",
        "    Compute the knowledge distillation loss between student and teacher outputs.",
        "    - alpha: weight for distillation loss vs task-specific loss",
        "    - temperature: softening parameter",
        "    \"\"\"",
        "    # Get the last hidden states",
        "    student_hidden = student_outputs.last_hidden_state",
        "    teacher_hidden = teacher_outputs.last_hidden_state",
        "    ",
        "    # Adapt dimensions if needed",
        "    if student_hidden.shape != teacher_hidden.shape:",
        "        # Add projection layer if shapes don't match",
        "        projection = nn.Linear(student_hidden.shape[-1], teacher_hidden.shape[-1]).to(student_hidden.device)",
        "        student_hidden = projection(student_hidden)",
        "    ",
        "    # MSE loss between hidden states",
        "    mse_loss = F.mse_loss(student_hidden, teacher_hidden)",
        "    ",
        "    return mse_loss",
        "",
        "# Example distillation process",
        "def distill_example():",
        "    # Sample input",
        "    inputs = teacher_tokenizer(\"Knowledge distillation helps create smaller models.\", return_tensors=\"pt\")",
        "    ",
        "    # Teacher forward pass",
        "    with torch.no_grad():",
        "        teacher_outputs = teacher_model(**inputs)",
        "    ",
        "    # Student forward pass",
        "    student_outputs = student_model(**inputs)",
        "    ",
        "    # Compute distillation loss",
        "    loss = distillation_loss(student_outputs, teacher_outputs)",
        "    ",
        "    # In a real scenario, you would now:",
        "    # optimizer.zero_grad()",
        "    # loss.backward()",
        "    # optimizer.step()",
        "    ",
        "    return loss",
        "",
        "loss = distill_example()",
        "print(f\"Distillation loss: {loss.item():.4f}\")",
        "",
        "# Compare model sizes",
        "teacher_params = sum(p.numel() for p in teacher_model.parameters())",
        "student_params = sum(p.numel() for p in student_model.parameters())",
        "",
        "print(f\"Teacher parameters: {teacher_params:,}\")",
        "print(f\"Student parameters: {student_params:,}\")",
        "print(f\"Size reduction: {(1 - student_params/teacher_params)*100:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Applications of Distillation",
        "",
        "Knowledge distillation has enabled many practical applications:",
        "",
        "1. **Mobile and Edge Deployment**",
        "   - DistilBERT: 40% smaller, 60% faster, retains 97% performance",
        "   - MobileBERT: 4.3x smaller, 5.5x faster",
        "   - TinyBERT: 7.5x smaller, 9.4x faster",
        "",
        "2. **Production Deployment Benefits**",
        "   - Lower inference costs in cloud environments",
        "   - Reduced latency for real-time applications",
        "   - Smaller memory footprint for concurrent serving",
        "",
        "3. **Specialized Applications**",
        "   - On-device assistants with distilled language models",
        "   - Real-time translation with compact models",
        "   - Lightweight recommendation systems",
        "",
        "4. **Augmented Training**",
        "   - Using teacher hints to make student training more efficient",
        "   - Transfer of domain expertise to general models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation Considerations",
        "",
        "When implementing knowledge distillation:",
        "",
        "1. **Architecture Alignment**",
        "   - Student should have a similar architecture family as teacher",
        "   - Consider intermediate feature matching only at compatible layers",
        "",
        "2. **Layer Mapping Strategies**",
        "   - Uniform mapping: regular intervals",
        "   - Last-layer focused: emphasize higher-level features",
        "   - Task-specific: prioritize layers most relevant to the task",
        "",
        "3. **Training Dynamics**",
        "   - Higher learning rates often work well for student models",
        "   - Progressive distillation: gradually increase complexity",
        "   - Early stopping based on student validation performance, not loss",
        "",
        "4. **Data Considerations**",
        "   - Unlabeled data can be utilized (teacher provides soft targets)",
        "   - Data augmentation particularly beneficial for distillation",
        "   - Selective distillation on examples where teacher is confident",
        "",
        "Knowledge distillation remains one of the most practical approaches to deploy powerful AI models in resource-constrained environments while preserving most of their capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. **RLHF (Reinforcement Learning with Human Feedback)**",
        "- Trains models to align with human values and preferences.",
        "- Used in ChatGPT and other conversational agents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Reinforcement Learning from Human Feedback (RLHF) is a powerful technique that helps align language models with human preferences and values. It addresses a critical limitation of traditional training methods: they optimize for statistical pattern matching rather than producing outputs that humans would prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Problem RLHF Solves",
        "",
        "Traditional language models are typically trained with two objectives:",
        "",
        "1. **Next-token prediction** (supervised learning on text corpora)",
        "2. **Imitation learning** (fine-tuning on human-written demonstrations)",
        "",
        "However, these approaches don't directly optimize for what humans actually want from the model. Models may learn to:",
        "- Generate toxic or harmful content",
        "- Produce factually incorrect but plausible-sounding text",
        "- Optimize for metrics that don't align with human preferences",
        "",
        "RLHF introduces human judgment directly into the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How RLHF Works: The Three-Step Process",
        "",
        "```",
        "RLHF Training Pipeline:",
        "",
        "Step 1: Supervised Fine-Tuning",
        "+---------------------+",
        "| Pretrained LM       |  ->  Fine-tuned on human demonstrations",
        "+---------------------+",
        "          |",
        "          v",
        "+---------------------+",
        "| SFT Model           |  (Supervised Fine-Tuned Model)",
        "+---------------------+",
        "          |",
        "          v",
        "Step 2: Reward Model Training",
        "+---------------------+",
        "| Human Preferences   |  ->  Train reward model to predict human preferences",
        "| (A better than B)   |",
        "+---------------------+",
        "          |",
        "          v",
        "+---------------------+",
        "| Reward Model (RM)   |  ->  Scores responses based on human preferences",
        "+---------------------+",
        "          |",
        "          v",
        "Step 3: Reinforcement Learning Optimization",
        "+---------------------+",
        "| SFT Model           |  ->  Optimized with RL to maximize rewards",
        "+---------------------+",
        "          |",
        "          v",
        "+---------------------+",
        "| RLHF Model          |  ->  Final model aligned with human preferences",
        "+---------------------+",
        "```",
        "",
        "Let's break down each step:",
        "",
        "1. **Supervised Fine-Tuning (SFT)**",
        "   - Start with a pretrained language model",
        "   - Fine-tune it on high-quality human demonstrations",
        "   - This creates a decent baseline that can follow instructions",
        "",
        "2. **Reward Model Training**",
        "   - Collect pairs of model responses to the same prompt",
        "   - Ask humans to rank which response they prefer",
        "   - Train a reward model to predict these human preferences",
        "   - The reward model learns to score outputs based on their quality",
        "",
        "3. **Reinforcement Learning Optimization**",
        "   - Use Proximal Policy Optimization (PPO) or other RL algorithms",
        "   - The SFT model is the initial policy",
        "   - For each prompt, generate outputs and compute their rewards",
        "   - Update the policy to maximize expected rewards",
        "   - Add a KL divergence penalty to prevent the model from deviating too much from the original SFT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Key Components in RLHF",
        "",
        "1. **Human Feedback Collection**",
        "   - Typically involves presenting humans with:",
        "     * A specific prompt",
        "     * Multiple model-generated responses",
        "     * Instructions to rank or rate the responses",
        "   - Must be designed carefully to reduce bias",
        "   - Often uses structured criteria (helpfulness, harmlessness, honesty)",
        "",
        "2. **Reward Modeling**",
        "   - Neural network trained to predict human preferences",
        "   - Usually based on the same architecture as the policy model",
        "   - Scores = estimator of \"human preferred-ness\"",
        "   - The quality of the reward model directly impacts final model performance",
        "",
        "3. **PPO Algorithm in RLHF**",
        "   - Actor (policy) = language model being trained",
        "   - Critic = estimates value of current policy",
        "   - Objective = maximize reward while staying close to SFT model",
        "   - Uses tricks like value clipping and advantage normalization for stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Formulation",
        "",
        "The RLHF objective can be expressed as:",
        "",
        "```",
        "Maximize: E[R(x,y)] - \u03b2 * KL[\u03c0_new(y|x) || \u03c0_old(y|x)]",
        "",
        "Where:",
        "- R(x,y) is the reward for generating output y given input x",
        "- KL is the Kullback-Leibler divergence",
        "- \u03c0_new is the new policy (model being trained)",
        "- \u03c0_old is the original policy (SFT model)",
        "- \u03b2 is a hyperparameter controlling deviation from the original model",
        "```",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "import numpy as np",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer",
        "from tqdm import tqdm",
        "",
        "# In a real implementation, you would use distributed computing frameworks like Ray",
        "# and specialized RL libraries. This is a simplified educational version.",
        "",
        "class RLHFTrainer:",
        "    def __init__(self, sft_model_name, reward_model_name, kl_coef=0.1,",
        "                 lr=1e-5, batch_size=4):",
        "        \"\"\"",
        "        Initialize RLHF trainer",
        "        ",
        "        Args:",
        "            sft_model_name: Name or path of the supervised fine-tuned model",
        "            reward_model_name: Name or path of the reward model",
        "            kl_coef: KL divergence coefficient (controls deviation from SFT model)",
        "            lr: Learning rate",
        "            batch_size: Batch size for training",
        "        \"\"\"",
        "        # Policy model (the one we're optimizing)",
        "        self.policy_model = GPT2LMHeadModel.from_pretrained(sft_model_name)",
        "        self.policy_tokenizer = GPT2Tokenizer.from_pretrained(sft_model_name)",
        "        ",
        "        # Reference model (frozen copy of initial SFT model for KL calculation)",
        "        self.ref_model = GPT2LMHeadModel.from_pretrained(sft_model_name)",
        "        for param in self.ref_model.parameters():",
        "            param.requires_grad = False",
        "        ",
        "        # Reward model",
        "        self.reward_model = GPT2LMHeadModel.from_pretrained(reward_model_name)",
        "        for param in self.reward_model.parameters():",
        "            param.requires_grad = False",
        "        ",
        "        # Hyperparameters",
        "        self.kl_coef = kl_coef",
        "        self.batch_size = batch_size",
        "        ",
        "        # Optimizer",
        "        self.optimizer = torch.optim.Adam(self.policy_model.parameters(), lr=lr)",
        "        ",
        "        # Set models to evaluation mode (we're not training in the usual way)",
        "        self.policy_model.train()",
        "        self.ref_model.eval()",
        "        self.reward_model.eval()",
        "    ",
        "    def compute_rewards(self, prompts, responses):",
        "        \"\"\"",
        "        Compute rewards for generated responses using the reward model",
        "        ",
        "        Args:",
        "            prompts: List of prompt strings",
        "            responses: List of response strings",
        "            ",
        "        Returns:",
        "            torch.Tensor of reward values",
        "        \"\"\"",
        "        rewards = []",
        "        ",
        "        with torch.no_grad():",
        "            for prompt, response in zip(prompts, responses):",
        "                # Combine prompt and response",
        "                full_text = prompt + response",
        "                ",
        "                # Tokenize",
        "                inputs = self.policy_tokenizer(full_text, return_tensors=\"pt\")",
        "                ",
        "                # Get reward model output (simplification - in real implementation",
        "                # the reward model would have specific architecture for scoring)",
        "                reward_outputs = self.reward_model(**inputs)",
        "                ",
        "                # Use the model's loss as a proxy for reward in this simplified example",
        "                # In practice, the reward model is specifically designed to output scores",
        "                reward = -reward_outputs.loss.item()",
        "                rewards.append(reward)",
        "        ",
        "        return torch.tensor(rewards)",
        "    ",
        "    def compute_kl_divergence(self, prompt_ids, response_ids):",
        "        \"\"\"",
        "        Compute KL divergence between policy and reference model",
        "        ",
        "        Args:",
        "            prompt_ids: Tensor of tokenized prompts",
        "            response_ids: Tensor of tokenized responses",
        "            ",
        "        Returns:",
        "            KL divergence values",
        "        \"\"\"",
        "        # Combine prompt and response",
        "        input_ids = torch.cat([prompt_ids, response_ids], dim=1)",
        "        ",
        "        # Create attention mask",
        "        attention_mask = torch.ones_like(input_ids)",
        "        ",
        "        # Get logits from policy model",
        "        with torch.set_grad_enabled(True):",
        "            policy_outputs = self.policy_model(input_ids=input_ids, ",
        "                                             attention_mask=attention_mask)",
        "            policy_logits = policy_outputs.logits",
        "        ",
        "        # Get logits from reference model (no gradient tracking)",
        "        with torch.no_grad():",
        "            ref_outputs = self.ref_model(input_ids=input_ids,",
        "                                       attention_mask=attention_mask)",
        "            ref_logits = ref_outputs.logits",
        "        ",
        "        # Compute KL divergence",
        "        kl_div = F.kl_div(",
        "            F.log_softmax(policy_logits, dim=-1),",
        "            F.softmax(ref_logits, dim=-1),",
        "            reduction='none'",
        "        ).sum(-1)",
        "        ",
        "        # Only consider KL divergence for response tokens",
        "        response_mask = torch.cat([",
        "            torch.zeros_like(prompt_ids), ",
        "            torch.ones_like(response_ids)",
        "        ], dim=1)",
        "        ",
        "        kl_div = (kl_div * response_mask).sum(-1) / response_mask.sum(-1)",
        "        ",
        "        return kl_div",
        "    ",
        "    def generate_responses(self, prompts, max_length=100):",
        "        \"\"\"",
        "        Generate responses for the given prompts",
        "        ",
        "        Args:",
        "            prompts: List of prompt strings",
        "            max_length: Maximum length of generated responses",
        "            ",
        "        Returns:",
        "            List of generated responses",
        "        \"\"\"",
        "        responses = []",
        "        ",
        "        for prompt in prompts:",
        "            # Tokenize prompt",
        "            inputs = self.policy_tokenizer(prompt, return_tensors=\"pt\")",
        "            ",
        "            # Generate response",
        "            with torch.no_grad():",
        "                outputs = self.policy_model.generate(",
        "                    inputs.input_ids,",
        "                    max_length=max_length,",
        "                    do_sample=True,",
        "                    temperature=0.7,",
        "                    top_p=0.9,",
        "                    pad_token_id=self.policy_tokenizer.eos_token_id",
        "                )",
        "                ",
        "                # Decode response (removing prompt)",
        "                prompt_length = inputs.input_ids.shape[1]",
        "                response = self.policy_tokenizer.decode(",
        "                    outputs[0][prompt_length:],",
        "                    skip_special_tokens=True",
        "                )",
        "                ",
        "                responses.append(response)",
        "        ",
        "        return responses",
        "    ",
        "    def ppo_step(self, prompts):",
        "        \"\"\"",
        "        Perform a single PPO optimization step",
        "        ",
        "        Args:",
        "            prompts: List of prompt strings",
        "            ",
        "        Returns:",
        "            Dictionary of training metrics",
        "        \"\"\"",
        "        # Generate responses",
        "        responses = self.generate_responses(prompts)",
        "        ",
        "        # Compute rewards",
        "        rewards = self.compute_rewards(prompts, responses)",
        "        ",
        "        # Tokenize prompts and responses",
        "        prompt_encodings = [self.policy_tokenizer(p, return_tensors=\"pt\").input_ids ",
        "                           for p in prompts]",
        "        response_encodings = [self.policy_tokenizer(r, return_tensors=\"pt\").input_ids ",
        "                             for r in responses]",
        "        ",
        "        # Compute KL divergence",
        "        kl_divs = []",
        "        for p_enc, r_enc in zip(prompt_encodings, response_encodings):",
        "            kl_div = self.compute_kl_divergence(p_enc, r_enc)",
        "            kl_divs.append(kl_div.item())",
        "        ",
        "        kl_divs = torch.tensor(kl_divs)",
        "        ",
        "        # Compute PPO loss",
        "        # (simplified - real PPO has more components)",
        "        loss = -(rewards - self.kl_coef * kl_divs).mean()",
        "        ",
        "        # Backward pass and optimize",
        "        self.optimizer.zero_grad()",
        "        loss.backward()",
        "        self.optimizer.step()",
        "        ",
        "        # Return metrics",
        "        return {",
        "            \"loss\": loss.item(),",
        "            \"rewards\": rewards.mean().item(),",
        "            \"kl_div\": kl_divs.mean().item()",
        "        }",
        "    ",
        "    def train(self, prompt_dataset, num_epochs=3):",
        "        \"\"\"",
        "        Train the policy model using RLHF",
        "        ",
        "        Args:",
        "            prompt_dataset: Dataset of prompts",
        "            num_epochs: Number of training epochs",
        "            ",
        "        Returns:",
        "            Training history",
        "        \"\"\"",
        "        history = []",
        "        ",
        "        for epoch in range(num_epochs):",
        "            epoch_metrics = {",
        "                \"loss\": 0,",
        "                \"rewards\": 0,",
        "                \"kl_div\": 0",
        "            }",
        "            ",
        "            # Process dataset in batches",
        "            num_batches = len(prompt_dataset) // self.batch_size",
        "            ",
        "            for i in tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\"):",
        "                # Get batch of prompts",
        "                batch_start = i * self.batch_size",
        "                batch_end = (i + 1) * self.batch_size",
        "                batch_prompts = prompt_dataset[batch_start:batch_end]",
        "                ",
        "                # Perform PPO step",
        "                batch_metrics = self.ppo_step(batch_prompts)",
        "                ",
        "                # Update epoch metrics",
        "                for k, v in batch_metrics.items():",
        "                    epoch_metrics[k] += v / num_batches",
        "            ",
        "            history.append(epoch_metrics)",
        "            print(f\"Epoch {epoch+1}/{num_epochs}: {epoch_metrics}\")",
        "        ",
        "        return history",
        "",
        "# Example usage (would need actual models and datasets)",
        "def rlhf_example():",
        "    # This is just a sketch - you would need:",
        "    # 1. An actual SFT model (e.g., GPT trained on high-quality demonstrations)",
        "    # 2. A trained reward model",
        "    # 3. A dataset of prompts",
        "    ",
        "    # Example prompts (in real scenario, this would be a larger dataset)",
        "    example_prompts = [",
        "        \"Write a poem about nature.\",",
        "        \"Explain quantum physics to a 5-year-old.\",",
        "        \"Write a short story about a robot finding emotions.\",",
        "        \"Summarize the history of the internet.\"",
        "    ]",
        "    ",
        "    # Initialize trainer (with models you've trained separately)",
        "    # trainer = RLHFTrainer(",
        "    #     sft_model_name=\"path/to/your/sft/model\",",
        "    #     reward_model_name=\"path/to/your/reward/model\"",
        "    # )",
        "    ",
        "    # Train with RLHF",
        "    # history = trainer.train(example_prompts)",
        "    ",
        "    print(\"RLHF training would optimize the model to align with human preferences\")",
        "",
        "# rlhf_example()  # Uncomment to run a simulated example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Impact",
        "",
        "RLHF has been a game-changer for language model development:",
        "",
        "1. **ChatGPT and GPT-4**: OpenAI used RLHF extensively to make these models more helpful, harmless, and honest",
        "2. **Claude**: Anthropic developed Constitutional AI, a variation of RLHF",
        "3. **Open-Source Models**: Many open-source models (like Llama-2) now incorporate RLHF",
        "",
        "The key impacts include:",
        "",
        "- **Reduced harmful outputs**: Models are better at avoiding toxic, biased, or harmful content",
        "- **Improved helpfulness**: Responses are more useful, direct, and relevant to user queries",
        "- **Better instruction following**: Models better understand and adhere to given instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Limitations and Challenges",
        "",
        "RLHF isn't perfect and faces several challenges:",
        "",
        "1. **Quality of human feedback**: The final model is only as good as the human preferences it learns from",
        "2. **Scaling feedback collection**: Gathering high-quality human feedback is expensive and doesn't scale as easily as collecting more pretraining data",
        "3. **Reward hacking**: Models may learn to optimize for the reward function rather than true human preferences",
        "4. **Diversity of human preferences**: Different humans have different preferences, making it hard to create a universally preferred model",
        "",
        "Despite these challenges, RLHF represents one of the most important advances in aligning powerful AI systems with human values, making it a crucial technique for developing safer, more beneficial AI systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. **Fine-Tuning**",
        "- Training a pre-trained model on **your custom data** to make it more task-specific.",
        "- Can be done on the entire model or specific components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Fine-tuning is a transfer learning technique that adapts a pre-trained model to a specific task or domain by updating its parameters using task-specific data. This approach leverages knowledge acquired during pre-training on vast datasets to achieve strong performance on downstream tasks with much less data and training time than would be required for training from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Fine-Tuning Works",
        "",
        "Fine-tuning leverages several key principles of machine learning:",
        "",
        "1. **Transfer Learning**: Knowledge from general domains can be transferred to specific tasks",
        "2. **Feature Reuse**: Lower layers of neural networks learn general features useful across tasks",
        "3. **Parameter Efficiency**: Adjusting existing representations is easier than learning from scratch",
        "4. **Low-Resource Adaptation**: Specialized tasks often have limited labeled data",
        "",
        "For language models, pre-training captures linguistic structures, world knowledge, and semantic relationships, providing a strong foundation that can be adapted to a wide range of downstream tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Fine-Tuning Process",
        "",
        "```",
        "Fine-Tuning Pipeline:",
        "",
        " Pre-training                   Fine-tuning                    Deployment",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502             \u2502               \u2502             \u2502               \u2502             \u2502",
        "\u2502  Massive    \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba   \u2502Task-specific\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba   \u2502   Deploy    \u2502",
        "\u2502  General    \u2502  Initialize   \u2502 Supervised  \u2502  Save model   \u2502  for task   \u2502",
        "\u2502  Dataset    \u2502   weights     \u2502  Training   \u2502               \u2502             \u2502",
        "\u2502             \u2502               \u2502             \u2502               \u2502             \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "       \u2502                             \u25b2",
        "       \u2502                             \u2502",
        "       \u2502                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "       \u2502                      \u2502  Labeled    \u2502",
        "       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502  Task Data  \u2502",
        "         Guide architecture   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "         and optimization",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Framework",
        "",
        "From a mathematical perspective, fine-tuning can be understood as optimizing the parameters of a pre-trained model $\\theta_{pre}$ to arrive at task-specific parameters $\\theta_{task}$.",
        "",
        "For a pre-trained language model with parameters $\\theta_{pre}$, fine-tuning involves finding:",
        "",
        "$$\\theta_{task} = \\arg\\min_{\\theta} \\mathcal{L}_{task}(\\theta, \\mathcal{D}_{task})$$",
        "",
        "Where:",
        "- $\\mathcal{L}_{task}$ is the task-specific loss function",
        "- $\\mathcal{D}_{task}$ is the task-specific dataset",
        "- $\\theta$ is initialized as $\\theta_{pre}$",
        "",
        "For language models, common task-specific loss functions include:",
        "",
        "1. **Classification**: Cross-entropy loss over class predictions",
        "2. **Sequence Labeling**: Token-level cross-entropy loss",
        "3. **Text Generation**: Next-token prediction loss",
        "4. **Regression**: Mean squared error loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tuning Approaches",
        "",
        "The fine-tuning landscape offers various approaches with different tradeoffs:",
        "",
        "1. **Full Fine-Tuning**",
        "   - Updates all model parameters",
        "   - Highest potential performance",
        "   - Most computationally expensive",
        "   - Highest risk of catastrophic forgetting",
        "   - Separate model copy for each task",
        "",
        "2. **Parameter-Efficient Fine-Tuning (PEFT)**",
        "   - Updates a small subset of parameters",
        "   - Nearly matching performance to full fine-tuning",
        "   - Significantly reduced computation and storage",
        "   - Main variants:",
        "     - Adapter-based (add small modules)",
        "     - LoRA (low-rank matrix updates)",
        "     - Prompt tuning (learn soft prompts)",
        "     - BitFit (bias-term only tuning)",
        "",
        "3. **Head-Only Fine-Tuning**",
        "   - Freezes pre-trained layers",
        "   - Updates only task-specific output layers",
        "   - Extremely parameter-efficient",
        "   - Lower performance but very fast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fine-Tuning Hyperparameters",
        "",
        "The success of fine-tuning depends heavily on hyperparameter selection:",
        "",
        "| Parameter | Typical Range | Impact |",
        "|-----------|---------------|--------|",
        "| Learning Rate | 1e-5 to 5e-5 | Critical - too high disrupts knowledge, too low prevents adaptation |",
        "| Batch Size | 8 to 32 | Affects gradient stability and memory usage |",
        "| Epochs | 2 to 10 | Fewer for large datasets, more for small ones |",
        "| Weight Decay | 0.01 to 0.1 | Prevents overfitting to small datasets |",
        "| Warmup Steps | 6-10% of total steps | Stabilizes early training |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Challenges and Solutions",
        "",
        "1. **Catastrophic Forgetting**",
        "   - Problem: Model loses general knowledge when adapting to specific tasks",
        "   - Solutions:",
        "     - Regularization techniques (L2, KL-divergence to original model)",
        "     - Replay methods (mix in pre-training data)",
        "     - Freezing lower layers",
        "",
        "2. **Overfitting Small Datasets**",
        "   - Problem: Model memorizes rather than generalizes when task data is limited",
        "   - Solutions:",
        "     - Data augmentation",
        "     - Early stopping",
        "     - Parameter-efficient fine-tuning",
        "",
        "3. **Training Instability**",
        "   - Problem: Training can diverge or plateau",
        "   - Solutions:",
        "     - Learning rate schedules (linear decay, cosine)",
        "     - Gradient clipping",
        "     - Layer-wise learning rate decay",
        "",
        "4. **Domain Shift**",
        "   - Problem: Task data differs significantly from pre-training distribution",
        "   - Solutions:",
        "     - Domain adaptive pre-training (intermediate training)",
        "     - Domain-specific vocabulary adaptation",
        "     - Domain-specific feature extraction",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments",
        "from datasets import load_dataset",
        "",
        "# Load model and tokenizer",
        "model_name = \"bert-base-uncased\"",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)",
        "",
        "# Load and prepare dataset (using SST-2 sentiment dataset)",
        "dataset = load_dataset(\"glue\", \"sst2\")",
        "",
        "# Preprocessing function",
        "def preprocess_function(examples):",
        "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\")",
        "",
        "# Apply preprocessing",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)",
        "",
        "# Define training arguments",
        "training_args = TrainingArguments(",
        "    output_dir=\"./results\",",
        "    num_train_epochs=3,",
        "    per_device_train_batch_size=16,",
        "    per_device_eval_batch_size=64,",
        "    warmup_steps=500,",
        "    weight_decay=0.01,",
        "    logging_dir=\"./logs\",",
        "    logging_steps=10,",
        "    evaluation_strategy=\"epoch\",",
        "    save_strategy=\"epoch\",",
        "    load_best_model_at_end=True,",
        ")",
        "",
        "# Initialize Trainer",
        "trainer = Trainer(",
        "    model=model,",
        "    args=training_args,",
        "    train_dataset=tokenized_dataset[\"train\"],",
        "    eval_dataset=tokenized_dataset[\"validation\"],",
        ")",
        "",
        "# Fine-tune the model",
        "# trainer.train()",
        "",
        "# Evaluate",
        "# eval_result = trainer.evaluate()",
        "# print(f\"Evaluation accuracy: {eval_result['eval_accuracy']:.4f}\")",
        "",
        "# Example inference",
        "text = \"This movie was fantastic!\"",
        "inputs = tokenizer(text, return_tensors=\"pt\")",
        "with torch.no_grad():",
        "    outputs = model(**inputs)",
        "    ",
        "predictions = outputs.logits.argmax(dim=-1)",
        "print(f\"Text: '{text}'\")",
        "print(f\"Sentiment: {'Positive' if predictions.item() == 1 else 'Negative'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Common Fine-Tuning Tasks and Techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Natural Language Understanding Tasks",
        "",
        "1. **Text Classification**",
        "   - Sentiment analysis, topic classification, intent detection",
        "   - Add classification head to pooled representation",
        "   - Example: BERT for sentiment analysis (as shown in code)",
        "",
        "2. **Named Entity Recognition**",
        "   - Identifying entities in text (people, organizations, locations)",
        "   - Token-level classification head",
        "   - Use BIO/IOB tagging scheme",
        "",
        "3. **Question Answering**",
        "   - Extractive QA: finds answer span in context",
        "   - Add start/end span prediction heads",
        "   - Example: SQuAD fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Natural Language Generation Tasks",
        "",
        "1. **Text Summarization**",
        "   - Adapting encoder-decoder models",
        "   - Fine-tune with target summary as generation target",
        "   - Often uses ROUGE metrics for evaluation",
        "",
        "2. **Text Completion/Generation**",
        "   - Fine-tuning decoder-only models (GPT family)",
        "   - Next-token prediction objective",
        "   - Can be steered with specific prefixes/formats",
        "",
        "3. **Machine Translation**",
        "   - Adaptation of encoder-decoder models",
        "   - Target language sequences as generation targets",
        "   - Often evaluated with BLEU score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Fine-Tuning Techniques",
        "",
        "1. **Multi-task Fine-Tuning**",
        "   - Training on multiple related tasks simultaneously",
        "   - Can improve generalization and prevent overfitting",
        "   - Requires balancing different task objectives",
        "",
        "2. **Curriculum Learning**",
        "   - Start with easier examples, gradually introduce harder ones",
        "   - Can improve final performance and convergence speed",
        "   - Requires defining notion of \"difficulty\"",
        "",
        "3. **Stage-wise Fine-Tuning**",
        "   - Intermediate fine-tuning on related task/domain",
        "   - Final fine-tuning on target task",
        "   - Example: DAPT (Domain Adaptive Pre-training) \u2192 Task fine-tuning",
        "",
        "4. **Self-training and Pseudo-labeling**",
        "   - Use small labeled dataset to train initial model",
        "   - Generate predictions on unlabeled data",
        "   - Fine-tune on combined labeled and high-confidence pseudo-labeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Considerations",
        "",
        "When implementing fine-tuning in production:",
        "",
        "1. **Resource Requirements**",
        "   - Full fine-tuning needs significant GPU memory",
        "   - Parameter-efficient methods reduce requirements by 90-99%",
        "   - Consider throughput needs for batch inference",
        "",
        "2. **Model Versioning**",
        "   - Track both base model and fine-tuning dataset provenance",
        "   - Version control for fine-tuned models",
        "   - A/B testing new fine-tuned variants",
        "",
        "3. **Evaluation Strategy**",
        "   - Task-specific metrics (accuracy, F1, BLEU, ROUGE)",
        "   - Behavioral testing for robustness",
        "   - Bias and fairness assessments",
        "",
        "4. **Deployment Options**",
        "   - Model distillation after fine-tuning for efficiency",
        "   - Quantization compatibility",
        "   - Monitoring for performance drift",
        "",
        "Fine-tuning has revolutionized NLP by enabling relatively small amounts of task-specific data to adapt powerful pre-trained models, making state-of-the-art performance accessible for specialized applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. **Beam Search / Sampling / Top-K / Top-P**",
        "- Different **decoding methods** to generate text.",
        "- Each offers different tradeoffs between diversity and quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Text generation in language models involves selecting tokens sequentially to form coherent outputs. The decoding algorithm\u2014how we select the next token given the model's probability distribution\u2014critically influences output quality, diversity, and computational efficiency. Different decoding strategies offer distinct tradeoffs between deterministic coherence and creative diversity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Text Generation Framework",
        "",
        "For a language model with parameters \u03b8, the conditional probability of the next token $y_t$ given previous tokens $y_{<t}$ is:",
        "",
        "$$p_\u03b8(y_t|y_{<t})$$",
        "",
        "Text generation involves sampling from or maximizing this probability distribution sequentially until an end condition is met (e.g., maximum length or end token).",
        "",
        "The fundamental challenge is balancing:",
        "- **Quality**: Producing coherent, grammatical, factual text",
        "- **Diversity**: Generating varied, creative, non-repetitive outputs",
        "- **Computational efficiency**: Managing resources during inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Comparison of Decoding Methods",
        "",
        "```",
        "Decoding Methods Spectrum:",
        "",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                                                               \u2502",
        "\u2502  Deterministic         Controlled Randomness         Highly   \u2502",
        "\u2502                                                     Random    \u2502",
        "\u2502                                                               \u2502",
        "\u2502  Greedy   \u2500\u2500\u2500\u2500\u25ba  Beam  \u2500\u2500\u2500\u2500\u25ba  Top-K  \u2500\u2500\u2500\u2500\u25ba  Top-p  \u2500\u2500\u2500\u2500\u25ba  Pure\u2502",
        "\u2502  Search          Search        Sampling      Sampling   Sampling",
        "\u2502                                                               \u2502",
        "\u2502  Quality:    High                              Low            \u2502",
        "\u2502  Diversity:  Low                               High           \u2502",
        "\u2502  Repetition: High                              Low            \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Greedy Search",
        "",
        "The simplest decoding strategy selects the single most probable token at each step:",
        "",
        "$$y_t = \\arg\\max_{w \\in V} p_\u03b8(w|y_{<t})$$",
        "",
        "Where:",
        "- $V$ is the vocabulary",
        "- $y_t$ is the token selected at position t",
        "",
        "**Characteristics:**",
        "- Deterministic: Same input always produces same output",
        "- Very fast (only needs to evaluate top candidate)",
        "- Often produces repetitive text",
        "- Tends to get stuck in loops (\"the the the...\")",
        "- Cannot recover from mistakes",
        "",
        "**Mathematical Representation:**",
        "The sequence probability under greedy search is:",
        "",
        "$$P(Y) = \\prod_{t=1}^{T} \\max_{w \\in V} p_\u03b8(w|y_{<t})$$",
        "",
        "**Visual Representation:**",
        "```",
        "Greedy Search Example:",
        "",
        "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "   \u2502          \"The cat sat\"            \u2502",
        "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                       \u2502",
        "                       \u25bc",
        "             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "             \u2502  LLM Probability \u2502",
        "             \u2502   Distribution   \u2502",
        "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                      \u2502",
        "                      \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"on\": 0.5, \"under\": 0.2, \"beside\": 0.1 \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u2502 Always choose highest probability",
        "           \u25bc",
        "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "     \u2502    \"on\"    \u2502",
        "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502       \"The cat sat on\"            \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Beam Search",
        "",
        "Beam search maintains the k most probable sequences at each step, offering a middle ground between greedy search and exhaustive exploration:",
        "",
        "1. Start with k candidate sequences (initially just start token)",
        "2. For each candidate, compute probability of all possible next tokens",
        "3. Select k best new sequences from all candidates' extensions",
        "4. Repeat until termination condition",
        "",
        "**Mathematical Formulation:**",
        "At each step t, beam search maximizes:",
        "",
        "$$\\text{BeamSet}_t = \\text{top-k}_{Y \\in \\text{Candidates}} \\prod_{i=1}^{t} p_\u03b8(y_i|y_{<i})$$",
        "",
        "Where:",
        "- Candidates = all possible one-token extensions of sequences in BeamSet_{t-1}",
        "- top-k selects the k sequences with highest probability",
        "",
        "**Characteristics:**",
        "- Still relatively deterministic",
        "- Explores multiple paths simultaneously",
        "- Computationally more expensive than greedy search",
        "- Tends to favor shorter sequences (addressable with length normalization)",
        "- Can suffer from lack of diversity (all beams may be similar)",
        "",
        "**Visual Representation:**",
        "```",
        "Beam Search (k=2) Example:",
        "",
        "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "              \u2502   \"The cat sat\"   \u2502",
        "              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                        \u2502",
        "                        \u25bc",
        "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "              \u2502 Model Prediction \u2502",
        "              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                        \u2502",
        "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "   \u2502                                      \u2502",
        "   \u25bc                                      \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502    \"on\" (0.5)    \u2502              \u2502   \"under\" (0.2)  \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "         \u2502                                 \u2502",
        "         \u25bc                                 \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"The cat sat on\" \u2502              \u2502\"The cat sat under\"\u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "         \u2502                                 \u2502",
        "         \u25bc                                 \u25bc",
        "     Continue with                    Continue with",
        "     these 2 beams                    these 2 beams",
        "```",
        "",
        "**Beam Search Enhancements:**",
        "1. **Length Normalization**: Dividing log probability by sequence length^\u03b1",
        "   $$\\text{score}(Y) = \\frac{\\log P(Y)}{|Y|^\u03b1}$$",
        "   ",
        "2. **Diverse Beam Search**: Adding diversity penalties between beams",
        "3. **Constrained Beam Search**: Enforcing specific constraints on outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Pure Sampling",
        "",
        "Pure sampling draws the next token randomly according to the model's probability distribution:",
        "",
        "$$y_t \\sim p_\u03b8(y_t|y_{<t})$$",
        "",
        "**Characteristics:**",
        "- Highly diverse outputs",
        "- Can be very creative",
        "- Often produces incoherent text",
        "- High entropy/unpredictability",
        "- Useful for creative applications",
        "",
        "**Mathematical Representation:**",
        "Pure sampling selects token w with probability:",
        "",
        "$$P(y_t = w) = p_\u03b8(w|y_{<t})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Top-K Sampling",
        "",
        "Top-K sampling restricts random sampling to only the k most likely next tokens:",
        "",
        "1. Get model's next-token probability distribution",
        "2. Keep only the k highest probability tokens",
        "3. Renormalize probabilities to sum to 1",
        "4. Sample from this truncated distribution",
        "",
        "$$y_t \\sim \\text{Normalize}(\\text{Top-K}_{w \\in V} p_\u03b8(w|y_{<t}))$$",
        "",
        "Where Normalize ensures the probabilities sum to 1:",
        "",
        "$$\\text{Normalize}(p_1, p_2, ..., p_k) = \\frac{(p_1, p_2, ..., p_k)}{\\sum_{i=1}^{k} p_i}$$",
        "",
        "**Characteristics:**",
        "- Balances quality and diversity",
        "- Prevents sampling from low-probability (often nonsensical) tokens",
        "- Fixed k can be too restrictive for some contexts and too permissive for others",
        "- Typical values: k=10 to 50",
        "",
        "**Visual Representation:**",
        "```",
        "Top-K Sampling (k=3) Example:",
        "",
        "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "   \u2502          \"The cat sat\"            \u2502",
        "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                       \u2502",
        "                       \u25bc",
        "             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "             \u2502  LLM Probability \u2502",
        "             \u2502   Distribution   \u2502",
        "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                      \u2502",
        "                      \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"on\": 0.5, \"under\": 0.2, \"beside\": 0.1, ...others  \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u2502 Keep top 3, renormalize",
        "           \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"on\": 0.625, \"under\": 0.25, \"beside\": 0.125 \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u2502 Sample from this distribution",
        "           \u25bc",
        "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "     \u2502  Selected  \u2502",
        "     \u2502   Token    \u2502",
        "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Top-p (Nucleus) Sampling",
        "",
        "Top-p sampling (also called nucleus sampling) dynamically selects the smallest set of tokens whose cumulative probability exceeds threshold p:",
        "",
        "1. Sort tokens by probability",
        "2. Select minimal set of highest probability tokens with sum \u2265 p",
        "3. Renormalize and sample from this set",
        "",
        "$$V_p = \\min\\{k : \\sum_{i=1}^{k} p_\u03b8(w_i|y_{<t}) \\geq p\\}$$",
        "$$y_t \\sim \\text{Normalize}(\\{p_\u03b8(w|y_{<t}) : w \\in V_p\\})$$",
        "",
        "Where $V_p$ is the smallest vocabulary subset whose cumulative probability \u2265 p.",
        "",
        "**Characteristics:**",
        "- Adaptive: nucleus size changes based on confidence",
        "- Works well across varying contexts",
        "- Balances quality and diversity better than fixed top-k",
        "- Handles both confident predictions (small nucleus) and uncertain ones (larger nucleus)",
        "- Typical values: p=0.9 or 0.95",
        "",
        "**Visual Representation:**",
        "```",
        "Top-p (Nucleus) Sampling (p=0.8) Example:",
        "",
        "   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "   \u2502          \"The cat sat\"            \u2502",
        "   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                       \u2502",
        "                       \u25bc",
        "             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "             \u2502  LLM Probability \u2502",
        "             \u2502   Distribution   \u2502",
        "             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                      \u2502",
        "                      \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"on\": 0.5, \"under\": 0.2, \"beside\": 0.1, \"near\": 0.05... \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u2502 Select tokens until sum \u2265 0.8",
        "           \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \"on\": 0.5, \"under\": 0.2, \"beside\": 0.1 \u2502 (sum = 0.8)",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "           \u2502",
        "           \u2502 Renormalize and sample",
        "           \u25bc",
        "     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "     \u2502  Selected  \u2502",
        "     \u2502   Token    \u2502",
        "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6. Temperature Sampling",
        "",
        "Temperature (\u03c4) modifies the probability distribution by adjusting its \"sharpness\":",
        "",
        "$$p_\u03c4(y_t = w|y_{<t}) = \\frac{\\exp(\\log p_\u03b8(w|y_{<t}) / \u03c4)}{\\sum_{w' \\in V} \\exp(\\log p_\u03b8(w'|y_{<t}) / \u03c4)}$$",
        "",
        "**Effects:**",
        "- \u03c4 < 1: Sharpens distribution, favors high-probability tokens",
        "- \u03c4 > 1: Flattens distribution, increases randomness",
        "- \u03c4 \u2192 0: Approaches greedy search",
        "- \u03c4 \u2192 \u221e: Approaches uniform sampling",
        "- Typical values: 0.7-0.9 for balanced output",
        "",
        "**Combined with Other Methods:**",
        "Temperature is often used with top-k or top-p sampling:",
        "1. Apply temperature to modify distribution",
        "2. Apply top-k or top-p sampling on the modified distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7. Combined Sampling Strategies",
        "",
        "Modern systems often combine multiple techniques:",
        "",
        "**Top-K + Top-p + Temperature Sampling:**",
        "1. Adjust probabilities with temperature",
        "2. Apply top-k filtering",
        "3. Further apply top-p filtering",
        "4. Sample from resulting distribution",
        "",
        "This provides fine-grained control over the quality-diversity tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison of Decoding Methods",
        "",
        "| Method | Quality | Diversity | Speed | Use Cases |",
        "|--------|---------|-----------|-------|-----------|",
        "| Greedy | Highest | Lowest | Fastest | Translation, summarization |",
        "| Beam | High | Low | Medium | Translation, structured generation |",
        "| Top-K | Medium-High | Medium | Medium | General text generation |",
        "| Top-p | Medium | Medium-High | Medium | Creative writing, chat |",
        "| Pure | Lowest | Highest | Medium | Creative exploration |",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer",
        "",
        "# Load model and tokenizer",
        "model_name = \"gpt2\"",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)",
        "model = GPT2LMHeadModel.from_pretrained(model_name)",
        "",
        "# Prefix to start generation",
        "text = \"Artificial intelligence will\"",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")",
        "",
        "# 1. Greedy search (always pick highest probability token)",
        "greedy_output = model.generate(",
        "    input_ids, ",
        "    max_length=50,",
        "    do_sample=False",
        ")",
        "",
        "# 2. Beam search (maintain multiple likely sequences)",
        "beam_outputs = model.generate(",
        "    input_ids,",
        "    max_length=50,",
        "    num_beams=5,",
        "    no_repeat_ngram_size=2,",
        "    early_stopping=True",
        ")",
        "",
        "# 3. Top-K sampling (sample from K most likely tokens)",
        "topk_outputs = model.generate(",
        "    input_ids,",
        "    max_length=50,",
        "    do_sample=True,",
        "    top_k=50",
        ")",
        "",
        "# 4. Top-p (nucleus) sampling (sample from smallest set of tokens with cumulative probability \u2265 p)",
        "topp_outputs = model.generate(",
        "    input_ids,",
        "    max_length=50,",
        "    do_sample=True,",
        "    top_p=0.9",
        ")",
        "",
        "# 5. Combined top-k and top-p sampling with temperature",
        "combined_outputs = model.generate(",
        "    input_ids,",
        "    max_length=50,",
        "    do_sample=True,",
        "    top_k=50,",
        "    top_p=0.95,",
        "    temperature=0.7",
        ")",
        "",
        "# Print results",
        "print(\"== Greedy Decoding ==\")",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))",
        "",
        "print(\"\\n== Beam Search ==\")",
        "print(tokenizer.decode(beam_outputs[0], skip_special_tokens=True))",
        "",
        "print(\"\\n== Top-K Sampling ==\")",
        "print(tokenizer.decode(topk_outputs[0], skip_special_tokens=True))",
        "",
        "print(\"\\n== Top-P Sampling ==\")",
        "print(tokenizer.decode(topp_outputs[0], skip_special_tokens=True))",
        "",
        "print(\"\\n== Combined Top-K & Top-P with Temperature ==\")",
        "print(tokenizer.decode(combined_outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Decoding Techniques",
        "",
        "Beyond the core methods described above, several advanced techniques have emerged:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Contrastive Search",
        "",
        "Balances sequence probability with diversity by encouraging the model to generate tokens that are high probability but different from previous tokens:",
        "",
        "$$y_t = \\arg\\max_{w \\in V} \\{ \\alpha \\cdot p_\u03b8(w|y_{<t}) - (1 - \\alpha) \\cdot \\max_{i<t} \\text{sim}(h_w, h_{y_i}) \\}$$",
        "",
        "Where:",
        "- $h_w$ is the hidden representation of candidate token w",
        "- $h_{y_i}$ is the hidden representation of previous token $y_i$",
        "- $\\text{sim}$ is a similarity function (often cosine similarity)",
        "- $\\alpha$ balances probability and degeneration penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Guided Generation",
        "",
        "Using auxiliary models or rules to guide the generation process:",
        "",
        "1. **Classifier-Guided Decoding**: Using a classifier to rate candidate continuations",
        "2. **PPLM (Plug and Play Language Models)**: Steering generation using attribute classifiers",
        "3. **Controlled Generation**: Using control codes or special tokens to direct output style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Iterative Refinement",
        "",
        "Some approaches generate a draft and then iteratively refine it:",
        "",
        "1. **Deliberation Networks**: Generate multiple drafts and then refine",
        "2. **Recursive Re-ranking**: Generate candidates and rerank using a separate model",
        "3. **Self-Correction**: Let the model critique and improve its own outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementation Considerations",
        "",
        "When implementing decoding strategies:",
        "",
        "1. **Parameter Selection**:",
        "   - Smaller models often need higher temperature/top-p values",
        "   - Test different settings for your specific application",
        "   - Consider A/B testing to find optimal values",
        "",
        "2. **Special Constraints**:",
        "   - Length limits and penalties",
        "   - Repetition penalties to avoid loops",
        "   - Domain-specific constraints (e.g., for code generation)",
        "",
        "3. **Efficiency Optimization**:",
        "   - Batch processing for multiple generations",
        "   - Caching for repeated contexts",
        "   - Early stopping when appropriate",
        "",
        "4. **Evaluation Metrics**:",
        "   - Perplexity for quality estimation",
        "   - Self-BLEU for diversity measurement",
        "   - Human evaluation for overall quality",
        "",
        "The choice of decoding strategy should align with your application's needs\u2014whether prioritizing deterministic correctness, creative variety, or a balanced middle ground."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. **Perplexity**",
        "- A measure of how **well the model predicts text**.",
        "- Lower = better prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Perplexity is a fundamental evaluation metric in language modeling that quantifies how well a probability model predicts a sample. In the context of language models, perplexity measures how \"surprised\" or \"confused\" a model is when encountering test data, with lower perplexity indicating better predictive performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundation",
        "",
        "Perplexity is derived from information theory and is defined as the exponentiated average negative log-likelihood of a sequence. For a language model that assigns probability $P(x)$ to a sequence $x = (x_1, x_2, ..., x_T)$, the perplexity is:",
        "",
        "$$\\text{PPL}(x) = \\exp\\left(-\\frac{1}{T}\\sum_{t=1}^{T}\\log P(x_t|x_{<t})\\right)$$",
        "",
        "Alternatively, this can be written as:",
        "",
        "$$\\text{PPL}(x) = \\sqrt[T]{\\frac{1}{P(x_1, x_2, ..., x_T)}}$$",
        "",
        "Where:",
        "- $T$ is the sequence length",
        "- $P(x_t|x_{<t})$ is the conditional probability of token $x_t$ given previous tokens $x_{<t}$",
        "- $\\log$ is the natural logarithm",
        "",
        "The intuition behind perplexity can be understood as:",
        "- The weighted average branching factor of a language model",
        "- The geometric mean of inverse token probabilities",
        "- How many equally likely choices the model is \"perplexed\" by at each step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Perplexity Interpretation:",
        "",
        "                                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                \u2502         Text Sequence           \u2502",
        "                                \u2502    \"The cat sat on the mat\"     \u2502",
        "                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                               \u2502",
        "                                               \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                         Probability Assigned by Model                          \u2502",
        "\u2502                                                                                \u2502",
        "\u2502 P(\"The\") = 0.1   P(\"cat\"|\"The\") = 0.05   P(\"sat\"|\"The cat\") = 0.1  ...        \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                               \u2502",
        "                               \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                     Take Negative Log of Each Probability                      \u2502",
        "\u2502                                                                                \u2502",
        "\u2502 -log(0.1) = 2.3    -log(0.05) = 3.0     -log(0.1) = 2.3      ...              \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                               \u2502",
        "                               \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                       Average Over Sequence Length                             \u2502",
        "\u2502                                                                                \u2502",
        "\u2502 (2.3 + 3.0 + 2.3 + ... + 1.6) / 6 = 2.1                                       \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                               \u2502",
        "                               \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                           Exponentiate Result                                  \u2502",
        "\u2502                                                                                \u2502",
        "\u2502 exp(2.1) = 8.2                                                                 \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                               \u2502",
        "                               \u25bc",
        "                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                      \u2502   Perplexity     \u2502",
        "                      \u2502      8.2         \u2502",
        "                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Interpreting Perplexity Values",
        "",
        "The perplexity value has a concrete interpretation:",
        "",
        "1. **Perplexity = 1**: Perfect prediction (model assigns 100% probability to each correct token)",
        "2. **Perplexity = |V|**: Random prediction (model assigns uniform probability 1/|V| to each token in vocabulary V)",
        "3. **Typical values**:",
        "   - Strong language models on general text: 15-30",
        "   - Domain-specific models on in-domain text: 5-15",
        "   - High perplexity (>100): Model is struggling to predict the text",
        "",
        "A model with perplexity N can be interpreted as being as confused as if it had to choose uniformly between N options at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Perplexity vs. Loss",
        "",
        "Perplexity is directly related to the cross-entropy loss commonly used to train language models:",
        "",
        "$$\\text{Perplexity} = \\exp(\\text{Cross-entropy Loss})$$",
        "",
        "This relationship means:",
        "- Minimizing cross-entropy loss is equivalent to minimizing perplexity",
        "- A reduction in loss from 3.0 to 2.0 corresponds to reducing perplexity from exp(3)\u224820 to exp(2)\u22487.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Perplexity Computation Challenges",
        "",
        "Several practical challenges arise when computing perplexity:",
        "",
        "1. **Long sequences**: Need to break into manageable chunks with sliding windows",
        "2. **Out-of-vocabulary tokens**: Require special handling (typically assigned very low probability)",
        "3. **Tokenization differences**: Can affect perplexity scores (more tokens = different normalization)",
        "4. **Domain mismatch**: Perplexity can be extremely high on out-of-domain text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Applications of Perplexity",
        "",
        "Beyond model evaluation, perplexity serves numerous practical purposes:",
        "",
        "1. **Model Selection**: Comparing different architectures or hyperparameters",
        "2. **Early Stopping**: Halting training when validation perplexity stops improving",
        "3. **Domain Detection**: Identifying the domain/topic of text by comparing perplexity across domain-specific models",
        "4. **Anomaly Detection**: Flagging unusual text by high perplexity scores",
        "5. **Quality Estimation**: Assessing generated text quality without references",
        "6. **Data Filtering**: Removing low-quality examples from training sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Variants and Extensions",
        "",
        "Several variations of perplexity are used for specific applications:",
        "",
        "1. **Pseudo-Perplexity**: For masked language models (BERT-style) that don't model full sequence probability",
        "2. **Conditional Perplexity**: Evaluating performance on specific conditioning contexts",
        "3. **Sequence-normalized Perplexity**: Adjusting for varying sequence lengths and complexities",
        "4. **Span-based Perplexity**: Measuring perplexity on specific spans of interest",
        "5. **Character-level Perplexity**: Computing at character rather than token level",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import math",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer",
        "",
        "# Load model and tokenizer",
        "model_name = \"gpt2\"",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)",
        "model = GPT2LMHeadModel.from_pretrained(model_name)",
        "",
        "# Calculate perplexity of a text",
        "def calculate_perplexity(text, model, tokenizer, stride=512):",
        "    # Encode text",
        "    encodings = tokenizer(text, return_tensors=\"pt\")",
        "    ",
        "    # Get max sequence length for model",
        "    max_length = model.config.n_positions",
        "    ",
        "    # Initialize variables for perplexity calculation",
        "    log_likelihood = 0.0",
        "    total_tokens = 0",
        "    ",
        "    # Process long text in overlapping chunks with stride",
        "    for i in range(0, encodings.input_ids.size(1), stride):",
        "        # Get chunk and ensure it's not too long",
        "        begin_loc = max(i + stride - max_length, 0)",
        "        end_loc = min(i + stride, encodings.input_ids.size(1))",
        "        target_len = end_loc - i",
        "        ",
        "        # Extract input IDs for this chunk",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc]",
        "        ",
        "        # Don't compute loss for tokens we're conditioning on",
        "        target_ids = input_ids.clone()",
        "        if i > 0:",
        "            # For overlap tokens, set labels to -100 so they're ignored in loss calculation",
        "            target_ids[:, :i - begin_loc] = -100",
        "        ",
        "        # Forward pass",
        "        with torch.no_grad():",
        "            outputs = model(input_ids, labels=target_ids)",
        "            ",
        "        log_likelihood += outputs.loss.item() * target_len",
        "        total_tokens += target_len",
        "    ",
        "    # Calculate perplexity",
        "    perplexity = math.exp(log_likelihood / total_tokens)",
        "    return perplexity",
        "",
        "# Example texts with different levels of coherence",
        "good_text = \"Artificial intelligence has made significant advances in recent years.\"",
        "random_text = \"Intelligence years the in has artificial made significant advances recent.\"",
        "nonsense_text = \"Zgfhj kfls poabt klxn artificial bjkfs poiten.\"",
        "",
        "# Calculate perplexity",
        "good_ppl = calculate_perplexity(good_text, model, tokenizer)",
        "random_ppl = calculate_perplexity(random_text, model, tokenizer)",
        "nonsense_ppl = calculate_perplexity(nonsense_text, model, tokenizer)",
        "",
        "print(f\"Good text perplexity: {good_ppl:.2f}\")",
        "print(f\"Shuffled text perplexity: {random_ppl:.2f}\")",
        "print(f\"Nonsense text perplexity: {nonsense_ppl:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Advanced Perplexity Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Perplexity in Different Language Model Types",
        "",
        "Different types of language models calculate perplexity in slightly different ways:",
        "",
        "1. **Autoregressive Models (GPT-style)**:",
        "   - Standard left-to-right perplexity calculation",
        "   - Each token is predicted based on previous tokens only",
        "",
        "2. **Masked Language Models (BERT-style)**:",
        "   - Use pseudo-perplexity through masking",
        "   - Mask one token at a time and predict it from surrounding context",
        "   - Results are not directly comparable to autoregressive perplexity",
        "",
        "3. **Encoder-Decoder Models (T5/BART)**:",
        "   - Can compute perplexity on the decoder probabilities",
        "   - Often use conditional perplexity given encoded input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparing Perplexity Across Models",
        "",
        "When comparing perplexity between different models, important considerations include:",
        "",
        "1. **Tokenization Differences**: Models with different vocabularies will have different baseline perplexities",
        "   - Character-level models typically have higher perplexity than word-level models",
        "   - Subword tokenization affects perplexity (more tokens = different normalization)",
        "",
        "2. **Normalization Strategies**:",
        "   - Token-level normalization: Standard approach",
        "   - Word-level normalization: Adjusting for subword tokenization differences",
        "   - Byte-level normalization: Accounting for character encoding variations",
        "",
        "3. **Fair Comparison**: When comparing models, standardize on:",
        "   - Same test set",
        "   - Same tokenization when possible",
        "   - Same handling of unknown tokens",
        "   - Same stride and context handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using Perplexity for Data Analysis",
        "",
        "Perplexity can provide valuable insights about your text data:",
        "",
        "1. **Difficulty Analysis**: Identifying challenging text segments",
        "   - High perplexity regions often contain rare words, complex syntax, or domain-specific terminology",
        "   - Can help target areas needing model improvement",
        "",
        "2. **Distribution Shift Detection**: Tracking when text diverges from training distribution",
        "   - Monitoring perplexity over time can detect concept drift",
        "   - Useful for identifying when models need retraining",
        "",
        "3. **Quality Filtering**: Using perplexity to filter generation results",
        "   - Setting thresholds for acceptable perplexity",
        "   - Rejecting outputs that have suspicious statistical properties",
        "",
        "4. **Controlled Generation**: Guiding generation toward desired perplexity ranges",
        "   - Too low: May be repetitive or plagiarized",
        "   - Too high: Likely incoherent or off-topic",
        "   - \"Just right\": Novel but coherent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementation Tips for Accurate Perplexity",
        "",
        "For accurate and efficient perplexity calculation:",
        "",
        "1. **Handling Long Sequences**:",
        "   - Use sliding window approach with stride",
        "   - Properly handle token overlap between windows",
        "   - Discount initial tokens in each window that have full context",
        "",
        "2. **Padding and Special Tokens**:",
        "   - Exclude padding tokens from loss calculation",
        "   - Consider whether to include or exclude special tokens (like EOS)",
        "   - Be consistent across evaluations",
        "",
        "3. **Batching for Efficiency**:",
        "   - Calculate perplexity on batches of sequences",
        "   - Ensure proper masking of padding tokens",
        "   - Aggregate loss correctly across batches",
        "",
        "4. **Memory Efficiency**:",
        "   - Use gradient-free computation (torch.no_grad())",
        "   - Process very long texts in chunks",
        "   - Consider using lower precision (fp16) for large models",
        "",
        "Perplexity remains the most important intrinsic evaluation metric for language models, providing a mathematically principled way to assess predictive power before any downstream task evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. **Temperature**",
        "- Controls randomness in generation.",
        "- Low (0.2) = deterministic, high (1.0+) = more creative/random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Temperature is a hyperparameter that controls the randomness or entropy of probability distributions in language model generation. It fundamentally alters how confident or exploratory a model is when selecting tokens, acting as a knob that adjusts the balance between predictability and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundation",
        "",
        "In statistical mechanics, temperature comes from the Boltzmann distribution and controls how probability is distributed across different energy states. In language models, temperature (\u03c4) modifies the logits (pre-softmax activations) before converting them to probabilities:",
        "",
        "$$p_\\tau(x_i) = \\frac{\\exp(z_i/\\tau)}{\\sum_j \\exp(z_j/\\tau)}$$",
        "",
        "Where:",
        "- $z_i$ are the logits (unnormalized scores) for token $i$",
        "- $\\tau$ is the temperature parameter",
        "- $p_\\tau(x_i)$ is the resulting probability distribution",
        "",
        "This transformation has profound effects on the distribution shape:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Temperature Effects on Probability Distribution:",
        "",
        "                  Original Logits",
        "                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                \u2502 \"dog\": 5.0    \u2502",
        "                \u2502 \"cat\": 2.0    \u2502",
        "                \u2502 \"bird\": 1.0   \u2502",
        "                \u2502 \"fish\": 0.2   \u2502",
        "                \u2502 ...other words\u2502",
        "                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                        \u2502",
        "                        \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502                   \u2502                    \u2502                   \u2502",
        "    \u25bc                   \u25bc                    \u25bc                   \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 \u03c4 = 0.1  \u2502       \u2502 \u03c4 = 0.5  \u2502        \u2502 \u03c4 = 1.0  \u2502        \u2502 \u03c4 = 2.0  \u2502",
        "\u2502(Very Low)\u2502       \u2502  (Low)   \u2502        \u2502(Standard)\u2502        \u2502 (High)   \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518",
        "     \u2502                  \u2502                   \u2502                   \u2502",
        "     \u25bc                  \u25bc                   \u25bc                   \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502\"dog\":0.999\u2502       \u2502\"dog\":0.952\u2502        \u2502\"dog\":0.82 \u2502        \u2502\"dog\":0.57 \u2502",
        "\u2502\"cat\":0.001\u2502       \u2502\"cat\":0.042\u2502        \u2502\"cat\":0.14 \u2502        \u2502\"cat\":0.25 \u2502",
        "\u2502\"bird\":~0  \u2502       \u2502\"bird\":0.006\u2502       \u2502\"bird\":0.03 \u2502       \u2502\"bird\":0.13\u2502",
        "\u2502\"fish\":~0  \u2502       \u2502\"fish\":0.001\u2502       \u2502\"fish\":0.01 \u2502       \u2502\"fish\":0.05\u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "",
        "Increasing Temperature \u2192 Flatter Distribution \u2192 Higher Randomness",
        "Decreasing Temperature \u2192 Sharper Distribution \u2192 Higher Determinism",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Effects of Temperature",
        "",
        "Temperature modifies token selection in several key ways:",
        "",
        "1. **Low Temperature (\u03c4 < 1.0)**:",
        "   - Sharpens probability distribution",
        "   - Amplifies differences between high and low probabilities",
        "   - Makes high-probability tokens much more likely to be chosen",
        "   - As \u03c4 \u2192 0, approaches greedy selection (always pick highest probability)",
        "   - Often leads to repetitive, \"safe\" outputs",
        "",
        "2. **High Temperature (\u03c4 > 1.0)**:",
        "   - Flattens probability distribution",
        "   - Reduces differences between high and low probabilities",
        "   - Increases the chance of sampling less likely tokens",
        "   - As \u03c4 \u2192 \u221e, approaches uniform random selection",
        "   - Often leads to more diverse but potentially incoherent outputs",
        "",
        "3. **Neutral Temperature (\u03c4 = 1.0)**:",
        "   - Leaves original logits unchanged",
        "   - Preserves the model's native probability assessment",
        "   - Standard softmax normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Impact on Entropy",
        "",
        "Temperature directly affects the entropy of the distribution. For a categorical distribution with probabilities $p_i$, the entropy is:",
        "",
        "$$H(p) = -\\sum_i p_i \\log p_i$$",
        "",
        "- Low temperature decreases entropy (more certainty)",
        "- High temperature increases entropy (more randomness)",
        "",
        "The relationship between temperature and entropy is monotonic but non-linear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### When to Use Different Temperatures",
        "",
        "Temperature can be strategically selected based on generation goals:",
        "",
        "| Temperature | Range | Use Cases | Characteristics |",
        "|-------------|-------|-----------|-----------------|",
        "| Very Low | 0.1-0.3 | Factual QA, Translation | High precision, low diversity, repetitive |",
        "| Low | 0.3-0.7 | Summarization, Structured text | Balanced coherence, limited creativity |",
        "| Medium | 0.7-1.0 | Chat, Content creation | Model's native distribution, good balance |",
        "| High | 1.0-1.5 | Brainstorming, Creative writing | Higher diversity, occasional incoherence |",
        "| Very High | 1.5+ | Exploration, Novel ideas | Highly unpredictable, often nonsensical |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Temperature in Multi-Stage Generation",
        "",
        "Some applications benefit from temperature adjustments during different phases:",
        "",
        "1. **Dynamic Temperature**:",
        "   - Decrease temperature for factual/logical sections",
        "   - Increase temperature for creative/divergent sections",
        "   - Adjust based on generation context or prompt type",
        "",
        "2. **Temperature Scheduling**:",
        "   - Start with higher temperature for exploration",
        "   - Gradually decrease to refine and focus the generation",
        "   - Similar to simulated annealing in optimization",
        "",
        "3. **Multi-Sample + Reranking**:",
        "   - Generate multiple samples at higher temperature",
        "   - Rerank outputs using a separate quality metric",
        "   - Select best candidate from diverse options",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer",
        "import torch",
        "",
        "# Load model and tokenizer",
        "model_name = \"gpt2\"",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)",
        "model = GPT2LMHeadModel.from_pretrained(model_name)",
        "",
        "# Prefix to start generation",
        "text = \"Once upon a time in a land far away,\"",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\")",
        "",
        "# Generate with different temperatures",
        "temperatures = [0.2, 0.5, 0.8, 1.0, 1.5]",
        "",
        "for temp in temperatures:",
        "    # Generate text",
        "    outputs = model.generate(",
        "        input_ids,",
        "        max_length=100,",
        "        do_sample=True,",
        "        temperature=temp,",
        "        top_k=50,",
        "        top_p=0.95,",
        "        num_return_sequences=1",
        "    )",
        "    ",
        "    # Decode and print",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)",
        "    print(f\"\\n== Temperature: {temp} ==\")",
        "    print(generated_text)",
        "    ",
        "    # Visualize token probability distribution for next token (optional)",
        "    with torch.no_grad():",
        "        logits = model(input_ids).logits[:, -1, :]",
        "        ",
        "        # Apply temperature",
        "        scaled_logits = logits / temp",
        "        ",
        "        # Convert to probabilities",
        "        probs = torch.nn.functional.softmax(scaled_logits, dim=-1)",
        "        ",
        "        # Get top tokens",
        "        top_probs, top_indices = torch.topk(probs, k=5, dim=-1)",
        "        ",
        "        print(\"\\nTop 5 next token probabilities:\")",
        "        for prob, idx in zip(top_probs[0], top_indices[0]):",
        "            token = tokenizer.decode([idx])",
        "            print(f\"  '{token}': {prob.item():.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Implementation Considerations",
        "",
        "When implementing temperature in text generation systems:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Finding the Right Temperature",
        "",
        "- **System Purpose**: Task-specific optimal values",
        "  - Chatbots: 0.7-0.9 for conversational variety",
        "  - Code generation: 0.2-0.4 for correctness",
        "  - Creative writing: 0.8-1.2 for novel ideas",
        "",
        "- **Model Size Effects**: ",
        "  - Larger models often work better with lower temperatures",
        "  - Smaller models may need higher temperatures to avoid repetition",
        "  ",
        "- **Systematic Exploration**:",
        "  - Generate outputs at various temperatures (0.2, 0.5, 0.8, 1.0, 1.5)",
        "  - Evaluate output diversity and coherence",
        "  - Select best tradeoff for your application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Combining with Other Decoding Methods",
        "",
        "Temperature works synergistically with other decoding strategies:",
        "",
        "- **Temperature + Top-K**:",
        "  - Apply temperature first to modify logits",
        "  - Then apply top-K filtering on the temperature-adjusted probabilities",
        "  - Prevents high temperature from considering truly unlikely tokens",
        "",
        "- **Temperature + Top-p**:",
        "  - Temperature adjusts the shape of the distribution",
        "  - Top-p controls the size of the sampling pool",
        "  - Together they provide fine-grained control over randomness",
        "  ",
        "- **Effective Combinations**:",
        "  - Low-risk settings: Low temperature (0.3) + low top-p (0.5)",
        "  - Balanced settings: Medium temperature (0.8) + medium top-p (0.9)",
        "  - Explorative settings: Higher temperature (1.2) + high top-p (0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Advanced Applications",
        "",
        "- **Context-Dependent Temperature**:",
        "  - Analyze prompt to determine appropriate temperature",
        "  - Use lower temperature for factual/sensitive content",
        "  - Use higher temperature for creative requests",
        "",
        "- **User-Controlled Creativity**:",
        "  - Expose temperature as a \"creativity slider\" in user interfaces",
        "  - Allow users to adjust the determinism/randomness tradeoff",
        "  - Set appropriate defaults based on use case",
        "",
        "- **Temperature Annealing**:",
        "  - Systematically decrease temperature during generation",
        "  - Start with exploration, end with refinement",
        "  - Particularly useful for long-form creative content",
        "",
        "Temperature is one of the most powerful yet accessible hyperparameters for controlling language model behavior, offering a simple knob that profoundly affects generation style and creativity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. **Model Evaluation Metrics**",
        "",
        "Effective evaluation is crucial for understanding model performance and limitations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Model evaluation metrics provide quantitative measures to assess the performance of language models across different tasks and capabilities. These metrics help researchers and practitioners compare models, track improvements, identify weaknesses, and make informed decisions about deployment readiness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fundamental Concepts",
        "",
        "**1. Reference-Based vs. Reference-Free Evaluation**",
        "- **Reference-Based**: Compares model outputs to human-created \"gold standard\" references",
        "- **Reference-Free**: Evaluates outputs based on intrinsic qualities without comparison to references",
        "",
        "**2. Automatic vs. Human Evaluation**",
        "- **Automatic**: Algorithmic assessment that can be scaled to large datasets",
        "- **Human**: Manual evaluation by human judges, often capturing nuances that automatic metrics miss",
        "",
        "**3. Task-Specific vs. General Metrics**",
        "- **Task-Specific**: Designed for particular applications (translation, summarization, etc.)",
        "- **General**: Assess broader capabilities like fluency, coherence, or factuality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy Metrics",
        "- **BLEU Score**: Measures the quality of machine translation by comparing model outputs to reference translations.",
        "- **ROUGE**: Evaluates the quality of summaries by measuring overlap with reference summaries.",
        "- **Perplexity**: Measures how well a model predicts a sample, with lower scores indicating better performance.",
        "- **F1 Score**: The harmonic mean of precision and recall, commonly used for classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundations",
        "",
        "**BLEU (Bilingual Evaluation Understudy)**",
        "",
        "BLEU measures n-gram precision between candidate translation and reference translations:",
        "",
        "$$\\text{BLEU} = \\text{BP} \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$",
        "",
        "Where:",
        "- $p_n$ is the modified n-gram precision",
        "- $w_n$ is the weight for each n-gram precision (typically uniform)",
        "- BP is the brevity penalty to penalize short translations: $\\text{BP} = \\min(1, e^{1-r/c})$",
        "- $r$ is the reference length and $c$ is the candidate length",
        "",
        "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**",
        "",
        "ROUGE-N measures the n-gram recall between candidate summary and reference summaries:",
        "",
        "$$\\text{ROUGE-N} = \\frac{\\sum_{S \\in \\text{References}} \\sum_{\\text{n-gram} \\in S} \\text{Count}_{\\text{match}}(\\text{n-gram})}{\\sum_{S \\in \\text{References}} \\sum_{\\text{n-gram} \\in S} \\text{Count}(\\text{n-gram})}$$",
        "",
        "**Perplexity**",
        "",
        "Perplexity measures how well a probability model predicts a sample:",
        "",
        "$$\\text{PPL}(X) = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N}\\log p(x_i|x_1, x_2, ..., x_{i-1})\\right)$$",
        "",
        "Where $p(x_i|x_1, x_2, ..., x_{i-1})$ is the model's predicted probability of token $x_i$ given previous tokens.",
        "",
        "**F1 Score**",
        "",
        "$$\\text{F1} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$",
        "",
        "Where:",
        "- $\\text{precision} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$",
        "- $\\text{recall} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "          Model Evaluation Spectrum",
        "          ",
        "Low-level                                High-level",
        "Metrics                                  Metrics",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 Perplexity\u2502 \u2192   \u2502 BLEU/ROUGE\u2502 \u2192   \u2502Task Benchmark\u2502 \u2192   \u2502Human Judgment\u2502",
        "\u2502 Log Loss  \u2502     \u2502 F1/METEOR \u2502     \u2502 MMLU/HELM    \u2502     \u2502 Turing Test  \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "      \u25b2                 \u25b2                  \u25b2                    \u25b2",
        "      \u2502                 \u2502                  \u2502                    \u2502",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 Language \u2502      \u2502 Generated\u2502      \u2502  Specific \u2502        \u2502  Human   \u2502",
        "\u2502 Modeling \u2502      \u2502   Text   \u2502      \u2502   Tasks   \u2502        \u2502 Alignment\u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Specialized Evaluations",
        "- **BIG-bench**: Collection of 200+ diverse tasks designed to probe model capabilities.",
        "- **MMLU (Massive Multitask Language Understanding)**: Tests knowledge across 57 subjects.",
        "- **HumanEval**: Measures code generation capabilities through functional correctness.",
        "- **TruthfulQA**: Evaluates a model's propensity to reproduce falsehoods found in training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Benchmark Evolution",
        "",
        "The evaluation landscape for language models has evolved rapidly:",
        "",
        "1. **First Wave**: Simple NLP tasks (named entity recognition, part-of-speech tagging)",
        "2. **Second Wave**: GLUE, SuperGLUE (natural language understanding benchmarks)",
        "3. **Third Wave**: Specialized capability testing (reasoning, knowledge, safety)",
        "4. **Current Frontier**: Holistic evaluation frameworks combining multiple dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Limitations of Current Metrics",
        "",
        "Understanding the limitations of metrics is crucial:",
        "",
        "- **Reference Dependency**: Metrics like BLEU/ROUGE can't recognize valid outputs that differ from references",
        "- **Surface Form Focus**: Many metrics emphasize lexical overlap over semantic similarity",
        "- **Gaming Potential**: Models can be optimized to perform well on specific metrics without improving overall capability",
        "- **Distribution Shift**: Performance on benchmarks may not translate to real-world performance",
        "- **Multidimensional Quality**: Single metrics can't capture all aspects of generation quality",
        "",
        "**Example of Perplexity Calculation:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "from transformers import AutoModelForCausalLM, AutoTokenizer",
        "",
        "model_name = \"gpt2\"",
        "model = AutoModelForCausalLM.from_pretrained(model_name)",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)",
        "",
        "def calculate_perplexity(text):",
        "    inputs = tokenizer(text, return_tensors=\"pt\")",
        "    with torch.no_grad():",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])",
        "    ",
        "    loss = outputs.loss",
        "    perplexity = torch.exp(loss)",
        "    ",
        "    return perplexity.item()",
        "",
        "test_text = \"Natural language processing has advanced significantly in recent years.\"",
        "perplexity = calculate_perplexity(test_text)",
        "print(f\"Perplexity: {perplexity}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When evaluating models, it's important to:",
        "- Use multiple complementary metrics rather than relying on a single measure",
        "- Include task-specific evaluations relevant to your application",
        "- Consider both automated metrics and human evaluation",
        "- Test on diverse datasets to ensure robust performance across different contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. **Model Limitations and Ethical Considerations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Limitations",
        "- **Hallucinations**: Language models can generate plausible-sounding but factually incorrect information.",
        "- **Context Window Limitations**: Even with recent advances, models have finite context windows that limit their ability to process very long documents.",
        "- **Reasoning Capabilities**: Current models may struggle with complex logical reasoning or mathematical tasks.",
        "- **Temporal Knowledge Cutoff**: Models only have knowledge up to their training cutoff date and lack real-time information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ethical Considerations",
        "- **Bias**: Models can reflect and amplify biases present in training data.",
        "- **Privacy Concerns**: When fine-tuned on private data, models may memorize and potentially leak sensitive information.",
        "- **Environmental Impact**: Training large language models requires significant computational resources and energy consumption.",
        "- **Misuse Potential**: Models can be used to generate misleading content or automate harmful activities.",
        "",
        "**Responsible Implementation Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def filter_harmful_content(generated_text):",
        "    \"\"\"",
        "    Simple example of content filtering for model outputs",
        "    In practice, more sophisticated methods would be used",
        "    \"\"\"",
        "    harmful_patterns = [\"violence\", \"hate speech\", \"personal data\"]",
        "    for pattern in harmful_patterns:",
        "        if pattern in generated_text.lower():",
        "            return \"[Content filtered for safety reasons]\"",
        "    return generated_text",
        "",
        "# Usage",
        "response = model.generate(prompt)",
        "filtered_response = filter_harmful_content(response)",
        "print(filtered_response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best practices for implementing language models include:",
        "- Using comprehensive evaluation suites to test for biases and harmful outputs",
        "- Implementing robust content filtering systems",
        "- Being transparent about model limitations",
        "- Collecting user feedback to improve safety measures",
        "- Regularly updating models to address discovered issues"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}