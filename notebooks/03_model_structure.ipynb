{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Model Structure & Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Model Structure & Transformer Architecture",
        "",
        "This file covers concepts related to the structure of modern AI models, with a focus on transformer architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. **Transformer**",
        "- The core architecture behind modern LLMs.",
        "- Based on **attention mechanisms** rather than recurrence or convolution.",
        "- Introduced in the paper \"Attention Is All You Need\" (2017) by Vaswani et al. at Google Brain.",
        "- Revolutionized NLP by enabling parallel processing and capturing long-range dependencies.",
        "- Key advantages:",
        "  - Parallelizable computation (unlike RNNs)",
        "  - Handles long-range dependencies better than CNNs/RNNs",
        "  - No vanishing/exploding gradient problems",
        "  - Scales effectively to very large models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "The Transformer architecture revolutionized natural language processing and is the foundation for virtually all modern large language models (LLMs) including GPT, BERT, T5, and more. Understanding this architecture is crucial for comprehending how these models work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Historical Context",
        "",
        "Before Transformers, most NLP models used recurrent neural networks (RNNs) or convolutional neural networks (CNNs):",
        "- **RNNs (including LSTMs and GRUs)**: Processed text sequentially, word by word, making them slow to train and limited in capturing long-range dependencies",
        "- **CNNs**: Could process text in parallel but struggled with long-distance relationships between words",
        "",
        "The Transformer architecture, introduced in 2017, solved these limitations with a design based entirely on attention mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Core Components of a Transformer",
        "",
        "A typical Transformer consists of:",
        "",
        "1. **Input Embedding Layer**: Converts tokens to vectors",
        "2. **Positional Encoding**: Adds information about token position in the sequence",
        "3. **Encoder Stack**: Multiple layers of self-attention and feed-forward networks (for understanding input)",
        "4. **Decoder Stack**: Similar to encoder, but with an additional layer to attend to encoder outputs (for generating output)",
        "5. **Output Layer**: Converts final representations to output probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Transformer Architecture Diagram:",
        "",
        "                  Output",
        "                     \u2191",
        "           +-------------------+",
        "           |   Linear Layer    |",
        "           +-------------------+",
        "                     \u2191",
        "   +----------------------------------+",
        "   |          Decoder Stack           |",
        "   |  +---------------------------+   |",
        "   |  | Self-Attention            |   |",
        "   |  +---------------------------+   |",
        "   |              \u2191                   |",
        "   |  +---------------------------+   |",
        "   |  | Cross-Attention           |\u25c4--+-- Connects to",
        "   |  +---------------------------+   |   Encoder outputs",
        "   |              \u2191                   |",
        "   |  +---------------------------+   |",
        "   |  | Feed-Forward Network      |   |",
        "   |  +---------------------------+   |",
        "   +----------------------------------+",
        "                     \u2191",
        "   +----------------------------------+",
        "   |          Encoder Stack           |",
        "   |  +---------------------------+   |",
        "   |  | Self-Attention            |   |",
        "   |  +---------------------------+   |",
        "   |              \u2191                   |",
        "   |  +---------------------------+   |",
        "   |  | Feed-Forward Network      |   |",
        "   |  +---------------------------+   |",
        "   +----------------------------------+",
        "                     \u2191",
        "           +-------------------+",
        "           | Positional Encoding|",
        "           +-------------------+",
        "                     \u2191",
        "           +-------------------+",
        "           |  Input Embeddings  |",
        "           +-------------------+",
        "                     \u2191",
        "                   Input",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder-Only vs. Decoder-Only vs. Encoder-Decoder Models",
        "",
        "Transformers come in three main variants:",
        "",
        "1. **Encoder-Only Models** (like BERT)",
        "   - Excellent for understanding input text",
        "   - Used for classification, named entity recognition, question answering",
        "   - Bidirectional attention (can look at the entire input at once)",
        "",
        "2. **Decoder-Only Models** (like GPT)",
        "   - Specialized for text generation",
        "   - Autoregressive (each token only sees previous tokens)",
        "   - Used for language modeling, text completion, chatbots",
        "",
        "3. **Encoder-Decoder Models** (like T5, BART)",
        "   - Used for sequence-to-sequence tasks",
        "   - The encoder processes the input, the decoder generates output",
        "   - Applications: translation, summarization, question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Transformer Training Process",
        "",
        "How Transformers learn:",
        "1. **Pretraining**: Model learns from vast amounts of text with self-supervised objectives:",
        "   - Masked Language Modeling (predict masked words)",
        "   - Next Token Prediction (predict the next word)",
        "   - Span Corruption (reconstruct corrupted text spans)",
        "",
        "2. **Fine-tuning**: Pretrained model is adapted to specific tasks with labeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling Laws",
        "",
        "A critical insight about Transformers is that their performance scales predictably with:",
        "1. Model size (number of parameters)",
        "2. Dataset size",
        "3. Computation used for training",
        "",
        "This predictable scaling has driven the trend toward increasingly large models (from 117M parameters in BERT-Base to 175B+ in GPT-3).",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import math",
        "import matplotlib.pyplot as plt",
        "import numpy as np",
        "",
        "# A more detailed and educational implementation of the Transformer",
        "class PositionalEncoding(nn.Module):",
        "    def __init__(self, d_model, max_seq_length=5000):",
        "        super(PositionalEncoding, self).__init__()",
        "        ",
        "        # Create positional encoding matrix",
        "        pe = torch.zeros(max_seq_length, d_model)",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))",
        "        ",
        "        # Calculate sine and cosine positional encodings",
        "        pe[:, 0::2] = torch.sin(position * div_term)",
        "        pe[:, 1::2] = torch.cos(position * div_term)",
        "        ",
        "        # Register as a buffer (not a parameter)",
        "        self.register_buffer('pe', pe.unsqueeze(0))",
        "        ",
        "    def forward(self, x):",
        "        # Add positional encoding to input embeddings",
        "        return x + self.pe[:, :x.size(1)]",
        "",
        "class MultiHeadAttention(nn.Module):",
        "    def __init__(self, d_model, num_heads):",
        "        super(MultiHeadAttention, self).__init__()",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"",
        "        ",
        "        self.d_model = d_model",
        "        self.num_heads = num_heads",
        "        self.d_k = d_model // num_heads",
        "        ",
        "        # Linear projections for Q, K, V, and output",
        "        self.q_linear = nn.Linear(d_model, d_model)",
        "        self.k_linear = nn.Linear(d_model, d_model)",
        "        self.v_linear = nn.Linear(d_model, d_model)",
        "        self.out_linear = nn.Linear(d_model, d_model)",
        "        ",
        "    def split_heads(self, x):",
        "        # Reshape to [batch_size, seq_length, num_heads, d_k]",
        "        batch_size, seq_length = x.size(0), x.size(1)",
        "        x = x.view(batch_size, seq_length, self.num_heads, self.d_k)",
        "        # Transpose to [batch_size, num_heads, seq_length, d_k]",
        "        return x.transpose(1, 2)",
        "    ",
        "    def forward(self, query, key, value, mask=None):",
        "        batch_size = query.size(0)",
        "        ",
        "        # Linear projections and split into heads",
        "        q = self.split_heads(self.q_linear(query))",
        "        k = self.split_heads(self.k_linear(key))",
        "        v = self.split_heads(self.v_linear(value))",
        "        ",
        "        # Scaled dot-product attention",
        "        # Transpose key for matrix multiplication",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)",
        "        ",
        "        # Apply mask if provided (for causal/padding attention)",
        "        if mask is not None:",
        "            scores = scores.masked_fill(mask == 0, -1e9)",
        "        ",
        "        # Apply softmax to get attention weights",
        "        attention_weights = torch.softmax(scores, dim=-1)",
        "        ",
        "        # Apply attention weights to values",
        "        context = torch.matmul(attention_weights, v)",
        "        ",
        "        # Reshape back to original dimensions",
        "        context = context.transpose(1, 2).contiguous().view(",
        "            batch_size, -1, self.d_model)",
        "        ",
        "        # Final linear projection",
        "        output = self.out_linear(context)",
        "        ",
        "        return output, attention_weights",
        "",
        "class FeedForward(nn.Module):",
        "    def __init__(self, d_model, d_ff, dropout=0.1):",
        "        super(FeedForward, self).__init__()",
        "        self.linear1 = nn.Linear(d_model, d_ff)",
        "        self.linear2 = nn.Linear(d_ff, d_model)",
        "        self.dropout = nn.Dropout(dropout)",
        "        self.relu = nn.ReLU()",
        "        ",
        "    def forward(self, x):",
        "        return self.linear2(self.dropout(self.relu(self.linear1(x))))",
        "",
        "class EncoderLayer(nn.Module):",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):",
        "        super(EncoderLayer, self).__init__()",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)",
        "        self.norm1 = nn.LayerNorm(d_model)",
        "        self.norm2 = nn.LayerNorm(d_model)",
        "        self.dropout = nn.Dropout(dropout)",
        "        ",
        "    def forward(self, x, mask=None):",
        "        # Self-attention with residual connection and layer normalization",
        "        attn_output, _ = self.self_attn(x, x, x, mask)",
        "        x = self.norm1(x + self.dropout(attn_output))",
        "        ",
        "        # Feed-forward with residual connection and layer normalization",
        "        ff_output = self.feed_forward(x)",
        "        x = self.norm2(x + self.dropout(ff_output))",
        "        ",
        "        return x",
        "",
        "class DecoderLayer(nn.Module):",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):",
        "        super(DecoderLayer, self).__init__()",
        "        ",
        "        # Self-attention",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)",
        "        self.norm1 = nn.LayerNorm(d_model)",
        "        ",
        "        # Cross-attention to encoder output",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)",
        "        self.norm2 = nn.LayerNorm(d_model)",
        "        ",
        "        # Feed-forward",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)",
        "        self.norm3 = nn.LayerNorm(d_model)",
        "        ",
        "        self.dropout = nn.Dropout(dropout)",
        "        ",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):",
        "        # Self-attention with residual connection and normalization",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)",
        "        x = self.norm1(x + self.dropout(attn_output))",
        "        ",
        "        # Cross-attention with residual connection and normalization",
        "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)",
        "        x = self.norm2(x + self.dropout(attn_output))",
        "        ",
        "        # Feed-forward with residual connection and normalization",
        "        ff_output = self.feed_forward(x)",
        "        x = self.norm3(x + self.dropout(ff_output))",
        "        ",
        "        return x",
        "",
        "class Transformer(nn.Module):",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, ",
        "                 num_encoder_layers=6, num_decoder_layers=6, d_ff=2048, ",
        "                 max_seq_length=5000, dropout=0.1):",
        "        super(Transformer, self).__init__()",
        "        ",
        "        # Embeddings",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)",
        "        ",
        "        # Encoder and Decoder layers",
        "        self.encoder_layers = nn.ModuleList([",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout) ",
        "            for _ in range(num_encoder_layers)",
        "        ])",
        "        ",
        "        self.decoder_layers = nn.ModuleList([",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout) ",
        "            for _ in range(num_decoder_layers)",
        "        ])",
        "        ",
        "        # Final output layer",
        "        self.final_layer = nn.Linear(d_model, tgt_vocab_size)",
        "        ",
        "        self.dropout = nn.Dropout(dropout)",
        "        self.d_model = d_model",
        "        ",
        "    def generate_square_subsequent_mask(self, sz):",
        "        # Create a causal mask for the decoder",
        "        # Prevents attention to future tokens",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))",
        "        return mask",
        "        ",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):",
        "        # Embed and add positional encoding for source sequence",
        "        src_embedded = self.dropout(self.positional_encoding(",
        "            self.encoder_embedding(src) * math.sqrt(self.d_model)))",
        "        ",
        "        # Embed and add positional encoding for target sequence",
        "        tgt_embedded = self.dropout(self.positional_encoding(",
        "            self.decoder_embedding(tgt) * math.sqrt(self.d_model)))",
        "        ",
        "        # Create causal mask for decoder if not provided",
        "        if tgt_mask is None:",
        "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)",
        "        ",
        "        # Pass through encoder layers",
        "        enc_output = src_embedded",
        "        for enc_layer in self.encoder_layers:",
        "            enc_output = enc_layer(enc_output, src_mask)",
        "        ",
        "        # Pass through decoder layers",
        "        dec_output = tgt_embedded",
        "        for dec_layer in self.decoder_layers:",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)",
        "        ",
        "        # Final linear projection to vocabulary",
        "        output = self.final_layer(dec_output)",
        "        ",
        "        return output",
        "",
        "# Example usage with a small dataset",
        "def transformer_example():",
        "    # Small vocabulary and sequences for demonstration",
        "    src_vocab_size = 1000",
        "    tgt_vocab_size = 1000",
        "    seq_length = 20",
        "    batch_size = 2",
        "    ",
        "    # Create model",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,",
        "                        num_encoder_layers=3, num_decoder_layers=3)",
        "    ",
        "    # Create dummy input data",
        "    src = torch.randint(1, src_vocab_size, (batch_size, seq_length))",
        "    tgt = torch.randint(1, tgt_vocab_size, (batch_size, seq_length))",
        "    ",
        "    # Forward pass",
        "    output = model(src, tgt)",
        "    ",
        "    print(f\"Input shape: {src.shape}\")",
        "    print(f\"Output shape: {output.shape}\")",
        "    ",
        "    # Visualize positional encoding",
        "    positional_encoding = PositionalEncoding(d_model=64, max_seq_length=100)",
        "    pe = positional_encoding.pe.squeeze(0).numpy()",
        "    ",
        "    plt.figure(figsize=(10, 6))",
        "    plt.pcolormesh(pe[:20, :], cmap='RdBu')",
        "    plt.xlabel('Embedding Dimension')",
        "    plt.ylabel('Position')",
        "    plt.colorbar()",
        "    plt.title(\"Positional Encoding Visualization\")",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# transformer_example()  # Uncomment to run the example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Applications",
        "",
        "Transformers are now the backbone of most modern NLP systems:",
        "",
        "1. **Text Generation**: GPT models generate human-like text for chatbots, content creation",
        "2. **Language Understanding**: BERT models power search engines and question answering systems",
        "3. **Translation**: Encoder-decoder Transformers enable high-quality machine translation",
        "4. **Summarization**: Creating concise summaries of long documents",
        "5. **Code Generation**: Models like Codex (powering GitHub Copilot) write code based on natural language",
        "6. **Multimodal Learning**: Extended transformers process images and text together (like DALL-E, Flamingo)",
        "",
        "The scalability of Transformers has led to increasingly capable AI systems and remains the dominant architecture in NLP research. Understanding its core mechanisms is essential for anyone working with modern language models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. **Attention Mechanism**",
        "- Helps the model focus on **important tokens** in the input.",
        "- Example: In the sentence \"The cat sat on the mat\", \"sat\" pays more attention to \"cat\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Attention mechanisms are a fundamental component of modern neural networks, particularly in sequence processing tasks. They allow models to focus on different parts of the input sequence when producing each part of the output sequence, mimicking how humans pay attention to specific details when processing information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Foundation",
        "",
        "The basic attention mechanism can be described as a weighted sum of values, where the weights are determined by a compatibility function between queries and keys:",
        "",
        "1. **Query (Q)**: What we're looking for",
        "2. **Key (K)**: What we have available to match against",
        "3. **Value (V)**: The information to extract if there's a match",
        "",
        "The attention function maps a query and a set of key-value pairs to an output:",
        "",
        "```",
        "Attention(Q, K, V) = softmax(QK^T/\u221ad_k)V",
        "```",
        "",
        "Where:",
        "- Q, K, V are matrices representing queries, keys, and values respectively",
        "- d_k is the dimension of the keys (scaling factor)",
        "- softmax normalizes the weights to sum to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Attention Mechanism:",
        "",
        "        \u250c\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2510",
        "        \u2502  Q  \u2502     \u2502  K  \u2502     \u2502  V  \u2502",
        "        \u2514\u2500\u2500\u252c\u2500\u2500\u2518     \u2514\u2500\u2500\u252c\u2500\u2500\u2518     \u2514\u2500\u2500\u252c\u2500\u2500\u2518",
        "           \u2502           \u2502           \u2502",
        "           \u2502           \u2502           \u2502",
        "           \u25bc           \u25bc           \u2502",
        "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502",
        "        \u2502      Q\u00b7K^T      \u2502        \u2502",
        "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502",
        "                 \u2502                 \u2502",
        "                 \u25bc                 \u2502",
        "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502",
        "        \u2502   Scale (\u00f7\u221ad_k) \u2502        \u2502",
        "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502",
        "                 \u2502                 \u2502",
        "                 \u25bc                 \u2502",
        "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502",
        "        \u2502     Softmax     \u2502        \u2502",
        "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502",
        "                 \u2502                 \u2502",
        "                 \u2502                 \u2502",
        "                 \u25bc                 \u25bc",
        "              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "              \u2502    Matrix Multiply  \u2502",
        "              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                         \u2502",
        "                         \u25bc",
        "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                    \u2502 Output  \u2502",
        "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Types of Attention",
        "",
        "1. **Additive Attention (Bahdanau)**:",
        "   - Uses a feed-forward network to compute alignment scores",
        "   - Often more expensive but can perform better for larger dimensions",
        "",
        "2. **Multiplicative Attention (Luong)**:",
        "   - Uses dot product between query and keys",
        "   - More memory-efficient and practical in many applications",
        "   - The scaled dot-product attention in Transformers is a variant of this",
        "",
        "3. **Global vs. Local Attention**:",
        "   - Global: Attends to all positions in the sequence",
        "   - Local: Focuses on a window of positions around a current position"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Attention Works",
        "",
        "1. **Direct connections**: Creates paths between distant positions, helping with long-range dependencies",
        "2. **Weighted information flow**: Allows the model to focus on relevant information",
        "3. **Interpretability**: Attention weights can be visualized to understand what the model is focusing on",
        "4. **Parallelization**: Unlike recurrent methods, all positions can be processed simultaneously"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Real-World Applications",
        "",
        "Attention mechanisms have transformed numerous applications:",
        "",
        "1. **Machine Translation**: Aligning words between languages",
        "2. **Image Captioning**: Focusing on relevant parts of an image when generating description",
        "3. **Speech Recognition**: Attending to important audio segments",
        "4. **Document Summarization**: Identifying key information in long documents",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "import matplotlib.pyplot as plt",
        "import numpy as np",
        "",
        "# Simple Attention Mechanism",
        "def attention(query, key, value):",
        "    # Shape: query, key, value = [batch_size, seq_len, d_model]",
        "    d_k = query.size(-1)",
        "    ",
        "    # Compute attention scores",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(d_k)",
        "    ",
        "    # Apply softmax to get attention weights",
        "    attention_weights = F.softmax(scores, dim=-1)",
        "    ",
        "    # Apply attention weights to values",
        "    output = torch.matmul(attention_weights, value)",
        "    ",
        "    return output, attention_weights",
        "",
        "# Visualization function for attention",
        "def visualize_attention(weights, tokens_q, tokens_k):",
        "    \"\"\"Visualize attention weights between queries and keys\"\"\"",
        "    plt.figure(figsize=(10, 8))",
        "    plt.imshow(weights, cmap='viridis')",
        "    plt.colorbar()",
        "    plt.xlabel('Key tokens')",
        "    plt.ylabel('Query tokens')",
        "    plt.xticks(range(len(tokens_k)), tokens_k, rotation=45)",
        "    plt.yticks(range(len(tokens_q)), tokens_q)",
        "    plt.title('Attention Weights')",
        "    plt.tight_layout()",
        "    return plt",
        "",
        "# Example data",
        "batch_size = 1",
        "seq_len = 4",
        "d_model = 8",
        "",
        "# Create random embeddings for a short sequence",
        "# Imagine these are embeddings for [\"The\", \"cat\", \"sat\", \"mat\"]",
        "embeddings = torch.randn(batch_size, seq_len, d_model)",
        "",
        "# For simplicity, use the same embeddings for query, key, value",
        "query = key = value = embeddings",
        "",
        "# Apply attention",
        "output, weights = attention(query, key, value)",
        "",
        "# Print shapes",
        "print(f\"Input shape: {embeddings.shape}\")",
        "print(f\"Output shape: {output.shape}\")",
        "print(f\"Attention weights shape: {weights.shape}\")",
        "",
        "# Visualize attention matrix",
        "plt.figure(figsize=(7, 5))",
        "plt.imshow(weights[0].detach(), cmap='viridis')",
        "plt.colorbar()",
        "plt.title(\"Attention Weights\")",
        "plt.xlabel(\"Key position\")",
        "plt.ylabel(\"Query position\")",
        "tokens = [\"The\", \"cat\", \"sat\", \"mat\"]",
        "plt.xticks(range(seq_len), tokens)",
        "plt.yticks(range(seq_len), tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention in Machine Translation Example",
        "",
        "Let's consider a concrete example of attention in machine translation, where we're translating from English to French:",
        "",
        "```",
        "English: \"The cat sat on the mat\"",
        "French:  \"Le chat s'assit sur le tapis\"",
        "```",
        "",
        "When generating the French word \"chat\", the attention mechanism focuses heavily on the English word \"cat\", allowing the model to make an accurate translation. Similarly, when generating \"s'assit\", the attention focuses on \"sat\".",
        "",
        "This is fundamentally different from older sequence-to-sequence models, where the entire input sentence had to be compressed into a single fixed-length vector. Attention allows the model to focus on different parts of the input depending on what it's currently generating, solving the bottleneck problem of fixed-context representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. **Self-Attention**",
        "- Token looks at **all other tokens** in the input to decide its meaning.",
        "- Essential for understanding context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Self-attention, a key innovation in transformer models, allows each token in a sequence to attend to all other tokens (including itself) to compute its representation. Unlike traditional attention mechanisms that relate different sequences (e.g., source and target in translation), self-attention operates within a single sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Formulation",
        "",
        "The self-attention mechanism follows the same general formula as the attention mechanism, but with a crucial difference: the queries, keys, and values all come from the same source sequence.",
        "",
        "```",
        "SelfAttention(X) = Attention(XW\u1d60, XW\u1d4f, XW\u1d5b)",
        "                  = softmax((XW\u1d60)(XW\u1d4f)\u1d40/\u221ad_k)(XW\u1d5b)",
        "```",
        "",
        "Where:",
        "- X is the input sequence representation",
        "- W\u1d60, W\u1d4f, W\u1d5b are learnable parameter matrices that project the input into query, key, and value spaces",
        "- d_k is the dimension of the keys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Self-Attention Process:",
        "",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502                       Input Sequence                          \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502                 \u2502                  \u2502",
        "  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "  \u2502   Query (Q)    \u2502  \u2502     Key (K)    \u2502  \u2502   Value (V)  \u2502",
        "  \u2502  Projection    \u2502  \u2502   Projection   \u2502  \u2502  Projection  \u2502",
        "  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502                 \u2502                  \u2502",
        "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2518                  \u2502",
        "                     \u25bc  \u25bc                       \u2502",
        "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502",
        "            \u2502    Q\u00b7K^T/\u221ad_k   \u2502                 \u2502",
        "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502",
        "                     \u2502                          \u2502",
        "                     \u25bc                          \u2502",
        "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502",
        "            \u2502     Softmax     \u2502                 \u2502",
        "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502",
        "                     \u2502                          \u2502",
        "                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                 \u25bc  \u25bc",
        "                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                         \u2502  Matrix Multiply\u2502",
        "                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                  \u2502",
        "                                  \u25bc",
        "                          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                          \u2502    Output    \u2502",
        "                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Self-Attention Matters",
        "",
        "Self-attention addresses several limitations of previous sequence modeling approaches:",
        "",
        "1. **Long-Range Dependencies**: Unlike RNNs, self-attention can directly model relationships between any pair of positions in a sequence, regardless of distance.",
        "",
        "2. **Parallelization**: Unlike sequential RNN processing, self-attention operations can be computed in parallel across all tokens.",
        "",
        "3. **Interpretability**: The attention weights reveal which parts of the input contribute most to each output position.",
        "",
        "4. **Position-Independent**: Self-attention itself is position-agnostic, allowing models to leverage positional encodings as a separate component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Contextual Understanding",
        "",
        "In self-attention, words with similar meanings can attend to each other, creating rich contextual representations:",
        "",
        "1. **Disambiguation**: Words with multiple meanings (e.g., \"bank\" as financial institution vs. river edge) develop different representations based on surrounding context.",
        "",
        "2. **Coreference Resolution**: Self-attention helps resolve pronouns by connecting them to their antecedents.",
        "",
        "3. **Phrase Understanding**: Related words can attend to each other, even across long distances, helping the model understand complex phrases and idioms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example: How Self-Attention Works in Practice",
        "",
        "Consider the sentence: \"The animal didn't cross the street because it was too wide.\"",
        "",
        "- When processing \"it\", self-attention allows the model to strongly attend to \"street\" rather than \"animal\".",
        "- When processing \"wide\", self-attention allows connection back to \"street\".",
        "",
        "This helps resolve what \"it\" refers to (the street, not the animal) by forming these attentional relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Computational Complexity",
        "",
        "Standard self-attention has quadratic complexity O(n\u00b2) with respect to sequence length, as it computes interactions between all pairs of tokens. This becomes a bottleneck for very long sequences, leading to many variations:",
        "",
        "1. **Sparse Attention**: Only attends to a subset of tokens (e.g., local windows, dilated patterns)",
        "2. **Linear Attention**: Reformulates attention to achieve O(n) complexity",
        "3. **Kernelized Attention**: Uses kernel methods to approximate the attention matrix",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "",
        "# Self-Attention implementation",
        "class SelfAttention(nn.Module):",
        "    def __init__(self, embed_size, heads):",
        "        super(SelfAttention, self).__init__()",
        "        self.embed_size = embed_size",
        "        self.heads = heads",
        "        self.head_dim = embed_size // heads",
        "        ",
        "        assert (self.head_dim * heads == embed_size), \"Embed size must be divisible by heads\"",
        "        ",
        "        # Linear projections",
        "        self.queries = nn.Linear(embed_size, embed_size)",
        "        self.keys = nn.Linear(embed_size, embed_size)",
        "        self.values = nn.Linear(embed_size, embed_size)",
        "        self.fc_out = nn.Linear(embed_size, embed_size)",
        "        ",
        "    def forward(self, x):",
        "        batch_size = x.shape[0]",
        "        seq_length = x.shape[1]",
        "        ",
        "        # Original shape: [batch_size, seq_len, embed_size]",
        "        queries = self.queries(x)",
        "        keys = self.keys(x)",
        "        values = self.values(x)",
        "        ",
        "        # Split into multiple heads",
        "        queries = queries.reshape(batch_size, seq_length, self.heads, self.head_dim)",
        "        keys = keys.reshape(batch_size, seq_length, self.heads, self.head_dim)",
        "        values = values.reshape(batch_size, seq_length, self.heads, self.head_dim)",
        "        ",
        "        # Calculate attention scores",
        "        # einsum notation: b=batch, h=heads, q/k=sequence length, d=head dimension",
        "        energy = torch.einsum(\"bqhd,bkhd->bhqk\", [queries, keys])",
        "        attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)",
        "        ",
        "        # Apply attention to values",
        "        out = torch.einsum(\"bhqk,bkhd->bqhd\", [attention, values])",
        "        out = out.reshape(batch_size, seq_length, self.embed_size)",
        "        out = self.fc_out(out)",
        "        ",
        "        return out, attention",
        "",
        "# Function to visualize self-attention",
        "def visualize_self_attention(attention_weights, tokens):",
        "    \"\"\"Visualize self-attention weights between tokens\"\"\"",
        "    plt.figure(figsize=(10, 8))",
        "    ax = sns.heatmap(attention_weights, annot=True, fmt=\".2f\", cmap=\"YlGnBu\")",
        "    plt.xlabel('Attended to')",
        "    plt.ylabel('Attending from')",
        "    plt.title('Self-Attention Weights')",
        "    ax.set_xticklabels(tokens)",
        "    ax.set_yticklabels(tokens)",
        "    plt.tight_layout()",
        "    return plt",
        "",
        "# Example usage with toy data",
        "embed_size = 256",
        "heads = 8",
        "seq_length = 4",
        "batch_size = 1",
        "",
        "# Create model",
        "self_attention = SelfAttention(embed_size, heads)",
        "",
        "# Example input (batch_size, seq_length, embed_size)",
        "x = torch.randn(batch_size, seq_length, embed_size)",
        "output, attention = self_attention(x)",
        "",
        "print(f\"Input shape: {x.shape}\")",
        "print(f\"Output shape: {output.shape}\")",
        "print(f\"Attention shape: {attention.shape}\")",
        "",
        "# Visualize attention for the first head",
        "plt.figure(figsize=(8, 6))",
        "tokens = [\"The\", \"cat\", \"sat\", \"mat\"]",
        "sns.heatmap(attention[0, 0].detach(), annot=True, fmt=\".2f\", xticklabels=tokens, yticklabels=tokens)",
        "plt.title(\"Self-Attention Matrix (First Head)\")",
        "plt.xlabel(\"Key Position\")",
        "plt.ylabel(\"Query Position\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Self-Attention for Sentiment Analysis",
        "",
        "Self-attention is particularly powerful for tasks like sentiment analysis. Consider the sentence:",
        "",
        "\"The food was good, but the service was terrible.\"",
        "",
        "In a sentiment analysis task, self-attention would allow:",
        "",
        "1. The token \"but\" to strongly attend to both \"good\" and \"terrible\"",
        "2. The final representation to emphasize \"terrible\" over \"good\" (since \"but\" signals a contrast with greater weight on what follows)",
        "3. The model to recognize that \"service\" is directly connected to \"terrible\" and \"food\" to \"good\"",
        "",
        "This contextual understanding allows the model to correctly identify the overall negative sentiment, despite having positive elements in the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. **Multi-Head Attention**",
        "- Runs attention multiple times in **parallel** with different \"heads\".",
        "- Each head captures different relationships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Multi-head attention is a powerful extension of the attention mechanism that allows the model to jointly attend to information from different representation subspaces. Instead of performing a single attention function, the model performs multiple attention operations in parallel, each with different learned linear projections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Formulation",
        "",
        "Multi-head attention performs h different attention operations in parallel, then concatenates the results and applies a final linear transformation:",
        "",
        "```",
        "MultiHead(Q, K, V) = Concat(head\u2081, head\u2082, ..., head\u2095)W^O",
        "",
        "where head\u1d62 = Attention(QW^Q_i, KW^K_i, VW^V_i)",
        "```",
        "",
        "Where:",
        "- h is the number of attention heads",
        "- W^Q_i, W^K_i, W^V_i are parameter matrices for the i-th head",
        "- W^O is the output projection matrix",
        "- Concat is the concatenation operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Multi-Head Attention Architecture:",
        "",
        "Input Q, K, V",
        "    \u2502",
        "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500       \u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502            \u2502            \u2502             \u2502            \u2502",
        "    \u25bc            \u25bc            \u25bc             \u25bc            \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502 Linear Q \u2502  \u2502 Linear Q \u2502  \u2502 Linear Q\u2502   \u2502 Linear Q \u2502  \u2502 Linear K \u2502  ...",
        "\u2502 Head 1   \u2502  \u2502 Head 2   \u2502  \u2502 Head 3  \u2502   \u2502 Head h   \u2502  \u2502 Head 1   \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518",
        "     \u2502            \u2502            \u2502             \u2502            \u2502",
        "     \u25bc            \u25bc            \u25bc             \u25bc            \u25bc",
        "     ",
        "     [Linear projections for K and V for all heads]",
        "     ",
        "     \u2502            \u2502            \u2502             \u2502            \u2502",
        "     \u25bc            \u25bc            \u25bc             \u25bc            \u25bc",
        "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "\u2502Attention \u2502  \u2502Attention \u2502  \u2502Attention\u2502   \u2502Attention \u2502",
        "\u2502 Head 1   \u2502  \u2502 Head 2   \u2502  \u2502 Head 3  \u2502   \u2502 Head h   \u2502",
        "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518",
        "     \u2502            \u2502            \u2502             \u2502",
        "     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500   \u2500\u2500\u2500\u2500\u2500\u2518",
        "                  \u2502",
        "                  \u25bc",
        "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "          \u2502  Concatenate \u2502",
        "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                  \u2502",
        "                  \u25bc",
        "          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "          \u2502 Linear Output\u2502",
        "          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                  \u2502",
        "                  \u25bc",
        "               Output",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Multiple Heads Matter",
        "",
        "The multi-head mechanism is crucial for the expressive power of transformer models for several reasons:",
        "",
        "1. **Different Semantic Relationships**: Each attention head can focus on different types of relationships between tokens:",
        "   - Head 1 might focus on syntactic relationships",
        "   - Head 2 might track subject-verb agreement",
        "   - Head 3 might focus on entity relationships",
        "   - Head 4 might attend to broader thematic elements",
        "",
        "2. **Increased Representation Power**: By projecting inputs into different subspaces before applying attention, the model can capture different aspects of the relationships between tokens.",
        "",
        "3. **Ensemble Effect**: Multiple heads provide a form of ensemble learning, where different heads can specialize in different patterns.",
        "",
        "4. **Stabilized Training**: Multi-head attention leads to more stable training, as the learning is distributed across multiple mechanisms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimal Number of Heads",
        "",
        "Research has explored the impact of varying the number of attention heads:",
        "",
        "1. **Too Few Heads**: Limited capacity to model different relationships simultaneously.",
        "2. **Too Many Heads**: Diminishing returns, increased computational cost, and potential overfitting.",
        "3. **Typical Values**: 8-16 heads for medium-sized models, with larger models using more heads (up to 32 or more in very large models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Head Specialization",
        "",
        "Analysis of trained transformers reveals fascinating specialization patterns:",
        "",
        "1. **Positional Heads**: Some heads focus on adjacent tokens or fixed positional patterns",
        "2. **Syntactic Heads**: Some focus on grammatical relationships (e.g., subject-verb)",
        "3. **Coreference Heads**: Some track entity references through text",
        "4. **Rare Token Heads**: Some heads provide special focus on rare or unusual tokens",
        "",
        "This specialization emerges naturally during training without explicit supervision.",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import matplotlib.pyplot as plt",
        "import numpy as np",
        "",
        "# Implementing Multi-Head Attention from scratch",
        "class MultiHeadAttention(nn.Module):",
        "    def __init__(self, embed_dim, num_heads):",
        "        super(MultiHeadAttention, self).__init__()",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"",
        "        ",
        "        self.embed_dim = embed_dim",
        "        self.num_heads = num_heads",
        "        self.head_dim = embed_dim // num_heads",
        "        ",
        "        # Linear projections",
        "        self.query_proj = nn.Linear(embed_dim, embed_dim)",
        "        self.key_proj = nn.Linear(embed_dim, embed_dim)",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)",
        "        self.output_proj = nn.Linear(embed_dim, embed_dim)",
        "        ",
        "        self.scale = self.head_dim ** -0.5",
        "        ",
        "    def forward(self, query, key, value, mask=None, return_attention=False):",
        "        batch_size = query.shape[0]",
        "        ",
        "        # Linear projections and reshape for multi-head attention",
        "        # Shape: (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, num_heads, head_dim)",
        "        q = self.query_proj(query).view(batch_size, -1, self.num_heads, self.head_dim)",
        "        k = self.key_proj(key).view(batch_size, -1, self.num_heads, self.head_dim)",
        "        v = self.value_proj(value).view(batch_size, -1, self.num_heads, self.head_dim)",
        "        ",
        "        # Transpose to (batch_size, num_heads, seq_len, head_dim)",
        "        q = q.transpose(1, 2)",
        "        k = k.transpose(1, 2)",
        "        v = v.transpose(1, 2)",
        "        ",
        "        # Compute attention scores",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale",
        "        ",
        "        # Apply mask if provided",
        "        if mask is not None:",
        "            scores = scores.masked_fill(mask == 0, -1e9)",
        "        ",
        "        # Apply softmax to get attention weights",
        "        attention_weights = torch.softmax(scores, dim=-1)",
        "        ",
        "        # Apply attention weights to values",
        "        context = torch.matmul(attention_weights, v)",
        "        ",
        "        # Transpose and reshape back",
        "        # (batch_size, num_heads, seq_len, head_dim) -> (batch_size, seq_len, embed_dim)",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)",
        "        ",
        "        # Final linear projection",
        "        output = self.output_proj(context)",
        "        ",
        "        if return_attention:",
        "            return output, attention_weights",
        "        ",
        "        return output",
        "",
        "# Visualize multiple attention heads",
        "def visualize_multi_head_attention(attention_weights, tokens, num_heads_to_plot=4):",
        "    \"\"\"Visualize attention weights from multiple heads\"\"\"",
        "    batch_idx = 0  # Plot first batch",
        "    ",
        "    # Select the first few heads to visualize (or all if fewer)",
        "    heads_to_plot = min(attention_weights.shape[1], num_heads_to_plot)",
        "    ",
        "    fig, axes = plt.subplots(1, heads_to_plot, figsize=(16, 4))",
        "    if heads_to_plot == 1:",
        "        axes = [axes]  # Make sure axes is indexable",
        "    ",
        "    for i in range(heads_to_plot):",
        "        im = axes[i].imshow(attention_weights[batch_idx, i].detach().numpy(), cmap=\"viridis\")",
        "        axes[i].set_title(f\"Head {i+1}\")",
        "        axes[i].set_xticks(range(len(tokens)))",
        "        axes[i].set_yticks(range(len(tokens)))",
        "        axes[i].set_xticklabels(tokens, rotation=45)",
        "        axes[i].set_yticklabels(tokens)",
        "    ",
        "    fig.suptitle(\"Multi-Head Attention Visualization\")",
        "    fig.tight_layout()",
        "    plt.colorbar(im, ax=axes)",
        "    return fig",
        "",
        "# Example sequence",
        "seq_len = 5",
        "batch_size = 2",
        "embed_dim = 512",
        "num_heads = 8",
        "",
        "# Create sample inputs",
        "query = key = value = torch.randn(batch_size, seq_len, embed_dim)",
        "",
        "# Create the multi-head attention model",
        "multi_head_attention = MultiHeadAttention(embed_dim, num_heads)",
        "",
        "# Apply multi-head attention",
        "outputs, attention_weights = multi_head_attention(query, key, value, return_attention=True)",
        "",
        "print(f\"Input shape: {query.shape}\")",
        "print(f\"Output shape: {outputs.shape}\")",
        "print(f\"Attention weights shape: {attention_weights.shape}\")",
        "",
        "# Example tokens for visualization",
        "tokens = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\"]",
        "",
        "# Visualize the first few attention heads",
        "plt.figure(figsize=(10, 8))",
        "visualize_multi_head_attention(attention_weights, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real-World Example: Different Heads Capture Different Information",
        "",
        "Consider analyzing a sentence: \"The city council refused the demonstrators a permit because they feared violence.\"",
        "",
        "In this sentence, the pronoun \"they\" could refer to either the council or the demonstrators. In a transformer model with multi-head attention:",
        "",
        "1. One head might focus strongly on connecting \"they\" to \"council\"",
        "2. Another head might focus on the connection between \"feared\" and \"council\"",
        "3. A third head might connect \"they\" with \"feared\"",
        "4. Yet another might track the relationship between \"refused\" and \"permit\"",
        "",
        "By having multiple heads attending to different relationship patterns, the model can better resolve the ambiguity in the sentence. In this example, the combined information from multiple heads would help determine that \"they\" likely refers to the council, not the demonstrators.",
        "",
        "This demonstrates how multi-head attention allows transformers to capture nuanced relationships in language that would be difficult to model with simpler architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. **Layer**",
        "- One stack in the transformer model.",
        "- Each layer has:",
        "  - Self-attention",
        "  - Feed-forward network",
        "  - Layer normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "A transformer layer is a fundamental building block of the transformer architecture. It processes input sequences by applying a series of operations that enable the model to learn complex patterns and relationships in the data. Transformer models typically stack multiple identical layers to form the encoder and decoder components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Layer Components",
        "",
        "The standard transformer layer consists of several key components:",
        "",
        "1. **Multi-Head Self-Attention**: Allows the model to attend to different positions in the input sequence.",
        "",
        "2. **Feed-Forward Network (FFN)**: A position-wise fully connected network applied to each position separately and identically.",
        "",
        "3. **Residual Connections**: Applied around each sub-layer to facilitate gradient flow during training.",
        "",
        "4. **Layer Normalization**: Normalizes the outputs of sub-layers to stabilize and accelerate training.",
        "",
        "5. **Dropout**: Applied for regularization to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder Layer vs. Decoder Layer",
        "",
        "While sharing many similarities, encoder and decoder layers have some key differences:",
        "",
        "1. **Encoder Layer**:",
        "   - Contains self-attention where each position attends to all positions in the input sequence",
        "   - Bidirectional attention (can look at the entire sequence)",
        "",
        "2. **Decoder Layer**:",
        "   - Contains masked self-attention to prevent positions from attending to subsequent positions",
        "   - Contains an additional cross-attention layer that attends to the encoder's output",
        "   - Autoregressive processing (can only look at previous positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Formulation",
        "",
        "For an encoder layer with input x:",
        "",
        "```",
        "// Sub-layer 1: Multi-head self-attention with residual connection and normalization",
        "attention_output = MultiHeadAttention(x)",
        "sub_layer1_output = LayerNorm(x + Dropout(attention_output))",
        "",
        "// Sub-layer 2: Feed-forward network with residual connection and normalization",
        "ffn_output = FFN(sub_layer1_output)",
        "output = LayerNorm(sub_layer1_output + Dropout(ffn_output))",
        "```",
        "",
        "For the feed-forward network:",
        "",
        "```",
        "FFN(x) = max(0, xW\u2081 + b\u2081)W\u2082 + b\u2082",
        "```",
        "",
        "Where W\u2081, W\u2082, b\u2081, and b\u2082 are weight matrices and bias vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Encoder Layer Structure:",
        "",
        "         Input",
        "           \u2502",
        "           \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502 Multi-Head   \u2502",
        "    \u2502Self-Attention\u2502",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502",
        "        \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510",
        "        \u2502 Add & \u2502\u25c4\u2500\u2500 Residual Connection",
        "        \u2502 Norm  \u2502",
        "        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518",
        "            \u2502",
        "            \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502 Feed-Forward \u2502",
        "    \u2502   Network    \u2502",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502",
        "        \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510",
        "        \u2502 Add & \u2502\u25c4\u2500\u2500 Residual Connection",
        "        \u2502 Norm  \u2502",
        "        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518",
        "            \u2502",
        "            \u25bc",
        "         Output",
        "",
        "",
        "Decoder Layer Structure:",
        "",
        "         Input",
        "           \u2502",
        "           \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502  Masked      \u2502",
        "    \u2502Self-Attention\u2502",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502",
        "        \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510",
        "        \u2502 Add & \u2502\u25c4\u2500\u2500 Residual Connection",
        "        \u2502 Norm  \u2502",
        "        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518",
        "            \u2502",
        "            \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502 Cross        \u2502\u25c4\u2500\u2500 Encoder Output",
        "    \u2502 Attention    \u2502",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502",
        "        \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510",
        "        \u2502 Add & \u2502\u25c4\u2500\u2500 Residual Connection",
        "        \u2502 Norm  \u2502",
        "        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518",
        "            \u2502",
        "            \u25bc",
        "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "    \u2502 Feed-Forward \u2502",
        "    \u2502   Network    \u2502",
        "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "            \u2502",
        "        \u250c\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2510",
        "        \u2502 Add & \u2502\u25c4\u2500\u2500 Residual Connection",
        "        \u2502 Norm  \u2502",
        "        \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518",
        "            \u2502",
        "            \u25bc",
        "         Output",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Layer Normalization",
        "",
        "Layer normalization is crucial for training deep transformer networks. Unlike batch normalization, it normalizes across the feature dimension for each individual example:",
        "",
        "```",
        "LayerNorm(x) = \u03b3 * (x - \u03bc) / \u221a(\u03c3\u00b2 + \u03b5) + \u03b2",
        "",
        "Where:",
        "- \u03bc and \u03c3 are the mean and standard deviation computed across the feature dimension",
        "- \u03b3 and \u03b2 are learned scaling and bias parameters",
        "- \u03b5 is a small constant for numerical stability",
        "```",
        "",
        "Layer normalization:",
        "1. Stabilizes the learning process",
        "2. Reduces training time",
        "3. Makes training less sensitive to initialization",
        "4. Allows for training deeper networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Feed-Forward Network",
        "",
        "The feed-forward network in a transformer layer is applied to each position independently:",
        "",
        "```",
        "FFN(x) = max(0, xW\u2081 + b\u2081)W\u2082 + b\u2082",
        "```",
        "",
        "This is equivalent to two linear transformations with a ReLU activation in between. Key characteristics:",
        "",
        "1. **Position-wise**: Applied identically to each position",
        "2. **Dimensionality**: Typically, the inner dimension is larger (often 4x) than the model dimension",
        "3. **Capacity**: Provides additional representational capacity beyond the attention mechanism",
        "4. **Function**: Can be viewed as a 1x1 convolution over the sequence",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "",
        "# Single Transformer Encoder Layer",
        "class TransformerEncoderLayer(nn.Module):",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):",
        "        super(TransformerEncoderLayer, self).__init__()",
        "        ",
        "        # Self-attention layer",
        "        self.attention = nn.MultiheadAttention(embed_size, heads, batch_first=True)",
        "        ",
        "        # Layer normalizations",
        "        self.norm1 = nn.LayerNorm(embed_size)",
        "        self.norm2 = nn.LayerNorm(embed_size)",
        "        ",
        "        # Feed forward network",
        "        self.feed_forward = nn.Sequential(",
        "            nn.Linear(embed_size, forward_expansion * embed_size),",
        "            nn.ReLU(),",
        "            nn.Linear(forward_expansion * embed_size, embed_size),",
        "        )",
        "        ",
        "        self.dropout = nn.Dropout(dropout)",
        "        ",
        "    def forward(self, x, mask=None):",
        "        # Self-attention with residual connection and layer normalization",
        "        attention_out, attention_weights = self.attention(",
        "            x, x, x, ",
        "            attn_mask=mask, ",
        "            need_weights=True",
        "        )",
        "        x = self.norm1(x + self.dropout(attention_out))",
        "        ",
        "        # Feed forward with residual connection and layer normalization",
        "        ff_out = self.feed_forward(x)",
        "        x = self.norm2(x + self.dropout(ff_out))",
        "        ",
        "        return x, attention_weights",
        "",
        "# Single Transformer Decoder Layer",
        "class TransformerDecoderLayer(nn.Module):",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):",
        "        super(TransformerDecoderLayer, self).__init__()",
        "        ",
        "        # Self-attention layer",
        "        self.self_attention = nn.MultiheadAttention(embed_size, heads, batch_first=True)",
        "        ",
        "        # Cross-attention layer",
        "        self.cross_attention = nn.MultiheadAttention(embed_size, heads, batch_first=True)",
        "        ",
        "        # Layer normalizations",
        "        self.norm1 = nn.LayerNorm(embed_size)",
        "        self.norm2 = nn.LayerNorm(embed_size)",
        "        self.norm3 = nn.LayerNorm(embed_size)",
        "        ",
        "        # Feed forward network",
        "        self.feed_forward = nn.Sequential(",
        "            nn.Linear(embed_size, forward_expansion * embed_size),",
        "            nn.ReLU(),",
        "            nn.Linear(forward_expansion * embed_size, embed_size),",
        "        )",
        "        ",
        "        self.dropout = nn.Dropout(dropout)",
        "        ",
        "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):",
        "        # Self-attention with residual connection and layer normalization",
        "        self_attention_out, self_attention_weights = self.self_attention(",
        "            x, x, x, ",
        "            attn_mask=tgt_mask, ",
        "            need_weights=True",
        "        )",
        "        x = self.norm1(x + self.dropout(self_attention_out))",
        "        ",
        "        # Cross-attention with residual connection and layer normalization",
        "        cross_attention_out, cross_attention_weights = self.cross_attention(",
        "            x, encoder_output, encoder_output, ",
        "            attn_mask=src_mask, ",
        "            need_weights=True",
        "        )",
        "        x = self.norm2(x + self.dropout(cross_attention_out))",
        "        ",
        "        # Feed forward with residual connection and layer normalization",
        "        ff_out = self.feed_forward(x)",
        "        x = self.norm3(x + self.dropout(ff_out))",
        "        ",
        "        return x, self_attention_weights, cross_attention_weights",
        "",
        "# Example usage",
        "embed_size = 256",
        "heads = 8",
        "dropout = 0.1",
        "forward_expansion = 4",
        "batch_size = 2",
        "seq_length = 10",
        "",
        "# Create encoder layer and sample input",
        "encoder_layer = TransformerEncoderLayer(embed_size, heads, dropout, forward_expansion)",
        "encoder_input = torch.randn(batch_size, seq_length, embed_size)",
        "",
        "# Create decoder layer and sample input",
        "decoder_layer = TransformerDecoderLayer(embed_size, heads, dropout, forward_expansion)",
        "decoder_input = torch.randn(batch_size, seq_length, embed_size)",
        "",
        "# Forward pass through encoder layer",
        "encoder_output, encoder_attention = encoder_layer(encoder_input)",
        "",
        "# Forward pass through decoder layer",
        "decoder_output, self_attention, cross_attention = decoder_layer(",
        "    decoder_input, encoder_output",
        ")",
        "",
        "# Print shapes",
        "print(f\"Encoder input shape: {encoder_input.shape}\")",
        "print(f\"Encoder output shape: {encoder_output.shape}\")",
        "print(f\"Encoder attention shape: {encoder_attention.shape}\")",
        "print(f\"Decoder output shape: {decoder_output.shape}\")",
        "print(f\"Self-attention shape: {self_attention.shape}\")",
        "print(f\"Cross-attention shape: {cross_attention.shape}\")",
        "",
        "# Visualize parameters",
        "print(\"\\nLayer parameters:\")",
        "total_params = 0",
        "for name, param in encoder_layer.named_parameters():",
        "    print(f\"{name}: {param.shape}\")",
        "    total_params += param.numel()",
        "print(f\"Total parameters in encoder layer: {total_params:,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stacking Multiple Layers",
        "",
        "Transformers gain their impressive capacity through stacking multiple layers. Typical transformers use 6-24 encoder/decoder layers, with larger models using even more. Each layer:",
        "",
        "1. Refines the representations from the previous layer",
        "2. Captures increasingly complex patterns",
        "3. Develops more abstract features",
        "",
        "The multi-layer design allows:",
        "- Lower layers to focus on local patterns and syntax",
        "- Middle layers to handle semantic relationships",
        "- Upper layers to model high-level abstractions and task-specific features",
        "",
        "This hierarchical learning is one reason transformers perform so well across diverse language tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. **Hidden State**",
        "- Intermediate output of a layer.",
        "- Passed from one layer to the next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Hidden states are the intermediate representations that flow through the layers of a transformer model. They encapsulate the evolving understanding of the input data as it's processed by the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What Are Hidden States?",
        "",
        "In transformer models, a hidden state is a vector (or sequence of vectors) that represents the information for each token in the input sequence. Each hidden state contains a rich, high-dimensional representation of a token in context.",
        "",
        "For example, in a transformer processing the sentence \"The cat sat on the mat\":",
        "- Each word has its own hidden state vector",
        "- These vectors start as word embeddings at the input layer",
        "- As they pass through transformer layers, they are transformed to capture increasingly complex relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mathematical Representation",
        "",
        "For a transformer model with L layers:",
        "```",
        "H\u2080 = WordEmbedding(input) + PositionalEncoding",
        "H\u2097 = TransformerLayer(H\u2097\u208b\u2081) for each layer l=1,2,...,L",
        "```",
        "",
        "Where:",
        "- H\u2080 is the initial hidden state created from embeddings",
        "- H\u2097 is the hidden state after layer l",
        "- Each hidden state is typically a tensor with shape [batch_size, sequence_length, hidden_dimension]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evolution Through Layers",
        "",
        "Hidden states transform in specific ways as they flow through the network:",
        "",
        "1. **Input Layer (H\u2080)**: Basic word meaning + position information",
        "2. **Early Layers (H\u2081, H\u2082)**: Capture local syntax and simple word relationships",
        "3. **Middle Layers (H\u2083, H\u2084,...)**: Develop more complex linguistic features",
        "4. **Final Layers (H_{L-1}, H_L)**: Produce task-specific representations",
        "",
        "Research has shown that different layers specialize in different types of linguistic information:",
        "- Lower layers: Part-of-speech, syntactic dependencies",
        "- Middle layers: Semantic roles, entity types",
        "- Higher layers: High-level semantic features, task-specific information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Why Hidden States Matter",
        "",
        "Hidden states are crucial for understanding how transformers work:",
        "",
        "1. **Information Flow**: They represent how information travels and transforms through the network",
        "2. **Model Interpretability**: Analyzing hidden states helps understand what the model has learned",
        "3. **Feature Extraction**: Hidden states can be extracted for downstream tasks",
        "4. **Transfer Learning**: Pre-trained hidden states contain rich linguistic knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualizing Hidden States",
        "",
        "Hidden states are typically high-dimensional (768+ dimensions), making direct visualization challenging. Common approaches include:",
        "",
        "1. **Dimensionality Reduction**: Using PCA, t-SNE or UMAP to project to 2D/3D",
        "2. **Similarity Analysis**: Comparing how similar hidden states are to each other",
        "3. **Probing Tasks**: Using classifiers to determine what information is present in hidden states",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import matplotlib.pyplot as plt",
        "from sklearn.decomposition import PCA",
        "",
        "# Simple multi-layer transformer to demonstrate hidden states",
        "class MultiLayerTransformer(nn.Module):",
        "    def __init__(self, embed_size, num_layers, heads, dropout):",
        "        super(MultiLayerTransformer, self).__init__()",
        "        ",
        "        # Create multiple encoder layers",
        "        self.layers = nn.ModuleList(",
        "            [nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads, ",
        "                                      batch_first=True, dropout=dropout) ",
        "             for _ in range(num_layers)]",
        "        )",
        "        ",
        "    def forward(self, x, return_hidden_states=False):",
        "        hidden_states = []",
        "        current_x = x",
        "        ",
        "        for layer in self.layers:",
        "            current_x = layer(current_x)",
        "            hidden_states.append(current_x)",
        "        ",
        "        if return_hidden_states:",
        "            return current_x, hidden_states",
        "        else:",
        "            return current_x",
        "",
        "# Function to visualize hidden states across layers",
        "def visualize_hidden_states(hidden_states, tokens):",
        "    # Use PCA to reduce dimensionality for visualization",
        "    pca = PCA(n_components=2)",
        "    ",
        "    # Create a figure with subplots for each layer",
        "    fig, axes = plt.subplots(1, len(hidden_states), figsize=(15, 5))",
        "    ",
        "    # Process each layer's hidden states",
        "    for i, states in enumerate(hidden_states):",
        "        # Get first sequence in batch",
        "        layer_states = states[0].detach().numpy()",
        "        ",
        "        # Apply PCA",
        "        reduced = pca.fit_transform(layer_states)",
        "        ",
        "        # Plot in 2D space",
        "        axes[i].scatter(reduced[:, 0], reduced[:, 1])",
        "        ",
        "        # Add token labels",
        "        for j, token in enumerate(tokens):",
        "            axes[i].annotate(token, (reduced[j, 0], reduced[j, 1]))",
        "        ",
        "        axes[i].set_title(f\"Layer {i+1}\")",
        "        ",
        "    plt.tight_layout()",
        "    return fig",
        "",
        "# Example usage",
        "embed_size = 256",
        "num_layers = 3",
        "heads = 8",
        "dropout = 0.1",
        "batch_size = 2",
        "seq_length = 10",
        "",
        "# Create model and input",
        "model = MultiLayerTransformer(embed_size, num_layers, heads, dropout)",
        "x = torch.randn(batch_size, seq_length, embed_size)",
        "",
        "# Forward pass with hidden states",
        "output, hidden_states = model(x, return_hidden_states=True)",
        "",
        "print(f\"Input shape: {x.shape}\")",
        "print(f\"Output shape: {output.shape}\")",
        "print(f\"Number of hidden states: {len(hidden_states)}\")",
        "for i, hidden_state in enumerate(hidden_states):",
        "    print(f\"Hidden state {i+1} shape: {hidden_state.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Applications of Hidden States",
        "",
        "Understanding and utilizing hidden states enables many important applications:",
        "",
        "1. **Feature Extraction**: Using hidden states from pre-trained models as features for downstream tasks",
        "2. **Knowledge Probing**: Testing what linguistic knowledge is captured in different layers",
        "3. **Representation Learning**: Creating sentence embeddings by pooling hidden states",
        "4. **Model Distillation**: Training smaller models to mimic the hidden states of larger models",
        "5. **Cross-Modal Learning**: Aligning hidden states across different modalities (text, images, audio)",
        "",
        "The rich information encoded in transformer hidden states is a key reason for their success across many natural language tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. **Parameters**",
        "- Total number of **learnable weights** in the model.",
        "- GPT-2 Small has 124M; GPT-3 has 175B+."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Theory",
        "",
        "Parameters are the learnable weights and biases in a neural network that the model adjusts during training to minimize the loss function. In transformer models, parameters represent the model's knowledge and capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### What Are Parameters?",
        "",
        "In transformer models, parameters include:",
        "",
        "1. **Embedding Weights**: Vectors that represent tokens in the vocabulary",
        "2. **Attention Weights**: Query, key, and value projection matrices in attention mechanisms",
        "3. **Feed-Forward Weights**: The weights of the position-wise feed-forward networks",
        "4. **Layer Normalization Parameters**: Scale and shift parameters for normalization",
        "5. **Output Layer Weights**: Final projection to vocabulary or task-specific outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parameter Distribution in Transformers",
        "",
        "The parameters in a transformer model are distributed across components roughly as follows:",
        "",
        "| Component                  | Typical % of Parameters |",
        "|----------------------------|-------------------------|",
        "| Token Embeddings          | 15-30%                  |",
        "| Self-Attention Layers     | 10-20%                  |",
        "| Feed-Forward Networks     | 50-65%                  |",
        "| Layer Normalization       | <1%                     |",
        "| Output Layer              | 5-20%                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Scaling Laws",
        "",
        "Transformer models follow predictable scaling laws where performance improves as a power law with:",
        "",
        "1. **Model Size**: Number of parameters",
        "2. **Dataset Size**: Amount of training data",
        "3. **Compute Budget**: Total computation used for training",
        "",
        "Research by Kaplan et al. (2020) shows that doubling the parameter count reliably yields a constant improvement in loss, following the relationship:",
        "",
        "```",
        "L(N) \u2248 C \u00b7 N^(-\u03b1)",
        "```",
        "Where:",
        "- L is the loss",
        "- N is the number of parameters",
        "- \u03b1 is a scaling exponent (typically around 0.05-0.1)",
        "- C is a constant"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visual Representation",
        "",
        "```",
        "Parameter Growth in LLMs:",
        "",
        "                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                        \u2502GPT-4 (?)  \u2502",
        "                                                        \u25021000B+ (?) \u2502",
        "                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                              \u2502",
        "                                                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                        \u2502GPT-3.5    \u2502",
        "                                                        \u2502~500B (?)  \u2502",
        "                                                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                       \u2502  PaLM 540B  \u2502",
        "                                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                       \u2502  GPT-3 175B \u2502",
        "                                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                         \u2502  Megatron-Turing NLG 530B \u2502 PaLM 62B  \u2502",
        "                                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                      \u2502 GPT-NeoX 20B \u2502",
        "                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                              \u2502 T5 11B \u2502 GPT-J 6B \u2502 OPT 6B \u2502",
        "                                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                     \u2502 BERT-Large 340M \u2502",
        "                                                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                      \u2502 GPT-2 1.5B  \u2502",
        "                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                              \u2502GPT 117M \u2502 BERT-Base 110M \u2502",
        "                                              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "                                                              \u25b2",
        "                                                              \u2502",
        "                                                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510",
        "                                                      \u2502  2018-2023  \u2502",
        "                                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Parameter Efficiency",
        "",
        "As models grow larger, researchers have developed methods to make more efficient use of parameters:",
        "",
        "1. **Parameter Sharing**: Techniques like Albert use tied parameters across layers",
        "2. **Mixture of Experts (MoE)**: Only activate a subset of parameters for each input",
        "3. **Low-Rank Adaptations (LoRA)**: Update low-rank projections instead of full weights",
        "4. **Quantization**: Reduce precision of weights (e.g., from 32-bit to 8-bit or 4-bit)",
        "5. **Pruning**: Remove less important weights after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How Parameters Scale with Model Size",
        "",
        "For a standard transformer model with:",
        "- L layers",
        "- H hidden dimension",
        "- A attention heads",
        "- V vocabulary size",
        "- F feed-forward dimension (typically 4H)",
        "",
        "The approximate parameter count is:",
        "",
        "```",
        "Params \u2248 2 \u00d7 V \u00d7 H                    (Embedding layers)",
        "      + L \u00d7 (4 \u00d7 H\u00b2 + 8 \u00d7 H \u00d7 H/A)    (Self-attention)",
        "      + L \u00d7 (8 \u00d7 H \u00d7 F)               (Feed-forward)",
        "      + 4 \u00d7 L \u00d7 H                     (Layer norm)",
        "```",
        "",
        "For example, GPT-2 Small has:",
        "- 12 layers",
        "- 768 hidden dimension",
        "- 12 attention heads",
        "- 50,257 vocabulary size",
        "- 3072 feed-forward dimension",
        "",
        "Which results in approximately 124 million parameters.",
        "",
        "**Code Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch",
        "import torch.nn as nn",
        "import matplotlib.pyplot as plt",
        "import pandas as pd",
        "import numpy as np",
        "",
        "# Function to count parameters in a model",
        "def count_parameters(model):",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)",
        "",
        "# Function to calculate transformer parameters analytically",
        "def calculate_transformer_params(layers, hidden_dim, heads, vocab_size, ff_dim=None):",
        "    if ff_dim is None:",
        "        ff_dim = 4 * hidden_dim",
        "        ",
        "    # Embedding parameters (input + positional)",
        "    embedding_params = vocab_size * hidden_dim + 1024 * hidden_dim",
        "    ",
        "    # Self-attention parameters per layer",
        "    attn_params_per_layer = 4 * hidden_dim * hidden_dim  # Q, K, V, and output projections",
        "    ",
        "    # Feed-forward parameters per layer",
        "    ff_params_per_layer = hidden_dim * ff_dim + ff_dim * hidden_dim  # Two linear layers",
        "    ",
        "    # Layer norm parameters per layer (2 layer norms per layer)",
        "    ln_params_per_layer = 4 * hidden_dim  # 2 * (scale + bias)",
        "    ",
        "    # Total parameters",
        "    total_params = embedding_params + layers * (attn_params_per_layer + ff_params_per_layer + ln_params_per_layer)",
        "    ",
        "    # Output projection",
        "    total_params += hidden_dim * vocab_size",
        "    ",
        "    return total_params",
        "",
        "# Define configurations for various models",
        "model_configs = {",
        "    \"Custom Small\": {\"layers\": 6, \"hidden_dim\": 384, \"heads\": 6, \"vocab_size\": 30000},",
        "    \"Custom Medium\": {\"layers\": 12, \"hidden_dim\": 512, \"heads\": 8, \"vocab_size\": 30000},",
        "    \"Custom Large\": {\"layers\": 12, \"hidden_dim\": 768, \"heads\": 12, \"vocab_size\": 30000},",
        "    \"GPT-2 Small\": {\"layers\": 12, \"hidden_dim\": 768, \"heads\": 12, \"vocab_size\": 50257},",
        "    \"GPT-2 Medium\": {\"layers\": 24, \"hidden_dim\": 1024, \"heads\": 16, \"vocab_size\": 50257},",
        "    \"GPT-2 Large\": {\"layers\": 36, \"hidden_dim\": 1280, \"heads\": 20, \"vocab_size\": 50257},",
        "    \"GPT-2 XL\": {\"layers\": 48, \"hidden_dim\": 1600, \"heads\": 25, \"vocab_size\": 50257},",
        "    \"GPT-3 Small\": {\"layers\": 12, \"hidden_dim\": 768, \"heads\": 12, \"vocab_size\": 50257},",
        "    \"GPT-3 Medium\": {\"layers\": 24, \"hidden_dim\": 2048, \"heads\": 16, \"vocab_size\": 50257},",
        "    \"GPT-3 Large\": {\"layers\": 24, \"hidden_dim\": 4096, \"heads\": 32, \"vocab_size\": 50257},",
        "    \"GPT-3 XL\": {\"layers\": 24, \"hidden_dim\": 8192, \"heads\": 64, \"vocab_size\": 50257},",
        "    \"GPT-3 13B\": {\"layers\": 40, \"hidden_dim\": 5140, \"heads\": 40, \"vocab_size\": 50257},",
        "    \"GPT-3 175B\": {\"layers\": 96, \"hidden_dim\": 12288, \"heads\": 96, \"vocab_size\": 50257},",
        "}",
        "",
        "# Calculate parameters for each model",
        "params_by_model = {}",
        "for model_name, config in model_configs.items():",
        "    params = calculate_transformer_params(",
        "        config[\"layers\"], ",
        "        config[\"hidden_dim\"], ",
        "        config[\"heads\"], ",
        "        config[\"vocab_size\"]",
        "    )",
        "    params_by_model[model_name] = params",
        "",
        "# Convert to DataFrame for analysis",
        "df = pd.DataFrame({",
        "    \"Model\": list(params_by_model.keys()),",
        "    \"Parameters\": list(params_by_model.values())",
        "})",
        "df[\"Parameters (Millions)\"] = df[\"Parameters\"] / 1e6",
        "df[\"Parameters (Billions)\"] = df[\"Parameters\"] / 1e9",
        "",
        "# Analyze parameter breakdown for one model",
        "def analyze_parameter_breakdown(model_name, config):",
        "    layers = config[\"layers\"]",
        "    hidden_dim = config[\"hidden_dim\"]",
        "    vocab_size = config[\"vocab_size\"]",
        "    ff_dim = 4 * hidden_dim",
        "    ",
        "    # Calculate component parameters",
        "    embedding_params = vocab_size * hidden_dim + 1024 * hidden_dim",
        "    attn_params = layers * 4 * hidden_dim * hidden_dim",
        "    ff_params = layers * (hidden_dim * ff_dim + ff_dim * hidden_dim)",
        "    ln_params = layers * 4 * hidden_dim",
        "    output_params = hidden_dim * vocab_size",
        "    ",
        "    total = embedding_params + attn_params + ff_params + ln_params + output_params",
        "    ",
        "    # Create breakdown",
        "    breakdown = {",
        "        \"Embeddings\": embedding_params / total * 100,",
        "        \"Self-Attention\": attn_params / total * 100,",
        "        \"Feed-Forward\": ff_params / total * 100,",
        "        \"Layer Norm\": ln_params / total * 100,",
        "        \"Output Layer\": output_params / total * 100",
        "    }",
        "    ",
        "    return breakdown",
        "",
        "# Analyze GPT-2 Small breakdown",
        "gpt2_small_breakdown = analyze_parameter_breakdown(\"GPT-2 Small\", model_configs[\"GPT-2 Small\"])",
        "",
        "# Plot parameter breakdown",
        "plt.figure(figsize=(10, 6))",
        "plt.pie(gpt2_small_breakdown.values(), labels=gpt2_small_breakdown.keys(), autopct='%1.1f%%')",
        "plt.title(\"Parameter Distribution in GPT-2 Small\")",
        "plt.axis('equal')",
        "",
        "# Plot parameter growth",
        "selected_models = [\"Custom Small\", \"Custom Medium\", \"Custom Large\", ",
        "                 \"GPT-2 Small\", \"GPT-2 Medium\", \"GPT-2 Large\", \"GPT-2 XL\",",
        "                 \"GPT-3 Small\", \"GPT-3 Medium\", \"GPT-3 Large\", \"GPT-3 XL\", \"GPT-3 13B\", \"GPT-3 175B\"]",
        "selected_df = df[df[\"Model\"].isin(selected_models)]",
        "",
        "plt.figure(figsize=(12, 6))",
        "plt.bar(selected_df[\"Model\"], selected_df[\"Parameters (Billions)\"])",
        "plt.yscale('log')",
        "plt.ylabel(\"Parameters (Billions, log scale)\")",
        "plt.xticks(rotation=45, ha='right')",
        "plt.title(\"Parameter Growth in Language Models\")",
        "plt.tight_layout()",
        "",
        "# Print the results",
        "print(\"Parameter counts for different models:\")",
        "print(df[[\"Model\", \"Parameters (Millions)\", \"Parameters (Billions)\"]].sort_values(by=\"Parameters\"))",
        "",
        "print(\"\\nParameter breakdown for GPT-2 Small:\")",
        "for component, percentage in gpt2_small_breakdown.items():",
        "    print(f\"{component}: {percentage:.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameter Initialization",
        "",
        "How parameters are initialized before training can significantly affect convergence:",
        "",
        "1. **Xavier/Glorot Initialization**: Weights sampled from a distribution with variance scaled by fan-in and fan-out",
        "2. **He Initialization**: Variance scaled by fan-in, works well with ReLU activations",
        "3. **Small Random Initialization**: Used in many transformer implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Memory Requirements",
        "",
        "The memory required to store and train models scales with the parameter count:",
        "",
        "1. **Storage Size**: Parameters typically stored in 16-bit (half-precision) or 32-bit (full-precision)",
        "   - 175B parameters in 16-bit = ~350 GB",
        "",
        "2. **Training Memory**: Requires additional memory for:",
        "   - Optimizer states (can be 2-4x the model size)",
        "   - Activation gradients",
        "   - Forward activations",
        "",
        "3. **Inference Memory**: Typically lower than training, but still substantial",
        "   - Quantization can reduce to 8-bit, 4-bit, or even lower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Emergent Abilities",
        "",
        "One of the most fascinating aspects of large parameter models is the emergence of capabilities that aren't present in smaller models. These \"emergent abilities\" appear at specific parameter thresholds and include:",
        "",
        "1. Few-shot learning",
        "2. Instruction following",
        "3. Complex reasoning",
        "4. Code generation",
        "5. Chain-of-thought reasoning",
        "",
        "The relationship between parameter count and these abilities is not linear but appears more step-like, with new capabilities appearing when models reach certain sizes.",
        "",
        "This emergent behavior is a key reason why scaling transformers to massive parameter counts has been so productive for advancing AI capabilities."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}